{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eacc7147",
   "metadata": {},
   "source": [
    "## Chicago Traffic Crash Analysis & Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e1d66",
   "metadata": {},
   "source": [
    "#### Part 1: Setup & Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c71299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_curve, precision_recall_curve, \n",
    "    auc, average_precision_score, confusion_matrix,\n",
    "    mean_absolute_error, mean_squared_error, r2_score, median_absolute_error,\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ML Models - Classification\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# ML Models - Regression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, PoissonRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# Advanced ML libraries\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "from folium.plugins import HeatMap\n",
    "import folium\n",
    "\n",
    "# Constants\n",
    "random_seed = 45665456\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "# Matplotlib updated syntax for colormap (fixing deprecation warning)\n",
    "def get_viridis_colormap(n=10):\n",
    "    \"\"\"Return viridis colormap using updated Matplotlib syntax\"\"\"\n",
    "    return plt.colormaps['viridis']  # Updated from plt.cm.get_cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98ca12",
   "metadata": {},
   "source": [
    "#### Part 2: Viridis Color Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4bf000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Enhanced Color Standardization with Lighter Palette ===\n",
    "def get_viridis_colors(n_colors=10, start=0.2, end=0.8):\n",
    "    \"\"\"\n",
    "    Get n colors from the viridis colormap with customizable range.\n",
    "    Using a more limited range (0.2-0.8) creates lighter colors.\n",
    "    \"\"\"\n",
    "    return [plt.cm.viridis(i) for i in np.linspace(start, end, n_colors)]\n",
    "\n",
    "# Define standard colors for consistent usage with lighter shades\n",
    "VIRIDIS_COLORS = {\n",
    "    # Primary palette (much lighter versions)\n",
    "    'main': '#6a50a7',       # Light purple\n",
    "    'secondary': '#52c2c4',  # Light teal\n",
    "    'tertiary': '#8ddc6e',   # Light green\n",
    "    'highlight': '#fee04c',  # Light yellow\n",
    "    \n",
    "    # Very light shades for fills and backgrounds\n",
    "    'light_main': '#b9a5dd',     # Very light purple\n",
    "    'light_secondary': '#a8e7e8', # Very light teal\n",
    "    'light_tertiary': '#c9edb0',  # Very light green\n",
    "    'light_highlight': '#fff3a6', # Very light yellow\n",
    "    \n",
    "    # Extra light background colors\n",
    "    'bg_main': '#f3effa',     # Extra light purple\n",
    "    'bg_secondary': '#f0fafa', # Extra light teal\n",
    "    'bg_tertiary': '#f7fcf1',  # Extra light green\n",
    "    'bg_highlight': '#fffbec', # Extra light yellow\n",
    "    \n",
    "    # Categorical color palettes (all lighter)\n",
    "    'categorical': get_viridis_colors(10, 0.2, 0.8),  # Light palette\n",
    "    'categorical_light': get_viridis_colors(10, 0.3, 0.7),  # Even lighter\n",
    "    'categorical_pastel': get_viridis_colors(10, 0.4, 0.7)  # Pastel palette\n",
    "}\n",
    "\n",
    "# Function to create an alpha version of any color\n",
    "def with_alpha(color, alpha=0.5):\n",
    "    \"\"\"Create a transparent version of a color.\"\"\"\n",
    "    if color.startswith('#'):\n",
    "        # Convert hex to RGBA\n",
    "        r = int(color[1:3], 16) / 255.0\n",
    "        g = int(color[3:5], 16) / 255.0\n",
    "        b = int(color[5:7], 16) / 255.0\n",
    "        return (r, g, b, alpha)\n",
    "    else:\n",
    "        # Assume it's already an RGBA tuple\n",
    "        return (*color[:3], alpha)\n",
    "\n",
    "# Standardize matplotlib settings with improved defaults\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 13\n",
    "plt.rcParams['xtick.labelsize'] = 11\n",
    "plt.rcParams['ytick.labelsize'] = 11\n",
    "plt.rcParams['axes.facecolor'] = '#fcfcfc'  # Nearly white background\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.2  # Lighter grid\n",
    "plt.rcParams['axes.spines.top'] = False  # Remove top spine\n",
    "plt.rcParams['axes.spines.right'] = False  # Remove right spine\n",
    "\n",
    "# Create a style function for consistent plot styling with spine color control\n",
    "def apply_viridis_style(ax=None):\n",
    "    \"\"\"Apply the standardized viridis style with lighter colors to the given axes.\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    # Style improvements\n",
    "    ax.set_facecolor('#fcfcfc')  # Nearly white background\n",
    "    ax.grid(True, alpha=0.2, linestyle='--', color='#cccccc')  # Lighter grid\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Set spine colors manually - this is the correct way to set spine colors\n",
    "    ax.spines['left'].set_color('#dddddd')   # Light gray spine\n",
    "    ax.spines['bottom'].set_color('#dddddd') # Light gray spine\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052748b7",
   "metadata": {},
   "source": [
    "#### Part 3: Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004621a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Loading ===\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load crash data with proper types.\"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_cols = ['CRASH_DATE', 'DATE_POLICE_NOTIFIED']\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            \n",
    "    print(f\"Loaded {len(df)} records with {df.columns.size} columns\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09301718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Preprocessing Functions ===\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Comprehensive preprocessing function.\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # 1. Standardize column names\n",
    "    df_processed.columns = df_processed.columns.str.upper().str.replace(' ', '_')\n",
    "    \n",
    "    # 2. Extract datetime components\n",
    "    df_processed['CRASH_DATETIME'] = df_processed['CRASH_DATE'] + pd.to_timedelta(df_processed['CRASH_HOUR'], unit='h')\n",
    "    \n",
    "    # 3. Derive time-based features\n",
    "    df_processed['YEAR'] = df_processed['CRASH_DATETIME'].dt.year\n",
    "    df_processed['MONTH'] = df_processed['CRASH_DATETIME'].dt.month\n",
    "    df_processed['DAY'] = df_processed['CRASH_DATETIME'].dt.day\n",
    "    df_processed['HOUR'] = df_processed['CRASH_DATETIME'].dt.hour\n",
    "    df_processed['DAY_OF_WEEK'] = df_processed['CRASH_DATETIME'].dt.day_name()\n",
    "    \n",
    "    # 4. Create severity flag\n",
    "    df_processed['SEVERE'] = ((df_processed['INJURIES_TOTAL'] > 0) | \n",
    "                             (df_processed['INJURIES_FATAL'] > 0)).astype(int)\n",
    "    \n",
    "    # 5. Create time-of-day category\n",
    "    df_processed['TIME_OF_DAY'] = df_processed['HOUR'].apply(get_time_of_day)\n",
    "    \n",
    "    # 6. Create seasonal category\n",
    "    df_processed['SEASON'] = df_processed['MONTH'].apply(get_season)\n",
    "    \n",
    "    # 7. Additional flags for analysis\n",
    "    df_processed['IS_WEEKEND'] = df_processed['CRASH_DAY_OF_WEEK'].isin([6,7]).astype(int)\n",
    "    df_processed['IS_NIGHTTIME'] = ((df_processed['HOUR'] < 6) | \n",
    "                                   (df_processed['HOUR'] >= 20)).astype(int)\n",
    "    df_processed['IS_VULNERABLE'] = df_processed['FIRST_CRASH_TYPE'].isin(\n",
    "        ['PEDESTRIAN', 'PEDALCYCLIST']).astype(int)\n",
    "    \n",
    "    # 8. Weather conditions\n",
    "    bad_weather = ['RAIN', 'SNOW', 'SLEET', 'FREEZING RAIN']\n",
    "    df_processed['BAD_WEATHER'] = df_processed['WEATHER_CONDITION'].isin(\n",
    "        bad_weather).astype(int)\n",
    "    \n",
    "    # 9. Surface conditions\n",
    "    bad_surface = ['ICE', 'SNOW OR SLUSH', 'WET']\n",
    "    df_processed['BAD_SURFACE'] = df_processed['ROADWAY_SURFACE_COND'].isin(\n",
    "        bad_surface).astype(int)\n",
    "    \n",
    "    # 10. Intersection flag\n",
    "    df_processed['AT_INTERSECTION'] = (df_processed['INTERSECTION_RELATED_I'] == 'Y').astype(int)\n",
    "    \n",
    "    # 11. Fill common NA values\n",
    "    for col in ['INJURIES_TOTAL', 'INJURIES_FATAL', 'INJURIES_INCAPACITATING',\n",
    "                'INJURIES_NON_INCAPACITATING', 'INJURIES_REPORTED_NOT_EVIDENT']:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[col] = df_processed[col].fillna(0)\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "def get_time_of_day(hour):\n",
    "    \"\"\"Convert hour to time of day category.\"\"\"\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "def get_season(month):\n",
    "    \"\"\"Convert month to season.\"\"\"\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027e755",
   "metadata": {},
   "source": [
    "#### Part 4: Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c46b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Missing Data Analysis ===\n",
    "def analyze_missing_data(df, plot=True):\n",
    "    \"\"\"Analyze and visualize missing data.\"\"\"\n",
    "    # Calculate missing counts and percentages\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "    missing_pct = (missing_counts / len(df) * 100).round(2)\n",
    "    \n",
    "    # Create a DataFrame for display\n",
    "    missing_df = pd.DataFrame({\n",
    "        'missing_count': missing_counts,\n",
    "        'missing_pct': missing_pct\n",
    "    })\n",
    "    \n",
    "    # Display top columns with missing data\n",
    "    print(f\"Found {len(missing_df)} columns with missing values\")\n",
    "    print(\"\\nTop columns with missing values:\")\n",
    "    print(missing_df.head(10))\n",
    "    \n",
    "    if plot and not missing_df.empty:\n",
    "        # Plot the top 20 columns by percentage missing\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        missing_df['missing_pct'].head(20).plot(kind='bar', color=VIRIDIS_COLORS['main'])\n",
    "        plt.title('Top Columns by Percentage of Missing Values', fontsize=14)\n",
    "        plt.ylabel('Percent Missing', fontsize=12)\n",
    "        plt.xlabel('Column', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return missing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1d04c4",
   "metadata": {},
   "source": [
    "#### Part 5: EDA Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Temporal Analysis Functions ===\n",
    "def plot_crashes_by_hour(df, color_palette=None):\n",
    "    \"\"\"Plot crash counts by hour of day with enhanced visualization.\"\"\"\n",
    "    hour_col = 'CRASH_HOUR' if 'CRASH_HOUR' in df.columns else 'HOUR'\n",
    "    hour_counts = df[hour_col].value_counts().sort_index()\n",
    "    \n",
    "    # Calculate the average count\n",
    "    average = hour_counts.mean()\n",
    "    \n",
    "    # Create color gradient\n",
    "    if color_palette is None:\n",
    "        # Updated syntax for Matplotlib 3.7+\n",
    "        viridis = plt.colormaps['viridis']  # Updated from plt.cm.get_cmap('viridis', 24)\n",
    "        colors = [viridis(i/23) for i in range(24)]\n",
    "    else:\n",
    "        colors = color_palette\n",
    "        \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(hour_counts.index, hour_counts.values, width=0.7, color=colors)\n",
    "    \n",
    "    # Add average line\n",
    "    plt.axhline(y=average, color='#555555', linestyle='--', linewidth=1.5)\n",
    "    plt.text(23, average * 1.05, f'Average: {average:.0f}',\n",
    "             ha='right', va='bottom', color='black', fontsize=10,\n",
    "             bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.3'))\n",
    "    \n",
    "    # Add peak annotation\n",
    "    peak_hour = hour_counts.idxmax()\n",
    "    peak_value = hour_counts.max()\n",
    "    plt.annotate(f'Peak: {peak_value:,} crashes',\n",
    "                xy=(peak_hour, peak_value),\n",
    "                xytext=(peak_hour - 3, peak_value * 0.8),\n",
    "                arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\"),\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7))\n",
    "    \n",
    "    # Highlight rush hours\n",
    "    rush_periods = [\n",
    "        {\"period\": \"Morning Rush\", \"start\": 7, \"end\": 9},\n",
    "        {\"period\": \"Evening Rush\", \"start\": 16, \"end\": 18}\n",
    "    ]\n",
    "    for period in rush_periods:\n",
    "        plt.axvspan(period[\"start\"], period[\"end\"], alpha=0.2, color='gray')\n",
    "        plt.text((period[\"start\"] + period[\"end\"]) / 2, max(hour_counts) * 0.9,\n",
    "                period[\"period\"], ha='center', fontsize=10)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('Crash Counts by Hour of Day', fontsize=14, pad=20)\n",
    "    plt.xlabel('Hour of Day (24h)', fontsize=12)\n",
    "    plt.ylabel('Number of Crashes', fontsize=12)\n",
    "    plt.xticks(range(24))\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# %%\n",
    "def plot_crashes_by_day_of_week(df):\n",
    "    \"\"\"Plot crash counts by day of week with improved visualization.\"\"\"\n",
    "    # Get day of week counts\n",
    "    dow_mapping = {\n",
    "        'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3,\n",
    "        'Friday': 4, 'Saturday': 5, 'Sunday': 6\n",
    "    }\n",
    "    \n",
    "    # Check if we have string day names or numeric values\n",
    "    if 'DAY_OF_WEEK' in df.columns and df['DAY_OF_WEEK'].dtype == 'object':\n",
    "        df_temp = df.copy()\n",
    "        df_temp['DOW_NUM'] = df_temp['DAY_OF_WEEK'].map(dow_mapping)\n",
    "        dow_counts = df_temp.groupby('DOW_NUM').size()\n",
    "    else:\n",
    "        dow_counts = df['CRASH_DAY_OF_WEEK'].value_counts().sort_index()\n",
    "    \n",
    "    # Get day names in correct order\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    \n",
    "    # Create colors from the viridis palette\n",
    "    colors = get_viridis_colors(7)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create the bar chart\n",
    "    bars = plt.bar(days, dow_counts, color=colors)\n",
    "    \n",
    "    # Add data labels on top of each bar\n",
    "    for i, (count, bar) in enumerate(zip(dow_counts, bars)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., count + dow_counts.max()*0.01,\n",
    "                f\"{count:,}\", ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Highlight weekends\n",
    "    plt.axvspan(4.5, 6.5, alpha=0.2, color='gray')\n",
    "    plt.text(5.5, dow_counts.max() * 0.9, \"Weekend\", ha='center',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7))\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Crashes by Day of Week', fontsize=14, pad=20)\n",
    "    plt.xlabel('Day of Week', fontsize=12)\n",
    "    plt.ylabel('Number of Crashes', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "def plot_monthly_trend(df, year=None):\n",
    "    \"\"\"Plot monthly crash trends with seasonal highlights.\"\"\"\n",
    "    # Filter by year if specified\n",
    "    if year is not None:\n",
    "        df_year = df[df['YEAR'] == year]\n",
    "    else:\n",
    "        df_year = df.copy()\n",
    "    \n",
    "    # Group by month\n",
    "    monthly_counts = df_year.groupby('MONTH').size()\n",
    "    \n",
    "    # Make sure all months are represented\n",
    "    all_months = pd.Series(index=range(1, 13), data=0)\n",
    "    monthly_counts = monthly_counts.add(all_months, fill_value=0)\n",
    "    \n",
    "    # Month names\n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    \n",
    "    # Create viridis color gradient\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, 12))\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot the bars\n",
    "    bars = plt.bar(range(1, 13), monthly_counts, color=colors)\n",
    "    \n",
    "    # Add data labels on top of each bar\n",
    "    for i, (count, bar) in enumerate(zip(monthly_counts, bars)):\n",
    "        plt.text(i+1, count + max(monthly_counts)*0.02,\n",
    "                f\"{count:,}\", ha='center', fontsize=10)\n",
    "    \n",
    "    # Add seasonal bands\n",
    "    seasons = [\n",
    "        {\"name\": \"Winter\", \"start\": 0.5, \"end\": 2.5, \"color\": \"#e6f2ff\"},\n",
    "        {\"name\": \"Spring\", \"start\": 2.5, \"end\": 5.5, \"color\": \"#e6ffe6\"},\n",
    "        {\"name\": \"Summer\", \"start\": 5.5, \"end\": 8.5, \"color\": \"#ffebcc\"},\n",
    "        {\"name\": \"Fall\", \"start\": 8.5, \"end\": 11.5, \"color\": \"#f2e6ff\"},\n",
    "        {\"name\": \"Winter\", \"start\": 11.5, \"end\": 12.5, \"color\": \"#e6f2ff\"}\n",
    "    ]\n",
    "    \n",
    "    for season in seasons:\n",
    "        plt.axvspan(season[\"start\"], season[\"end\"], alpha=0.2, color=season[\"color\"])\n",
    "        # Only add label if season spans enough space\n",
    "        if season[\"end\"] - season[\"start\"] > 1:\n",
    "            plt.text((season[\"start\"] + season[\"end\"]) / 2, max(monthly_counts) * 0.9,\n",
    "                    season[\"name\"], ha='center', fontsize=10,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", alpha=0.7))\n",
    "    \n",
    "    # Add average line\n",
    "    avg_crashes = monthly_counts.mean()\n",
    "    plt.axhline(y=avg_crashes, color='#333333', linestyle='--', alpha=0.7)\n",
    "    plt.text(12, avg_crashes * 1.05, f\"Monthly Average: {avg_crashes:,.0f}\",\n",
    "            ha='right', fontsize=10,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7))\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title(f'Monthly Crash Counts {f\"({year})\" if year else \"\"}', fontsize=14, pad=20)\n",
    "    plt.xlabel('Month', fontsize=12)\n",
    "    plt.ylabel('Number of Crashes', fontsize=12)\n",
    "    plt.xticks(range(1, 13), month_names)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "def plot_temporal_heatmap(df):\n",
    "    \"\"\"Create a heatmap of crashes by hour and day of week.\"\"\"\n",
    "    # Create pivot table\n",
    "    if 'DAY_OF_WEEK' in df.columns and df['DAY_OF_WEEK'].dtype == 'object':\n",
    "        # Map day names to numbers for proper ordering\n",
    "        dow_mapping = {\n",
    "            'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3,\n",
    "            'Friday': 4, 'Saturday': 5, 'Sunday': 6\n",
    "        }\n",
    "        df_temp = df.copy()\n",
    "        df_temp['DOW_NUM'] = df_temp['DAY_OF_WEEK'].map(dow_mapping)\n",
    "        pivot = pd.pivot_table(\n",
    "            df_temp, values='CRASH_RECORD_ID', index='HOUR',\n",
    "            columns='DOW_NUM', aggfunc='count', fill_value=0\n",
    "        )\n",
    "        # Rename columns back to day names\n",
    "        reverse_mapping = {v: k for k, v in dow_mapping.items()}\n",
    "        pivot.columns = [reverse_mapping[col] for col in pivot.columns]\n",
    "    else:\n",
    "        # Using numeric day of week\n",
    "        pivot = pd.pivot_table(\n",
    "            df, values='CRASH_RECORD_ID', index='HOUR',\n",
    "            columns='CRASH_DAY_OF_WEEK', aggfunc='count', fill_value=0\n",
    "        )\n",
    "        # Map numeric days to names if needed\n",
    "        if pivot.columns.dtype == 'int64' or pivot.columns.dtype == 'int32':\n",
    "            day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "            pivot.columns = [day_names[i-1] for i in pivot.columns]\n",
    "    \n",
    "    # Ensure we have all hours\n",
    "    all_hours = range(0, 24)\n",
    "    pivot = pivot.reindex(all_hours, fill_value=0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Create heatmap with improved colormap - updated syntax\n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        cmap='viridis',  # Using string name instead of plt.cm.viridis\n",
    "        linewidths=0.5,\n",
    "        annot=False,  # Too cluttered with annotations\n",
    "        fmt='d',\n",
    "        cbar_kws={'label': 'Number of Crashes'}\n",
    "    )\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Crashes by Hour and Day of Week', fontsize=16, pad=20)\n",
    "    plt.xlabel('Day of Week', fontsize=12)\n",
    "    plt.ylabel('Hour of Day (24h)', fontsize=12)\n",
    "    \n",
    "    # Add annotations for key patterns\n",
    "    # Morning rush hour\n",
    "    plt.annotate(\n",
    "        'Morning\\nRush Hour',\n",
    "        xy=(3.5, 8), xytext=(3.5, 6),\n",
    "        arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\"),\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7),\n",
    "        ha='center', va='center'\n",
    "    )\n",
    "    \n",
    "    # Evening rush hour\n",
    "    plt.annotate(\n",
    "        'Evening\\nRush Hour',\n",
    "        xy=(3.5, 17), xytext=(3.5, 20),\n",
    "        arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=-.2\"),\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7),\n",
    "        ha='center', va='center'\n",
    "    )\n",
    "    \n",
    "    # Weekend nighttime\n",
    "    plt.annotate(\n",
    "        'Weekend\\nNight Activity',\n",
    "        xy=(5.5, 1), xytext=(4, 3),\n",
    "        arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\"),\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7),\n",
    "        ha='center', va='center'\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "def plot_crash_types(df, n=10):\n",
    "    \"\"\"Plot top N crash types with Viridis colors.\"\"\"\n",
    "    crash_counts = df['FIRST_CRASH_TYPE'].value_counts().nlargest(n)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create color gradient\n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.8, len(crash_counts)))\n",
    "    \n",
    "    # Create horizontal bars for better readability\n",
    "    bars = plt.barh(\n",
    "        y=crash_counts.index[::-1],  # Reverse to put largest at top\n",
    "        width=crash_counts.values[::-1],\n",
    "        color=colors\n",
    "    )\n",
    "    \n",
    "    # Add data labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(\n",
    "            width + (max(crash_counts) * 0.01),\n",
    "            bar.get_y() + bar.get_height()/2,\n",
    "            f\"{width:,}\",\n",
    "            ha='left', va='center', fontsize=10\n",
    "        )\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(f'Top {n} Crash Types', fontsize=14, pad=20)\n",
    "    plt.xlabel('Number of Crashes', fontsize=12)\n",
    "    plt.ylabel('Crash Type', fontsize=12)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "def plot_severity_by_factor(df, factor_col, top_n=None, normalize=True):\n",
    "    \"\"\"\n",
    "    Plot severity proportions by a given factor.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the data\n",
    "    factor_col : str\n",
    "        Column name to group by\n",
    "    top_n : int, optional\n",
    "        Limit to top N categories by count\n",
    "    normalize : bool, default=True\n",
    "        Whether to normalize to show proportions instead of counts\n",
    "    \"\"\"\n",
    "    # Get factor counts\n",
    "    factor_counts = df[factor_col].value_counts()\n",
    "    \n",
    "    # Limit to top N if specified\n",
    "    if top_n is not None:\n",
    "        top_factors = factor_counts.nlargest(top_n).index\n",
    "        df_plot = df[df[factor_col].isin(top_factors)]\n",
    "    else:\n",
    "        df_plot = df\n",
    "    \n",
    "    # Create cross-tabulation\n",
    "    if normalize:\n",
    "        ct = pd.crosstab(df_plot[factor_col], df_plot['SEVERE'], normalize='index')\n",
    "        ylabel = 'Proportion'\n",
    "    else:\n",
    "        ct = pd.crosstab(df_plot[factor_col], df_plot['SEVERE'])\n",
    "        ylabel = 'Count'\n",
    "    \n",
    "    # Ensure we have both severity classes\n",
    "    if 0 not in ct.columns:\n",
    "        ct[0] = 0\n",
    "    if 1 not in ct.columns:\n",
    "        ct[1] = 0\n",
    "    \n",
    "    # Sort by severity proportion\n",
    "    ct = ct.sort_values(by=1, ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create the stacked bar chart with viridis colors\n",
    "    ct.plot(\n",
    "        kind='bar', \n",
    "        stacked=True, \n",
    "        color=[VIRIDIS_COLORS['tertiary'], VIRIDIS_COLORS['main']], \n",
    "        ax=plt.gca()\n",
    "    )\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title(f'Crash Severity by {factor_col.replace(\"_\", \" \").title()}', fontsize=14, pad=20)\n",
    "    plt.xlabel(factor_col.replace('_', ' ').title(), fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(['Non-Severe', 'Severe'], title='Severity')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ct\n",
    "\n",
    "# %%\n",
    "def plot_map_hotspots(df, zoom=10):\n",
    "    \"\"\"\n",
    "    Create an interactive map showing crash hotspots.\n",
    "    Requires plotly for interactive visualization.\n",
    "    \"\"\"\n",
    "    # Filter records with valid coordinates\n",
    "    map_df = df.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "    \n",
    "    # Check for invalid coordinates (sometimes 0,0 is used for missing)\n",
    "    map_df = map_df[(map_df['LATITUDE'] != 0) & (map_df['LONGITUDE'] != 0)]\n",
    "    \n",
    "    # Create color mapping for severity\n",
    "    map_df['color'] = map_df['SEVERE'].map({\n",
    "        0: VIRIDIS_COLORS['tertiary'],\n",
    "        1: VIRIDIS_COLORS['main']\n",
    "    })\n",
    "    \n",
    "    # Create interactive map\n",
    "    fig = px.density_mapbox(\n",
    "        map_df, \n",
    "        lat='LATITUDE', \n",
    "        lon='LONGITUDE', \n",
    "        z='SEVERE',\n",
    "        radius=10,\n",
    "        center=dict(lat=map_df['LATITUDE'].mean(), lon=map_df['LONGITUDE'].mean()),\n",
    "        zoom=zoom,\n",
    "        mapbox_style=\"carto-positron\",\n",
    "        title='Crash Hotspots by Severity',\n",
    "        opacity=0.7,\n",
    "        color_continuous_scale='viridis'\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0},\n",
    "        coloraxis_colorbar=dict(\n",
    "            title=\"Severity Density\",\n",
    "            thicknessmode=\"pixels\", thickness=20,\n",
    "            lenmode=\"pixels\", len=300\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_injuries_by_speed_bin(df):\n",
    "    \"\"\"Plot boxplot of injuries by speed limit bin.\"\"\"\n",
    "    if 'SPD_BIN' not in df.columns and 'POSTED_SPEED_LIMIT' in df.columns:\n",
    "        # Create speed bins if they don't exist\n",
    "        bins = [0, 20, 30, 40, 50, 60, 100]\n",
    "        labels = ['<20', '20–30', '30–40', '40–50', '50–60', '60+']\n",
    "        df = df.copy()\n",
    "        df['SPD_BIN'] = pd.cut(df['POSTED_SPEED_LIMIT'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    # Filter to only incidents with injuries\n",
    "    injured_df = df[df['INJURIES_TOTAL'] > 0]\n",
    "    \n",
    "    # Create boxplot\n",
    "    sns.boxplot(x='SPD_BIN', y='INJURIES_TOTAL', data=injured_df, \n",
    "                palette=VIRIDIS_COLORS['categorical'])\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('Speed Limit Bin (mph)', fontsize=12)\n",
    "    plt.ylabel('Total Injuries', fontsize=12)\n",
    "    plt.title('Injuries by Speed Limit Bin', fontsize=14, pad=20)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_injury_rate_by_crash_type(df, top_n=10):\n",
    "    \"\"\"Plot injury rates by crash type.\"\"\"\n",
    "    if 'FIRST_CRASH_TYPE' not in df.columns or 'injury_flag' not in df.columns:\n",
    "        print(\"Required columns not found in DataFrame\")\n",
    "        if 'INJURIES_TOTAL' in df.columns and 'FIRST_CRASH_TYPE' in df.columns:\n",
    "            df = df.copy()\n",
    "            df['injury_flag'] = (df['INJURIES_TOTAL'] > 0).astype(int)\n",
    "        else:\n",
    "            return\n",
    "    \n",
    "    # Compute injury rate by crash type\n",
    "    injury_by_type = (\n",
    "        df\n",
    "        .groupby('FIRST_CRASH_TYPE')['injury_flag']\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    # Limit to top N types\n",
    "    if top_n is not None:\n",
    "        injury_by_type = injury_by_type.head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # Use standardized color palette\n",
    "    sns.barplot(\n",
    "        x=injury_by_type.values * 100,  # convert to percent\n",
    "        y=injury_by_type.index,\n",
    "        palette='rocket'\n",
    "    )\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('Percent of Crashes with ≥1 Injury', fontsize=12)\n",
    "    plt.ylabel('Crash Type', fontsize=12)\n",
    "    plt.title('Injury Rate by First Crash Type', fontsize=14, pad=20)\n",
    "    plt.xlim(0, injury_by_type.max() * 100 + 5)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return injury_by_type\n",
    "\n",
    "def plot_injury_rate_by_cause(df, top_n=10):\n",
    "    \"\"\"Plot injury rates by primary contributory cause.\"\"\"\n",
    "    if 'PRIM_CONTRIBUTORY_CAUSE' not in df.columns:\n",
    "        print(\"Primary contributory cause column not found\")\n",
    "        return\n",
    "    \n",
    "    if 'injury_flag' not in df.columns and 'INJURIES_TOTAL' in df.columns:\n",
    "        df = df.copy()\n",
    "        df['injury_flag'] = (df['INJURIES_TOTAL'] > 0).astype(int)\n",
    "    \n",
    "    # First find the top N most frequent causes\n",
    "    top_causes = df['PRIM_CONTRIBUTORY_CAUSE'].value_counts().nlargest(top_n).index\n",
    "    \n",
    "    # Calculate injury rate for each top cause\n",
    "    injury_by_cause = (\n",
    "        df[df['PRIM_CONTRIBUTORY_CAUSE'].isin(top_causes)]\n",
    "        .groupby('PRIM_CONTRIBUTORY_CAUSE')['injury_flag']\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(\n",
    "        x=injury_by_cause.values * 100,\n",
    "        y=injury_by_cause.index,\n",
    "        palette='mako'\n",
    "    )\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('Percent of Crashes with ≥1 Injury', fontsize=12)\n",
    "    plt.ylabel('Primary Contributory Cause', fontsize=12)\n",
    "    plt.title(f'Injury Rate by Top {top_n} Contributory Causes', fontsize=14, pad=20)\n",
    "    plt.xlim(0, injury_by_cause.max() * 100 + 5)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return injury_by_cause\n",
    "def plot_injury_severity_by_weather(df, top_n=6):\n",
    "    \"\"\"Plot proportions of injury severity levels per weather condition.\"\"\"\n",
    "    if 'WEATHER_CONDITION' not in df.columns or 'MOST_SEVERE_INJURY' not in df.columns:\n",
    "        print(\"Required columns not found in DataFrame\")\n",
    "        return\n",
    "    \n",
    "    # Get the top N most common weather conditions\n",
    "    top_weather = df['WEATHER_CONDITION'].value_counts().nlargest(top_n).index.tolist()\n",
    "    \n",
    "    # Filter for only those weather conditions\n",
    "    weather_filtered = df[df['WEATHER_CONDITION'].isin(top_weather)]\n",
    "    \n",
    "    # Build the cross-tab of proportions with better ordering\n",
    "    ct = pd.crosstab(\n",
    "        weather_filtered['WEATHER_CONDITION'],\n",
    "        weather_filtered['MOST_SEVERE_INJURY'],\n",
    "        normalize='index'   # each row sums to 1 → proportions\n",
    "    )\n",
    "    \n",
    "    # Define a custom color palette that's colorblind-friendly\n",
    "    colors = sns.color_palette(\"viridis\", len(ct.columns))\n",
    "    \n",
    "    # Sort rows by severity if 'FATAL' is present\n",
    "    if 'FATAL' in ct.columns:\n",
    "        ct = ct.sort_values(by='FATAL', ascending=False)\n",
    "    \n",
    "    # Plot as a horizontal stacked bar chart with improved formatting\n",
    "    ax = ct.plot(\n",
    "        kind='barh',\n",
    "        stacked=True,\n",
    "        figsize=(12, 8),\n",
    "        color=colors,\n",
    "        linewidth=0.5,\n",
    "        edgecolor='white'  # thin white edge between segments\n",
    "    )\n",
    "    \n",
    "    # Add percentage labels to segments (for segments > 5%)\n",
    "    for i, row in enumerate(ct.values):\n",
    "        xpos = 0\n",
    "        for j, val in enumerate(row):\n",
    "            if val >= 0.05:  # Only label segments that are at least 5%\n",
    "                ax.text(xpos + val/2, i, f'{val:.0%}', \n",
    "                        ha='center', va='center', color='white', fontweight='bold')\n",
    "            xpos += val\n",
    "    \n",
    "    # Improve labels and legend\n",
    "    plt.xlabel('Proportion of Crashes', fontsize=12)\n",
    "    plt.ylabel('Weather Condition', fontsize=12)\n",
    "    plt.title('Crash Severity Distribution by Weather Condition', fontsize=14, pad=20)\n",
    "    plt.legend(title='Most Severe Injury', bbox_to_anchor=(1.05, 1.0))\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ct\n",
    "\n",
    "# === visualization for time series patterns ===\n",
    "def create_time_period_visualizations(df):\n",
    "    \"\"\"Create multiple visualizations for temporal patterns in crashes.\"\"\"\n",
    "    # Set up figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # Define viridis-based color palette\n",
    "    colors = [plt.cm.viridis(0.2), plt.cm.viridis(0.7)]\n",
    "    \n",
    "    # 1. Weekend vs Weekday visualization\n",
    "    if 'is_weekend' in df.columns:\n",
    "        weekend_col = 'is_weekend'\n",
    "    elif 'IS_WEEKEND' in df.columns:\n",
    "        weekend_col = 'IS_WEEKEND'\n",
    "    else:\n",
    "        # Create it if not present\n",
    "        if 'CRASH_DAY_OF_WEEK' in df.columns:\n",
    "            df['is_weekend'] = df['CRASH_DAY_OF_WEEK'].isin([5, 6]).astype(int)\n",
    "            weekend_col = 'is_weekend'\n",
    "        elif 'DAY_OF_WEEK' in df.columns:\n",
    "            df['is_weekend'] = df['DAY_OF_WEEK'].isin(['Saturday', 'Sunday']).astype(int)\n",
    "            weekend_col = 'is_weekend'\n",
    "        else:\n",
    "            weekend_col = None\n",
    "            \n",
    "    if weekend_col:\n",
    "        weekend_counts = df[weekend_col].value_counts().sort_index()\n",
    "        weekend_pct = (weekend_counts / weekend_counts.sum() * 100).round(1)\n",
    "        bar1 = sns.barplot(x=[0, 1], y=weekend_counts.values, ax=axes[0, 0], palette=colors)\n",
    "        axes[0, 0].set_title('Crashes by Day Type', fontsize=14)\n",
    "        axes[0, 0].set_xlabel('Weekend Flag', fontsize=12)\n",
    "        axes[0, 0].set_ylabel('Number of Crashes', fontsize=12)\n",
    "        axes[0, 0].set_xticklabels(['Weekday', 'Weekend'])\n",
    "        # Add count and percentage labels\n",
    "        for i, (count, pct) in enumerate(zip(weekend_counts, weekend_pct)):\n",
    "            axes[0, 0].text(i, count/2, f'{count:,}\\n({pct:.1f}%)',\n",
    "                          ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, \"Weekend data not available\", \n",
    "                       ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "    \n",
    "    # 2. Rush hour visualization\n",
    "    # Create rush hour data if not present\n",
    "    if 'is_rush_hour' not in df.columns and 'IS_RUSH_HOUR' not in df.columns:\n",
    "        if 'CRASH_HOUR' in df.columns:\n",
    "            df['is_rush_hour'] = (\n",
    "                df['CRASH_HOUR'].between(7, 9) | \n",
    "                df['CRASH_HOUR'].between(16, 18)\n",
    "            ).astype(int)\n",
    "            rush_hour_col = 'is_rush_hour'\n",
    "        elif 'HOUR' in df.columns:\n",
    "            df['is_rush_hour'] = (\n",
    "                df['HOUR'].between(7, 9) | \n",
    "                df['HOUR'].between(16, 18)\n",
    "            ).astype(int)\n",
    "            rush_hour_col = 'is_rush_hour'\n",
    "        else:\n",
    "            rush_hour_col = None\n",
    "    else:\n",
    "        rush_hour_col = 'is_rush_hour' if 'is_rush_hour' in df.columns else 'IS_RUSH_HOUR'\n",
    "        \n",
    "    if rush_hour_col:\n",
    "        rush_counts = df[rush_hour_col].value_counts().sort_index()\n",
    "        rush_pct = (rush_counts / rush_counts.sum() * 100).round(1)\n",
    "        bar2 = sns.barplot(x=[0, 1], y=rush_counts.values, ax=axes[0, 1], palette=colors)\n",
    "        axes[0, 1].set_title('Crashes by Time of Day', fontsize=14)\n",
    "        axes[0, 1].set_xlabel('Rush Hour Flag', fontsize=12)\n",
    "        axes[0, 1].set_ylabel('Number of Crashes', fontsize=12)\n",
    "        axes[0, 1].set_xticklabels(['Off Hours', 'Rush Hour'])\n",
    "        # Add count and percentage labels\n",
    "        for i, (count, pct) in enumerate(zip(rush_counts, rush_pct)):\n",
    "            axes[0, 1].text(i, count/2, f'{count:,}\\n({pct:.1f}%)',\n",
    "                          ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, \"Rush hour data not available\", \n",
    "                       ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    \n",
    "    # 3. Night visualization\n",
    "    if 'night_flag' in df.columns:\n",
    "        night_col = 'night_flag'\n",
    "    elif 'IS_NIGHTTIME' in df.columns:\n",
    "        night_col = 'IS_NIGHTTIME'\n",
    "    else:\n",
    "        # Create night flag if lighting condition is available\n",
    "        if 'LIGHTING_CONDITION' in df.columns:\n",
    "            df['night_flag'] = (~df['LIGHTING_CONDITION'].eq('DAYLIGHT')).astype(int)\n",
    "            night_col = 'night_flag'\n",
    "        # Or create based on hour if available\n",
    "        elif 'CRASH_HOUR' in df.columns:\n",
    "            df['night_flag'] = ((df['CRASH_HOUR'] < 6) | (df['CRASH_HOUR'] >= 20)).astype(int)\n",
    "            night_col = 'night_flag'\n",
    "        elif 'HOUR' in df.columns:\n",
    "            df['night_flag'] = ((df['HOUR'] < 6) | (df['HOUR'] >= 20)).astype(int)\n",
    "            night_col = 'night_flag'\n",
    "        else:\n",
    "            night_col = None\n",
    "            \n",
    "    if night_col:\n",
    "        night_counts = df[night_col].value_counts().sort_index()\n",
    "        night_pct = (night_counts / night_counts.sum() * 100).round(1)\n",
    "        bar3 = sns.barplot(x=[0, 1], y=night_counts.values, ax=axes[1, 0], palette=colors)\n",
    "        axes[1, 0].set_title('Crashes by Lighting Condition', fontsize=14)\n",
    "        axes[1, 0].set_xlabel('Night Flag', fontsize=12)\n",
    "        axes[1, 0].set_ylabel('Number of Crashes', fontsize=12)\n",
    "        axes[1, 0].set_xticklabels(['Daylight', 'Night/Dark'])\n",
    "        # Add count and percentage labels\n",
    "        for i, (count, pct) in enumerate(zip(night_counts, night_pct)):\n",
    "            axes[1, 0].text(i, count/2, f'{count:,}\\n({pct:.1f}%)',\n",
    "                          ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, \"Night/day data not available\", \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    \n",
    "    # 4. Speed bin visualization\n",
    "    if 'SPD_BIN' in df.columns:\n",
    "        speed_bin_col = 'SPD_BIN'\n",
    "    else:\n",
    "        # Create speed bins if speed limit is available\n",
    "        if 'POSTED_SPEED_LIMIT' in df.columns:\n",
    "            bins = [0, 20, 30, 40, 50, 60, 100]\n",
    "            labels = ['<20', '20-30', '30-40', '40-50', '50-60', '60+']\n",
    "            df['SPD_BIN'] = pd.cut(df['POSTED_SPEED_LIMIT'], bins=bins, labels=labels, right=False)\n",
    "            speed_bin_col = 'SPD_BIN'\n",
    "        else:\n",
    "            speed_bin_col = None\n",
    "            \n",
    "    if speed_bin_col and not df[speed_bin_col].isna().all():\n",
    "        # Handle categorical speed bins\n",
    "        speed_counts = df[speed_bin_col].value_counts().sort_index()\n",
    "        speed_pct = (speed_counts / speed_counts.sum() * 100).round(1)\n",
    "        # Use more colors for speed bins\n",
    "        speed_colors = plt.cm.viridis(np.linspace(0.1, 0.9, len(speed_counts)))\n",
    "        bar4 = sns.barplot(x=range(len(speed_counts)), y=speed_counts.values, ax=axes[1, 1], palette=speed_colors)\n",
    "        axes[1, 1].set_title('Crashes by Speed Limit', fontsize=14)\n",
    "        axes[1, 1].set_xlabel('Speed Limit (mph)', fontsize=12)\n",
    "        axes[1, 1].set_ylabel('Number of Crashes', fontsize=12)\n",
    "        axes[1, 1].set_xticklabels(speed_counts.index)\n",
    "        # Add count and percentage labels if there's enough space\n",
    "        if len(speed_counts) <= 10:  # Only add text if not too crowded\n",
    "            for i, (count, pct) in enumerate(zip(speed_counts, speed_pct)):\n",
    "                axes[1, 1].text(i, count/2, f'{count:,}\\n({pct:.1f}%)',\n",
    "                             ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, \"Speed bin data not available\", \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    \n",
    "    # Make all subplots have grid lines\n",
    "    for ax in axes.flatten():\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        # Set y-axis to start at 0\n",
    "        ax.set_ylim(bottom=0)\n",
    "    \n",
    "    # Add a main title\n",
    "    fig.suptitle('Crash Distribution by Key Factors', fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "    # === New function for interactive map visualization ===\n",
    "def plot_hex_crash_density(df):\n",
    "    \"\"\"Create a hexbin plot showing crash density on a map.\"\"\"\n",
    "    if 'LATITUDE' not in df.columns or 'LONGITUDE' not in df.columns:\n",
    "        print(\"Coordinates not found in DataFrame\")\n",
    "        return\n",
    "    \n",
    "    # Filter to valid bounds (for Chicago)\n",
    "    mask = (\n",
    "        (df['LATITUDE'] > 41.5) & (df['LATITUDE'] < 42.1) &\n",
    "        (df['LONGITUDE'] > -88.0) & (df['LONGITUDE'] < -87.4)\n",
    "    )\n",
    "    geo = df[mask]\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Use matplotlib colors for normalization\n",
    "    import matplotlib.colors as mcolors\n",
    "    \n",
    "    # Create the hexbin plot\n",
    "    hb = plt.hexbin(\n",
    "        geo['LONGITUDE'],\n",
    "        geo['LATITUDE'],\n",
    "        gridsize=50,                       # Adjust grid size \n",
    "        mincnt=5,                          # Only show bins with ≥5 crashes\n",
    "        cmap='magma',                      # Darker colormap\n",
    "        norm=mcolors.LogNorm(),            # Logarithmic color scaling\n",
    "        linewidths=0.2,                    # Thin lines between hexagons\n",
    "        edgecolors='black',                # Black edges for better contrast\n",
    "        alpha=1.0                          # Full opacity for rich colors\n",
    "    )\n",
    "    \n",
    "    # Add colorbar with formatting\n",
    "    cb = plt.colorbar(hb, fraction=0.046, pad=0.04)\n",
    "    cb.set_label('Crash Count (log scale)', fontsize=12)\n",
    "    cb.ax.tick_params(labelsize=10)\n",
    "    \n",
    "    # Add informative title and labels\n",
    "    plt.title('Crash Density Map of Chicago', fontsize=16, pad=20)\n",
    "    plt.xlabel('Longitude', fontsize=12)\n",
    "    plt.ylabel('Latitude', fontsize=12)\n",
    "    \n",
    "    # Add a summary stats textbox\n",
    "    summary_text = f\"Total Crashes: {len(geo):,}\\nArea: Chicago City Limits\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.7)\n",
    "    plt.annotate(summary_text, xy=(0.05, 0.05), xycoords='axes fraction', \n",
    "                fontsize=10, bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_crashes_by_weekday_donut(df):\n",
    "    \"\"\"Create a donut chart showing crash distribution by day of week.\"\"\"\n",
    "    # Map weekday numbers to names\n",
    "    weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    \n",
    "    # Get weekday data\n",
    "    if 'CRASH_WEEKDAY' in df.columns:\n",
    "        by_wd = df['CRASH_WEEKDAY'].map(lambda x: weekday_names[x] if pd.notna(x) and 0 <= x < 7 else None)\n",
    "        by_wd = by_wd.value_counts().reindex(weekday_names).fillna(0)\n",
    "    elif 'DAY_OF_WEEK' in df.columns:\n",
    "        by_wd = df['DAY_OF_WEEK'].value_counts().reindex(weekday_names).fillna(0)\n",
    "    elif 'CRASH_DAY_OF_WEEK' in df.columns:\n",
    "        by_wd = df['CRASH_DAY_OF_WEEK'].map(lambda x: weekday_names[x-1] if pd.notna(x) and 1 <= x <= 7 else None)\n",
    "        by_wd = by_wd.value_counts().reindex(weekday_names).fillna(0)\n",
    "    else:\n",
    "        print(\"No day of week column found\")\n",
    "        return None\n",
    "    \n",
    "    # Make sure all values are valid numbers (no NaN)\n",
    "    if by_wd.isna().any() or (by_wd == 0).all():\n",
    "        print(\"Warning: Not enough valid day of week data for donut chart\")\n",
    "        return None\n",
    "    \n",
    "    # Get counts and labels\n",
    "    counts = by_wd.values\n",
    "    labels = by_wd.index\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # Create pie chart with a hole\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        counts,\n",
    "        labels=labels,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        pctdistance=0.75,\n",
    "        colors=plt.cm.viridis(np.linspace(0.1, 0.9, len(counts)))\n",
    "    )\n",
    "    \n",
    "    # Draw center circle for the \"donut\" effect\n",
    "    centre_circle = plt.Circle((0, 0), 0.50, fc='white')\n",
    "    ax.add_artist(centre_circle)\n",
    "    \n",
    "    # Equal aspect ratio ensures the pie chart is circular\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    # Add title\n",
    "    plt.title('Proportion of Crashes by Day of Week')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "def analyze_seasonal_patterns(df, column='CRASH_DATE'):\n",
    "    \"\"\"\n",
    "    Analyze seasonal patterns using time series decomposition.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing crash data\n",
    "    column : str\n",
    "        Name of datetime column to use\n",
    "    \"\"\"\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    \n",
    "    # Create monthly time series\n",
    "    if 'CRASH_DATE' not in df.columns and 'CRASH_DATETIME' in df.columns:\n",
    "        column = 'CRASH_DATETIME'\n",
    "    \n",
    "    # Resample to monthly data\n",
    "    monthly_crashes = df.set_index(column).resample('M').size()\n",
    "    \n",
    "    # Fill any missing months with interpolation\n",
    "    monthly_crashes = monthly_crashes.interpolate()\n",
    "    \n",
    "    # Perform seasonal decomposition\n",
    "    decomposition = seasonal_decompose(monthly_crashes, model='additive', period=12)\n",
    "    \n",
    "    # Plot the decomposition\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 12))\n",
    "    \n",
    "    # Original data\n",
    "    decomposition.observed.plot(ax=ax1)\n",
    "    ax1.set_title('Observed', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trend component\n",
    "    decomposition.trend.plot(ax=ax2)\n",
    "    ax2.set_title('Trend', fontsize=14)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Seasonal component\n",
    "    decomposition.seasonal.plot(ax=ax3)\n",
    "    ax3.set_title('Seasonality', fontsize=14)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual component\n",
    "    decomposition.resid.plot(ax=ax4)\n",
    "    ax4.set_title('Residuals', fontsize=14)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate average monthly pattern\n",
    "    monthly_pattern = pd.DataFrame({\n",
    "        'month': range(1, 13),\n",
    "        'seasonal_factor': decomposition.seasonal.groupby(decomposition.seasonal.index.month).mean()\n",
    "    })\n",
    "    \n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(month_names, monthly_pattern['seasonal_factor'], \n",
    "                 color=plt.cm.viridis(np.linspace(0, 1, 12)))\n",
    "    \n",
    "    plt.title('Average Seasonal Effect by Month', fontsize=14)\n",
    "    plt.ylabel('Seasonal Factor (crashes above/below trend)', fontsize=12)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Annotate seasonal factors\n",
    "    for i, v in enumerate(monthly_pattern['seasonal_factor']):\n",
    "        plt.text(i, v + (v > 0) * 5 - (v < 0) * 20, f'{v:.1f}', \n",
    "                ha='center', fontsize=10)\n",
    "    \n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return decomposition, monthly_pattern\n",
    "\n",
    "def analyze_weather_seasonal_effects(df):\n",
    "    \"\"\"\n",
    "    Analyze how weather conditions vary across seasons and affect crash rates.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing crash data with weather information\n",
    "    \"\"\"\n",
    "    if 'WEATHER_CONDITION' not in df.columns or 'SEASON' not in df.columns:\n",
    "        print(\"Required columns not found in DataFrame\")\n",
    "        return\n",
    "    \n",
    "    # Create cross-tabulation of weather conditions by season\n",
    "    weather_by_season = pd.crosstab(df['SEASON'], df['WEATHER_CONDITION'])\n",
    "    \n",
    "    # Calculate proportions\n",
    "    weather_props = weather_by_season.div(weather_by_season.sum(axis=1), axis=0)\n",
    "    \n",
    "    # Select top weather conditions for clarity\n",
    "    top_weather = df['WEATHER_CONDITION'].value_counts().nlargest(5).index\n",
    "    weather_props_filtered = weather_props[top_weather]\n",
    "    \n",
    "    # Plot as stacked bars\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    weather_props_filtered.plot(\n",
    "        kind='bar', \n",
    "        stacked=True, \n",
    "        colormap='viridis',\n",
    "        figsize=(12, 6)\n",
    "    )\n",
    "    \n",
    "    plt.title('Weather Conditions by Season', fontsize=14, pad=20)\n",
    "    plt.xlabel('Season', fontsize=12)\n",
    "    plt.ylabel('Proportion', fontsize=12)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title='Weather Condition', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze crash severity by season and weather\n",
    "    if 'SEVERE' in df.columns:\n",
    "        # Create a pivot table of severity rates by season and weather\n",
    "        severity_pivot = df.pivot_table(\n",
    "            values='SEVERE', \n",
    "            index='SEASON',\n",
    "            columns='WEATHER_CONDITION',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Filter to top weather conditions\n",
    "        severity_pivot_filtered = severity_pivot[top_weather]\n",
    "        \n",
    "        # Plot heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(\n",
    "            severity_pivot_filtered * 100,  # Convert to percentage\n",
    "            annot=True,\n",
    "            fmt='.1f',\n",
    "            cmap='viridis',\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={'label': 'Severe Crash Percentage'}\n",
    "        )\n",
    "        \n",
    "        plt.title('Crash Severity Rate (%) by Season and Weather', fontsize=14, pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return weather_props, severity_pivot if 'SEVERE' in df.columns else None\n",
    "\n",
    "def analyze_hourly_patterns_by_season(df):\n",
    "    \"\"\"\n",
    "    Analyze how hourly crash patterns change across seasons.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing crash data\n",
    "    \"\"\"\n",
    "    if 'CRASH_HOUR' not in df.columns and 'HOUR' not in df.columns:\n",
    "        print(\"Hour column not found in DataFrame\")\n",
    "        return\n",
    "    \n",
    "    if 'SEASON' not in df.columns:\n",
    "        print(\"Season column not found in DataFrame\")\n",
    "        return\n",
    "    \n",
    "    hour_col = 'CRASH_HOUR' if 'CRASH_HOUR' in df.columns else 'HOUR'\n",
    "    \n",
    "    # Create a pivot table of hourly patterns by season\n",
    "    hourly_season = pd.crosstab(df[hour_col], df['SEASON'])\n",
    "    \n",
    "    # Normalize by season total for fair comparison\n",
    "    hourly_season_pct = hourly_season.div(hourly_season.sum())\n",
    "    \n",
    "    # Create a custom line plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Get distinct colors for each season from viridis\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(hourly_season_pct.columns)))\n",
    "    \n",
    "    # Plot each season\n",
    "    for i, season in enumerate(hourly_season_pct.columns):\n",
    "        plt.plot(\n",
    "            hourly_season_pct.index, \n",
    "            hourly_season_pct[season],\n",
    "            label=season,\n",
    "            color=colors[i],\n",
    "            linewidth=2,\n",
    "            marker='o',\n",
    "            markersize=4\n",
    "        )\n",
    "    \n",
    "    # Add shaded regions for standard periods\n",
    "    plt.axvspan(7, 9, alpha=0.1, color='gray', label='_Morning Rush')\n",
    "    plt.axvspan(16, 18, alpha=0.1, color='gray', label='_Evening Rush')\n",
    "    plt.axvspan(0, 5, alpha=0.1, color='darkblue', label='_Night')\n",
    "    plt.axvspan(21, 24, alpha=0.1, color='darkblue', label='_Night')\n",
    "    \n",
    "    # Add annotations for important periods\n",
    "    plt.text(8, max(hourly_season_pct.max()) * 0.95, \"Morning Rush\", \n",
    "             ha='center', va='top', bbox=dict(facecolor='white', alpha=0.7))\n",
    "    plt.text(17, max(hourly_season_pct.max()) * 0.95, \"Evening Rush\", \n",
    "             ha='center', va='top', bbox=dict(facecolor='white', alpha=0.7))\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('Hourly Crash Patterns by Season', fontsize=14, pad=20)\n",
    "    plt.xlabel('Hour of Day', fontsize=12)\n",
    "    plt.ylabel('Proportion of Daily Crashes', fontsize=12)\n",
    "    plt.xticks(range(0, 24, 2))\n",
    "    plt.xlim(-0.5, 23.5)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(title='Season')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate rush hour effect by season\n",
    "    morning_rush = hourly_season.loc[7:9].sum()\n",
    "    evening_rush = hourly_season.loc[16:18].sum()\n",
    "    night_time = hourly_season.loc[[0,1,2,3,4,22,23]].sum()\n",
    "    \n",
    "    rush_effects = pd.DataFrame({\n",
    "        'Morning Rush': morning_rush / hourly_season.sum(),\n",
    "        'Evening Rush': evening_rush / hourly_season.sum(),\n",
    "        'Night Time': night_time / hourly_season.sum()\n",
    "    })\n",
    "    \n",
    "    # Plot rush hour comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    rush_effects.plot(kind='bar', colormap='viridis')\n",
    "    plt.title('Time Period Distribution by Season', fontsize=14, pad=20)\n",
    "    plt.xlabel('Season', fontsize=12)\n",
    "    plt.ylabel('Proportion of Crashes', fontsize=12)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return hourly_season, rush_effects\n",
    "\n",
    "def plot_monthly_yoy_comparison(df, date_col='CRASH_DATE', years=None):\n",
    "    \"\"\"\n",
    "    Compare monthly crash patterns across different years.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing crash data\n",
    "    date_col : str\n",
    "        Name of datetime column\n",
    "    years : list, optional\n",
    "        List of years to include in the comparison\n",
    "    \"\"\"\n",
    "    # Make sure we have a datetime column\n",
    "    if date_col not in df.columns:\n",
    "        if 'CRASH_DATETIME' in df.columns:\n",
    "            date_col = 'CRASH_DATETIME'\n",
    "        else:\n",
    "            print(\"No date column found in DataFrame\")\n",
    "            return\n",
    "    \n",
    "    # Extract year and month\n",
    "    df = df.copy()\n",
    "    df['crash_year'] = df[date_col].dt.year\n",
    "    df['crash_month'] = df[date_col].dt.month\n",
    "    \n",
    "    # Get available years\n",
    "    available_years = sorted(df['crash_year'].unique())\n",
    "    \n",
    "    # Filter years if specified\n",
    "    if years is not None:\n",
    "        plot_years = [y for y in years if y in available_years]\n",
    "        if not plot_years:\n",
    "            print(f\"None of the specified years {years} found in data.\")\n",
    "            print(f\"Available years: {available_years}\")\n",
    "            return\n",
    "    else:\n",
    "        # Use latest years (up to 5) if not specified\n",
    "        plot_years = available_years[-5:] if len(available_years) > 5 else available_years\n",
    "    \n",
    "    # Create monthly counts for each year\n",
    "    monthly_counts = df[df['crash_year'].isin(plot_years)].groupby(\n",
    "        ['crash_year', 'crash_month']).size().unstack(level=0)\n",
    "    \n",
    "    # Ensure all months are represented\n",
    "    full_index = pd.Index(range(1, 13), name='crash_month')\n",
    "    monthly_counts = monthly_counts.reindex(full_index, fill_value=0)\n",
    "    \n",
    "    # Month names\n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    \n",
    "    # Get colors from viridis\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(plot_years)))\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot each year\n",
    "    for i, year in enumerate(plot_years):\n",
    "        if year in monthly_counts.columns:\n",
    "            plt.plot(\n",
    "                range(1, 13), \n",
    "                monthly_counts[year],\n",
    "                label=str(year),\n",
    "                color=colors[i],\n",
    "                marker='o',\n",
    "                linewidth=2,\n",
    "                markersize=6\n",
    "            )\n",
    "    \n",
    "    # Add seasonal bands\n",
    "    seasons = [\n",
    "        {\"name\": \"Winter\", \"months\": [12, 1, 2], \"color\": \"#e6f2ff\"},\n",
    "        {\"name\": \"Spring\", \"months\": [3, 4, 5], \"color\": \"#e6ffe6\"},\n",
    "        {\"name\": \"Summer\", \"months\": [6, 7, 8], \"color\": \"#ffebcc\"},\n",
    "        {\"name\": \"Fall\", \"months\": [9, 10, 11], \"color\": \"#f2e6ff\"}\n",
    "    ]\n",
    "    \n",
    "    for season in seasons:\n",
    "        for month in season[\"months\"]:\n",
    "            plt.axvspan(month-0.5, month+0.5, alpha=0.2, color=season[\"color\"])\n",
    "            \n",
    "        # Only add label for first month of each season\n",
    "        plt.text(\n",
    "            season[\"months\"][0], \n",
    "            monthly_counts.max().max() * 0.95,\n",
    "            season[\"name\"],\n",
    "            ha='center',\n",
    "            bbox=dict(facecolor='white', alpha=0.7)\n",
    "        )\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('Monthly Crashes by Year', fontsize=14, pad=20)\n",
    "    plt.xlabel('Month', fontsize=12)\n",
    "    plt.ylabel('Number of Crashes', fontsize=12)\n",
    "    plt.xticks(range(1, 13), month_names)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(title='Year')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and plot percent changes\n",
    "    if len(plot_years) >= 2:\n",
    "        # Get year-over-year changes\n",
    "        pct_changes = monthly_counts.pct_change(axis=1) * 100\n",
    "        \n",
    "        # Plot heatmap of changes\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(\n",
    "            pct_changes.T,  # Transpose for better visualization\n",
    "            annot=True,\n",
    "            fmt='.1f',\n",
    "            cmap='RdBu_r',  # Red-Blue diverging colormap\n",
    "            center=0,       # Center color map at zero\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={'label': 'Percent Change (%)'}\n",
    "        )\n",
    "        \n",
    "        plt.title('Year-over-Year Percent Change by Month', fontsize=14, pad=20)\n",
    "        plt.xlabel('Month', fontsize=12)\n",
    "        plt.ylabel('Year Transition', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return monthly_counts\n",
    "def plot_injury_rate_by_crash_type(df, top_n=10):\n",
    "    \"\"\"Plot injury rates by crash type.\"\"\"\n",
    "    if 'FIRST_CRASH_TYPE' not in df.columns or 'injury_flag' not in df.columns:\n",
    "        print(\"Required columns not found in DataFrame\")\n",
    "        if 'INJURIES_TOTAL' in df.columns and 'FIRST_CRASH_TYPE' in df.columns:\n",
    "            df = df.copy()\n",
    "            df['injury_flag'] = (df['INJURIES_TOTAL'] > 0).astype(int)\n",
    "        else:\n",
    "            return\n",
    "            \n",
    "    # Compute injury rate by crash type\n",
    "    injury_by_type = (\n",
    "        df\n",
    "        .groupby('FIRST_CRASH_TYPE')['injury_flag']\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    # Limit to top N types\n",
    "    if top_n is not None:\n",
    "        injury_by_type = injury_by_type.head(top_n)\n",
    "        \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # Use standardized color palette\n",
    "    sns.barplot(\n",
    "        x=injury_by_type.values * 100,  # convert to percent\n",
    "        y=injury_by_type.index,\n",
    "        palette='rocket'\n",
    "    )\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('Percent of Crashes with ≥1 Injury', fontsize=12)\n",
    "    plt.ylabel('Crash Type', fontsize=12)\n",
    "    plt.title('Injury Rate by First Crash Type', fontsize=14, pad=20)\n",
    "    plt.xlim(0, injury_by_type.max() * 100 + 5)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return injury_by_type\n",
    "def plot_injury_rate_by_cause(df, top_n=10):\n",
    "    \"\"\"Plot injury rates by primary contributory cause.\"\"\"\n",
    "    if 'PRIM_CONTRIBUTORY_CAUSE' not in df.columns:\n",
    "        print(\"Primary contributory cause column not found\")\n",
    "        return\n",
    "        \n",
    "    if 'injury_flag' not in df.columns and 'INJURIES_TOTAL' in df.columns:\n",
    "        df = df.copy()\n",
    "        df['injury_flag'] = (df['INJURIES_TOTAL'] > 0).astype(int)\n",
    "    \n",
    "    # First find the top N most frequent causes\n",
    "    top_causes = df['PRIM_CONTRIBUTORY_CAUSE'].value_counts().nlargest(top_n).index\n",
    "    \n",
    "    # Calculate injury rate for each top cause\n",
    "    injury_by_cause = (\n",
    "        df[df['PRIM_CONTRIBUTORY_CAUSE'].isin(top_causes)]\n",
    "        .groupby('PRIM_CONTRIBUTORY_CAUSE')['injury_flag']\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(\n",
    "        x=injury_by_cause.values * 100,\n",
    "        y=injury_by_cause.index,\n",
    "        palette='mako'\n",
    "    )\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('Percent of Crashes with ≥1 Injury', fontsize=12)\n",
    "    plt.ylabel('Primary Contributory Cause', fontsize=12)\n",
    "    plt.title(f'Injury Rate by Top {top_n} Contributory Causes', fontsize=14, pad=20)\n",
    "    plt.xlim(0, injury_by_cause.max() * 100 + 5)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return injury_by_cause\n",
    "def plot_injury_severity_by_weather(df, top_n=6):\n",
    "    \"\"\"Plot proportions of injury severity levels per weather condition.\"\"\"\n",
    "    if 'WEATHER_CONDITION' not in df.columns or 'MOST_SEVERE_INJURY' not in df.columns:\n",
    "        print(\"Required columns not found in DataFrame\")\n",
    "        return\n",
    "        \n",
    "    # Get the top N most common weather conditions\n",
    "    top_weather = df['WEATHER_CONDITION'].value_counts().nlargest(top_n).index.tolist()\n",
    "    \n",
    "    # Filter for only those weather conditions\n",
    "    weather_filtered = df[df['WEATHER_CONDITION'].isin(top_weather)]\n",
    "    \n",
    "    # Build the cross-tab of proportions with better ordering\n",
    "    ct = pd.crosstab(\n",
    "        weather_filtered['WEATHER_CONDITION'],\n",
    "        weather_filtered['MOST_SEVERE_INJURY'],\n",
    "        normalize='index'  # each row sums to 1 → proportions\n",
    "    )\n",
    "    \n",
    "    # Define a custom color palette that's colorblind-friendly\n",
    "    colors = sns.color_palette(\"viridis\", len(ct.columns))\n",
    "    \n",
    "    # Sort rows by severity if 'FATAL' is present\n",
    "    if 'FATAL' in ct.columns:\n",
    "        ct = ct.sort_values(by='FATAL', ascending=False)\n",
    "        \n",
    "    # Plot as a horizontal stacked bar chart with improved formatting\n",
    "    ax = ct.plot(\n",
    "        kind='barh',\n",
    "        stacked=True,\n",
    "        figsize=(12, 8),\n",
    "        color=colors,\n",
    "        linewidth=0.5,\n",
    "        edgecolor='white'  # thin white edge between segments\n",
    "    )\n",
    "    \n",
    "    # Add percentage labels to segments (for segments > 5%)\n",
    "    for i, row in enumerate(ct.values):\n",
    "        xpos = 0\n",
    "        for j, val in enumerate(row):\n",
    "            if val >= 0.05:  # Only label segments that are at least 5%\n",
    "                ax.text(xpos + val/2, i, f'{val:.0%}',\n",
    "                      ha='center', va='center', color='white', fontweight='bold')\n",
    "            xpos += val\n",
    "    \n",
    "    # Improve labels and legend\n",
    "    plt.xlabel('Proportion of Crashes', fontsize=12)\n",
    "    plt.ylabel('Weather Condition', fontsize=12)\n",
    "    plt.title('Crash Severity Distribution by Weather Condition', fontsize=14, pad=20)\n",
    "    plt.legend(title='Most Severe Injury', bbox_to_anchor=(1.05, 1.0))\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ct\n",
    "def plot_hex_crash_density(df):\n",
    "    \"\"\"Create a hexbin plot showing crash density on a map.\"\"\"\n",
    "    if 'LATITUDE' not in df.columns or 'LONGITUDE' not in df.columns:\n",
    "        print(\"Coordinates not found in DataFrame\")\n",
    "        return\n",
    "    \n",
    "    # Filter to valid bounds (for Chicago)\n",
    "    mask = (\n",
    "        (df['LATITUDE'] > 41.5) & (df['LATITUDE'] < 42.1) &\n",
    "        (df['LONGITUDE'] > -88.0) & (df['LONGITUDE'] < -87.4)\n",
    "    )\n",
    "    geo = df[mask]\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # Use matplotlib colors for normalization\n",
    "    import matplotlib.colors as mcolors\n",
    "    \n",
    "    # Create the hexbin plot\n",
    "    hb = plt.hexbin(\n",
    "        geo['LONGITUDE'],\n",
    "        geo['LATITUDE'],\n",
    "        gridsize=50,  # Adjust grid size\n",
    "        mincnt=5,     # Only show bins with ≥5 crashes\n",
    "        cmap='magma', # Darker colormap\n",
    "        norm=mcolors.LogNorm(),  # Logarithmic color scaling\n",
    "        linewidths=0.2,         # Thin lines between hexagons\n",
    "        edgecolors='black',     # Black edges for better contrast\n",
    "        alpha=1.0              # Full opacity for rich colors\n",
    "    )\n",
    "    \n",
    "    # Add colorbar with formatting\n",
    "    cb = plt.colorbar(hb, fraction=0.046, pad=0.04)\n",
    "    cb.set_label('Crash Count (log scale)', fontsize=12)\n",
    "    cb.ax.tick_params(labelsize=10)\n",
    "    \n",
    "    # Add informative title and labels\n",
    "    plt.title('Crash Density Map of Chicago', fontsize=16, pad=20)\n",
    "    plt.xlabel('Longitude', fontsize=12)\n",
    "    plt.ylabel('Latitude', fontsize=12)\n",
    "    \n",
    "    # Add a summary stats textbox\n",
    "    summary_text = f\"Total Crashes: {len(geo):,}\\nArea: Chicago City Limits\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.7)\n",
    "    plt.annotate(summary_text, xy=(0.05, 0.05), xycoords='axes fraction',\n",
    "                fontsize=10, bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def analyze_seasonal_factors(df, include_weather=True):\n",
    "    \"\"\"\n",
    "    Analyze seasonal factors affecting crash patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing crash data\n",
    "    include_weather : bool\n",
    "        Whether to include weather analysis\n",
    "    \"\"\"\n",
    "    # Create a copy for analysis\n",
    "    df_analysis = df.copy()\n",
    "    \n",
    "    # Make sure we have required columns\n",
    "    if 'MONTH' not in df_analysis.columns and 'CRASH_MONTH' in df_analysis.columns:\n",
    "        df_analysis['MONTH'] = df_analysis['CRASH_MONTH']\n",
    "    \n",
    "    if 'SEASON' not in df_analysis.columns and 'MONTH' in df_analysis.columns:\n",
    "        df_analysis['SEASON'] = df_analysis['MONTH'].apply(get_season)\n",
    "    \n",
    "    # Group data by month\n",
    "    monthly_stats = df_analysis.groupby('MONTH').agg({\n",
    "        'CRASH_RECORD_ID': 'count',\n",
    "        'INJURIES_TOTAL': 'sum',\n",
    "        'SEVERE': 'mean' if 'SEVERE' in df_analysis.columns else lambda x: 0,\n",
    "        'BAD_WEATHER': 'mean' if 'BAD_WEATHER' in df_analysis.columns else lambda x: 0,\n",
    "        'BAD_SURFACE': 'mean' if 'BAD_SURFACE' in df_analysis.columns else lambda x: 0,\n",
    "        'IS_WEEKEND': 'mean' if 'IS_WEEKEND' in df_analysis.columns else lambda x: 0\n",
    "    }).reset_index()\n",
    "    \n",
    "    monthly_stats['AVG_INJURIES_PER_CRASH'] = monthly_stats['INJURIES_TOTAL'] / monthly_stats['CRASH_RECORD_ID']\n",
    "    \n",
    "    # Add month names\n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    monthly_stats['MONTH_NAME'] = monthly_stats['MONTH'].apply(lambda x: month_names[x-1])\n",
    "    \n",
    "    # Create a multi-metric comparison plot\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
    "    \n",
    "    # Plot crash counts\n",
    "    axes[0].bar(\n",
    "        monthly_stats['MONTH'], \n",
    "        monthly_stats['CRASH_RECORD_ID'],\n",
    "        color=plt.cm.viridis(np.linspace(0, 1, 12))\n",
    "    )\n",
    "    axes[0].set_title('Monthly Crash Counts', fontsize=12)\n",
    "    axes[0].set_ylabel('Number of Crashes', fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot injury metrics\n",
    "    axes[1].plot(\n",
    "        monthly_stats['MONTH'], \n",
    "        monthly_stats['AVG_INJURIES_PER_CRASH'], \n",
    "        'o-',\n",
    "        color=VIRIDIS_COLORS['main'],\n",
    "        linewidth=2\n",
    "    )\n",
    "    axes[1].set_title('Average Injuries per Crash', fontsize=12)\n",
    "    axes[1].set_ylabel('Avg Injuries', fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot severity rate\n",
    "    severity_color = VIRIDIS_COLORS['tertiary']\n",
    "    axes[2].plot(\n",
    "        monthly_stats['MONTH'], \n",
    "        monthly_stats['SEVERE'] * 100, \n",
    "        'o-',\n",
    "        color=severity_color,\n",
    "        linewidth=2,\n",
    "        label='Severity Rate'\n",
    "    )\n",
    "    axes[2].set_title('Crash Severity Rate (%)', fontsize=12)\n",
    "    axes[2].set_ylabel('Percent Severe', fontsize=10)\n",
    "    axes[2].set_ylim(0, min(100, monthly_stats['SEVERE'].max() * 100 * 1.2))\n",
    "    \n",
    "    # Add weather factors if available\n",
    "    if include_weather and 'BAD_WEATHER' in monthly_stats.columns:\n",
    "        ax2 = axes[2].twinx()\n",
    "        ax2.plot(\n",
    "            monthly_stats['MONTH'], \n",
    "            monthly_stats['BAD_WEATHER'] * 100, \n",
    "            's--',\n",
    "            color='darkblue',\n",
    "            alpha=0.7,\n",
    "            label='Bad Weather'\n",
    "        )\n",
    "        if 'BAD_SURFACE' in monthly_stats.columns:\n",
    "            ax2.plot(\n",
    "                monthly_stats['MONTH'], \n",
    "                monthly_stats['BAD_SURFACE'] * 100, \n",
    "                '^--',\n",
    "                color='darkred',\n",
    "                alpha=0.7,\n",
    "                label='Bad Surface'\n",
    "            )\n",
    "        ax2.set_ylabel('Weather Factors (%)', fontsize=10)\n",
    "        ax2.legend(loc='upper right')\n",
    "    \n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].legend(loc='upper left')\n",
    "    \n",
    "    # Set shared x-axis labels\n",
    "    axes[2].set_xlabel('Month', fontsize=12)\n",
    "    axes[2].set_xticks(range(1, 13))\n",
    "    axes[2].set_xticklabels(month_names)\n",
    "    \n",
    "    # Add seasonal background\n",
    "    for ax in axes:\n",
    "        # Winter (Dec-Feb)\n",
    "        ax.axvspan(0.5, 2.5, alpha=0.1, color='#e6f2ff')\n",
    "        ax.axvspan(11.5, 12.5, alpha=0.1, color='#e6f2ff')\n",
    "        \n",
    "        # Spring (Mar-May)\n",
    "        ax.axvspan(2.5, 5.5, alpha=0.1, color='#e6ffe6')\n",
    "        \n",
    "        # Summer (Jun-Aug)\n",
    "        ax.axvspan(5.5, 8.5, alpha=0.1, color='#ffebcc')\n",
    "        \n",
    "        # Fall (Sep-Nov)\n",
    "        ax.axvspan(8.5, 11.5, alpha=0.1, color='#f2e6ff')\n",
    "    \n",
    "    # Add season labels to top plot\n",
    "    for i, season in enumerate(['Winter', 'Spring', 'Summer', 'Fall']):\n",
    "        month_pos = [1, 4, 7, 10][i]  # Middle month of each season\n",
    "        axes[0].text(\n",
    "            month_pos, \n",
    "            monthly_stats['CRASH_RECORD_ID'].max() * 0.9,\n",
    "            season,\n",
    "            ha='center',\n",
    "            bbox=dict(facecolor='white', alpha=0.7)\n",
    "        )\n",
    "    \n",
    "    plt.suptitle('Seasonal Patterns in Crash Metrics', fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    plt.show()\n",
    "    \n",
    "    return monthly_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343b96b",
   "metadata": {},
   "source": [
    "#### Part 6: Feature Engineering & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Feature Engineering & Selection ===\n",
    "def select_features(X, y, methods=None):\n",
    "    \"\"\"\n",
    "    Select features using multiple methods and return a consensus.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pandas.DataFrame\n",
    "        Feature matrix\n",
    "    y : pandas.Series\n",
    "        Target variable\n",
    "    methods : list, optional\n",
    "        List of methods to use ('mutual_info', 'rfe', 'permutation')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with results from each method and consensus features\n",
    "    \"\"\"\n",
    "\n",
    "    if methods is None:\n",
    "        methods = ['mutual_info', 'rfe', 'permutation']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Mutual Information (Filter Method)\n",
    "    if 'mutual_info' in methods:\n",
    "        mi = mutual_info_classif(X, y, random_state=random_seed )\n",
    "        mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
    "        results['mutual_info'] = mi_series\n",
    "    \n",
    "    # 2. Recursive Feature Elimination (Wrapper Method)\n",
    "    if 'rfe' in methods:\n",
    "        lr = LogisticRegression(solver='liblinear', max_iter=1000, random_state=random_seed )\n",
    "        rfe = RFE(lr, n_features_to_select=10, step=1)\n",
    "        rfe.fit(X, y)\n",
    "        selected = pd.Series(rfe.support_, index=X.columns)\n",
    "        results['rfe'] = selected[selected].index.tolist()\n",
    "    \n",
    "    # 3. Permutation Importance (Embedded Method)\n",
    "    if 'permutation' in methods:\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=random_seed , n_jobs=-1)\n",
    "        rf.fit(X, y)\n",
    "        perm = permutation_importance(rf, X, y, n_repeats=5, random_state=random_seed , n_jobs=-1)\n",
    "        perm_series = pd.Series(perm.importances_mean, index=X.columns).sort_values(ascending=False)\n",
    "        results['permutation'] = perm_series\n",
    "    \n",
    "    # 4. Create consensus feature set\n",
    "    consensus_features = []\n",
    "    \n",
    "    # Add top 5 from mutual info\n",
    "    if 'mutual_info' in results:\n",
    "        consensus_features.extend(results['mutual_info'].nlargest(5).index.tolist())\n",
    "    \n",
    "    # Add all from RFE\n",
    "    if 'rfe' in results:\n",
    "        consensus_features.extend(results['rfe'])\n",
    "    \n",
    "    # Add top 5 from permutation\n",
    "    if 'permutation' in results:\n",
    "        consensus_features.extend(results['permutation'].nlargest(5).index.tolist())\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    consensus_features = list(dict.fromkeys(consensus_features))\n",
    "    results['consensus'] = consensus_features\n",
    "    \n",
    "    return results\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create engineered features for analysis and modeling.\"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # 1. Original time-based flags\n",
    "    if 'CRASH_WEEKDAY' in df_eng.columns:\n",
    "        df_eng['is_weekend'] = df_eng['CRASH_WEEKDAY'].isin([5, 6]).astype(int)\n",
    "    elif 'CRASH_DAY_OF_WEEK' in df_eng.columns:\n",
    "        df_eng['is_weekend'] = df_eng['CRASH_DAY_OF_WEEK'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    if 'CRASH_HOUR' in df_eng.columns:\n",
    "        df_eng['is_rush_hour'] = (\n",
    "            df_eng['CRASH_HOUR'].between(7, 9) |\n",
    "            df_eng['CRASH_HOUR'].between(16, 18)\n",
    "        ).astype(int)\n",
    "    \n",
    "    # 2. Original environmental conditions\n",
    "    if 'LIGHTING_CONDITION' in df_eng.columns:\n",
    "        df_eng['night_flag'] = (~df_eng['LIGHTING_CONDITION'].eq('DAYLIGHT')).astype(int)\n",
    "    \n",
    "    # 3. Original speed limit bins\n",
    "    if 'POSTED_SPEED_LIMIT' in df_eng.columns:\n",
    "        bins = [0, 20, 30, 40, 50, 60, 100]\n",
    "        labels = ['<20', '20-30', '30-40', '40-50', '50-60', '60+']\n",
    "        df_eng['SPD_BIN'] = pd.cut(df_eng['POSTED_SPEED_LIMIT'],\n",
    "                                   bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # 4. Original target creation\n",
    "    if 'INJURIES_TOTAL' in df_eng.columns:\n",
    "        df_eng['injury_flag'] = (df_eng['INJURIES_TOTAL'] > 0).astype(int)\n",
    "    \n",
    "    # 5. Original weather conditions\n",
    "    if 'WEATHER_CONDITION' in df_eng.columns:\n",
    "        bad_weather = ['RAIN', 'SNOW', 'SLEET', 'FREEZING RAIN']\n",
    "        df_eng['BAD_WEATHER'] = df_eng['WEATHER_CONDITION'].isin(bad_weather).astype(int)\n",
    "    \n",
    "    # 6. Original road surface conditions\n",
    "    if 'ROADWAY_SURFACE_COND' in df_eng.columns:\n",
    "        bad_surface = ['ICE', 'SNOW OR SLUSH', 'WET']\n",
    "        df_eng['BAD_SURFACE'] = df_eng['ROADWAY_SURFACE_COND'].isin(bad_surface).astype(int)\n",
    "    \n",
    "    # 7. Original location type\n",
    "    if 'FIRST_CRASH_TYPE' in df_eng.columns:\n",
    "        df_eng['IS_VULNERABLE'] = df_eng['FIRST_CRASH_TYPE'].isin(\n",
    "            ['PEDESTRIAN', 'PEDALCYCLIST']).astype(int)\n",
    "    \n",
    "    # NEW FEATURES BELOW\n",
    "    # 8. Temporal interaction features\n",
    "    if 'CRASH_HOUR' in df_eng.columns and 'is_weekend' in df_eng.columns:\n",
    "        # Weekend evening/night interaction\n",
    "        df_eng['WEEKEND_NIGHT'] = (\n",
    "            (df_eng['is_weekend'] == 1) &\n",
    "            ((df_eng['CRASH_HOUR'] < 6) | (df_eng['CRASH_HOUR'] >= 20))\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Rush hour specific to weekdays\n",
    "        df_eng['WEEKDAY_RUSH_HOUR'] = (\n",
    "            (df_eng['is_weekend'] == 0) &\n",
    "            ((df_eng['CRASH_HOUR'].between(7, 9)) | (df_eng['CRASH_HOUR'].between(16, 18)))\n",
    "        ).astype(int)\n",
    "    \n",
    "    # 9. Weather and time interactions\n",
    "    if 'BAD_WEATHER' in df_eng.columns and 'night_flag' in df_eng.columns:\n",
    "        # Bad weather during night\n",
    "        df_eng['NIGHT_BAD_WEATHER'] = (\n",
    "            (df_eng['BAD_WEATHER'] == 1) &\n",
    "            (df_eng['night_flag'] == 1)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Bad weather during rush hour\n",
    "        if 'is_rush_hour' in df_eng.columns:\n",
    "            df_eng['RUSH_HOUR_BAD_WEATHER'] = (\n",
    "                (df_eng['BAD_WEATHER'] == 1) &\n",
    "                (df_eng['is_rush_hour'] == 1)\n",
    "            ).astype(int)\n",
    "    \n",
    "    # 10. Speed-related features\n",
    "    if 'POSTED_SPEED_LIMIT' in df_eng.columns:\n",
    "        # High speed indicator (over 40 mph)\n",
    "        df_eng['HIGH_SPEED_ROAD'] = (df_eng['POSTED_SPEED_LIMIT'] > 40).astype(int)\n",
    "        \n",
    "        # Speed and weather interaction\n",
    "        if 'BAD_WEATHER' in df_eng.columns:\n",
    "            df_eng['HIGH_SPEED_BAD_WEATHER'] = (\n",
    "                (df_eng['HIGH_SPEED_ROAD'] == 1) &\n",
    "                (df_eng['BAD_WEATHER'] == 1)\n",
    "            ).astype(int)\n",
    "        \n",
    "        # Speed and intersection interaction\n",
    "        if 'AT_INTERSECTION' in df_eng.columns:\n",
    "            df_eng['HIGH_SPEED_INTERSECTION'] = (\n",
    "                (df_eng['HIGH_SPEED_ROAD'] == 1) &\n",
    "                (df_eng['AT_INTERSECTION'] == 1)\n",
    "            ).astype(int)\n",
    "    \n",
    "    # 11. Vehicle count and crash complexity\n",
    "    if 'NUM_UNITS' in df_eng.columns:\n",
    "        # Multi-vehicle crash (3+ vehicles)\n",
    "        df_eng['MULTI_VEHICLE_CRASH'] = (df_eng['NUM_UNITS'] >= 3).astype(int)\n",
    "        # Single vehicle crash\n",
    "        df_eng['SINGLE_VEHICLE_CRASH'] = (df_eng['NUM_UNITS'] == 1).astype(int)\n",
    "    \n",
    "    # 12. Vulnerable road user features\n",
    "    if 'IS_VULNERABLE' in df_eng.columns:\n",
    "        # Vulnerable user in high-speed area\n",
    "        if 'HIGH_SPEED_ROAD' in df_eng.columns:\n",
    "            df_eng['VULNERABLE_HIGH_SPEED'] = (\n",
    "                (df_eng['IS_VULNERABLE'] == 1) &\n",
    "                (df_eng['HIGH_SPEED_ROAD'] == 1)\n",
    "            ).astype(int)\n",
    "        \n",
    "        # Vulnerable user at night\n",
    "        if 'night_flag' in df_eng.columns:\n",
    "            df_eng['VULNERABLE_NIGHT'] = (\n",
    "                (df_eng['IS_VULNERABLE'] == 1) &\n",
    "                (df_eng['night_flag'] == 1)\n",
    "            ).astype(int)\n",
    "    \n",
    "    # 13. Seasonal features\n",
    "    if 'SEASON' in df_eng.columns:\n",
    "        # One-hot encode season\n",
    "        season_dummies = pd.get_dummies(df_eng['SEASON'], prefix='SEASON')\n",
    "        df_eng = pd.concat([df_eng, season_dummies], axis=1)\n",
    "        \n",
    "        # Winter conditions interaction (winter and bad weather/surface)\n",
    "        if 'BAD_SURFACE' in df_eng.columns and 'SEASON_Winter' in df_eng.columns:\n",
    "            df_eng['WINTER_BAD_SURFACE'] = (\n",
    "                (df_eng['SEASON_Winter'] == 1) &\n",
    "                (df_eng['BAD_SURFACE'] == 1)\n",
    "            ).astype(int)\n",
    "    \n",
    "    # 14. Time since previous crash (if timestamp information is available)\n",
    "    if 'CRASH_DATETIME' in df_eng.columns:\n",
    "        # Sort by datetime\n",
    "        df_temp = df_eng.sort_values('CRASH_DATETIME')\n",
    "        # Calculate time difference in hours\n",
    "        df_temp['TIME_SINCE_PREV_CRASH'] = df_temp['CRASH_DATETIME'].diff().dt.total_seconds() / 3600\n",
    "        # Add back to original dataframe\n",
    "        df_eng['TIME_SINCE_PREV_CRASH'] = df_temp['TIME_SINCE_PREV_CRASH']\n",
    "        # Fill NaN for first crash (using direct assignment instead of inplace)\n",
    "        df_eng['TIME_SINCE_PREV_CRASH'] = df_eng['TIME_SINCE_PREV_CRASH'].fillna(24)  # Assume 24 hours for first crash\n",
    "        # Create bins for time since previous crash\n",
    "        df_eng['RECENT_CRASH_AREA'] = (df_eng['TIME_SINCE_PREV_CRASH'] < 2).astype(int)\n",
    "    \n",
    "    # 15. Holiday indicators (optional - handle gracefully if holidays module not available)\n",
    "    if 'CRASH_DATE' in df_eng.columns:\n",
    "        try:\n",
    "            import holidays\n",
    "            us_holidays = holidays.US()\n",
    "            # Check if crash occurred on a holiday\n",
    "            df_eng['IS_HOLIDAY'] = df_eng['CRASH_DATE'].apply(lambda x: x in us_holidays).astype(int)\n",
    "            # Check if crash occurred day before or after holiday\n",
    "            df_eng['NEAR_HOLIDAY'] = df_eng['CRASH_DATE'].apply(\n",
    "                lambda x: (x + pd.Timedelta(days=1) in us_holidays) or\n",
    "                         (x - pd.Timedelta(days=1) in us_holidays)\n",
    "            ).astype(int)\n",
    "        except ImportError:\n",
    "            print(\"Note: 'holidays' module not available - holiday features will not be created\")\n",
    "            df_eng['IS_HOLIDAY'] = 0  # Default values when module not available\n",
    "            df_eng['NEAR_HOLIDAY'] = 0\n",
    "    \n",
    "    # 16. Road condition and vehicle interactions\n",
    "    if 'ROADWAY_SURFACE_COND' in df_eng.columns and 'NUM_UNITS' in df_eng.columns:\n",
    "        # Multiple vehicles on bad road surface\n",
    "        if 'BAD_SURFACE' in df_eng.columns:\n",
    "            df_eng['MULTI_VEHICLE_BAD_SURFACE'] = (\n",
    "                (df_eng['NUM_UNITS'] >= 2) &\n",
    "                (df_eng['BAD_SURFACE'] == 1)\n",
    "            ).astype(int)\n",
    "    \n",
    "    # 17. Time of year and daylight interaction\n",
    "    if 'CRASH_MONTH' in df_eng.columns and 'night_flag' in df_eng.columns:\n",
    "        # Winter months with darkness\n",
    "        winter_months = [11, 12, 1, 2]\n",
    "        df_eng['WINTER_DARKNESS'] = (\n",
    "            (df_eng['CRASH_MONTH'].isin(winter_months)) &\n",
    "            (df_eng['night_flag'] == 1)\n",
    "        ).astype(int)\n",
    "    \n",
    "    # 18. Traffic signals and intersection features\n",
    "    if 'TRAFFIC_CONTROL_DEVICE' in df_eng.columns:\n",
    "        # No traffic control at intersection\n",
    "        if 'AT_INTERSECTION' in df_eng.columns:\n",
    "            no_signal = ['NONE', 'NO CONTROLS']\n",
    "            df_eng['UNCONTROLLED_INTERSECTION'] = (\n",
    "                (df_eng['AT_INTERSECTION'] == 1) &\n",
    "                (df_eng['TRAFFIC_CONTROL_DEVICE'].isin(no_signal))\n",
    "            ).astype(int)\n",
    "    \n",
    "    # 19. Age-based features if available\n",
    "    if 'DRIVER_AGE' in df_eng.columns:\n",
    "        # Young driver indicator\n",
    "        df_eng['YOUNG_DRIVER'] = (df_eng['DRIVER_AGE'] < 25).astype(int)\n",
    "        # Senior driver indicator\n",
    "        df_eng['SENIOR_DRIVER'] = (df_eng['DRIVER_AGE'] >= 65).astype(int)\n",
    "        \n",
    "        # Young driver at night\n",
    "        if 'night_flag' in df_eng.columns:\n",
    "            df_eng['YOUNG_DRIVER_NIGHT'] = (\n",
    "                (df_eng['YOUNG_DRIVER'] == 1) &\n",
    "                (df_eng['night_flag'] == 1)\n",
    "            ).astype(int)\n",
    "    \n",
    "    # 20. Alcohol and time features\n",
    "    if 'ALCOHOL_INVOLVED' in df_eng.columns and 'CRASH_HOUR' in df_eng.columns:\n",
    "        # Late night alcohol crashes\n",
    "        df_eng['LATE_NIGHT_ALCOHOL'] = (\n",
    "            (df_eng['ALCOHOL_INVOLVED'] == 1) &\n",
    "            ((df_eng['CRASH_HOUR'] >= 22) | (df_eng['CRASH_HOUR'] <= 4))\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Weekend alcohol crashes\n",
    "        if 'is_weekend' in df_eng.columns:\n",
    "            df_eng['WEEKEND_ALCOHOL'] = (\n",
    "                (df_eng['ALCOHOL_INVOLVED'] == 1) &\n",
    "                (df_eng['is_weekend'] == 1)\n",
    "            ).astype(int)\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "def analyze_model_with_shap(model, X_test, feature_names=None):\n",
    "    \"\"\"\n",
    "    Analyze model predictions using SHAP values for interpretability.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : trained model instance\n",
    "        The model to analyze\n",
    "    X_test : pandas.DataFrame or numpy.ndarray\n",
    "        Test feature matrix\n",
    "    feature_names : list, optional\n",
    "        List of feature names if X_test is not a DataFrame\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing SHAP values and feature importance\n",
    "    \"\"\"\n",
    "\n",
    "    # Get feature names\n",
    "    if feature_names is None and hasattr(X_test, 'columns'):\n",
    "        feature_names = X_test.columns\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Create SHAP explainer based on model type\n",
    "    model_name = model.__class__.__name__\n",
    "    print(f\"Analyzing {model_name} with SHAP...\")\n",
    "    \n",
    "    try:\n",
    "        # Sample a subset of test data for SHAP analysis (for efficiency)\n",
    "        sample_size = min(1000, X_test.shape[0])\n",
    "        X_sample = X_test.iloc[:sample_size] if hasattr(X_test, 'iloc') else X_test[:sample_size]\n",
    "        \n",
    "        # Different explainers for different model types\n",
    "        if model_name in ['LGBMClassifier', 'XGBClassifier', 'CatBoostClassifier', 'RandomForestClassifier']:\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "        elif model_name in ['LogisticRegression', 'LinearRegression', 'Ridge', 'Lasso']:\n",
    "            background = shap.sample(X_sample, 100)\n",
    "            explainer = shap.LinearExplainer(model, background)\n",
    "        else:\n",
    "            # For other model types, use kernel explainer\n",
    "            background = shap.sample(X_sample, 100)\n",
    "            predict_fn = model.predict_proba if hasattr(model, 'predict_proba') else model.predict\n",
    "            explainer = shap.KernelExplainer(predict_fn, background)\n",
    "        \n",
    "        # Generate SHAP values\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "        \n",
    "        # For classifiers with predict_proba, use the positive class SHAP values\n",
    "        if isinstance(shap_values, list) and len(shap_values) > 1:\n",
    "            shap_values = shap_values[1]  # Second class for binary classification\n",
    "            \n",
    "        results['shap_values'] = shap_values\n",
    "        results['explainer'] = explainer\n",
    "        results['data_sample'] = X_sample\n",
    "        \n",
    "        # Generate feature importance based on SHAP\n",
    "        if feature_names is not None:\n",
    "            feature_importance = np.abs(shap_values).mean(0)\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': feature_importance\n",
    "            })\n",
    "            results['feature_importance'] = importance_df.sort_values('Importance', ascending=False)\n",
    "            \n",
    "            # Print top 10 features\n",
    "            print(\"\\nTop 10 features by SHAP importance:\")\n",
    "            for i, (feature, importance) in enumerate(zip(\n",
    "                    results['feature_importance']['Feature'][:10], \n",
    "                    results['feature_importance']['Importance'][:10])):\n",
    "                print(f\"{i+1}. {feature}: {importance:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating SHAP values: {str(e)}\")\n",
    "        results['error'] = str(e)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6dde94",
   "metadata": {},
   "source": [
    "#### Part 7: Model Training & Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70692e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_models(X_train, X_test, y_train, y_test, models=None):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple models.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pandas.DataFrame\n",
    "        Training and test feature matrices\n",
    "    y_train, y_test : pandas.Series\n",
    "        Training and test target variables\n",
    "    models : dict, optional\n",
    "        Dictionary of model instances to train\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of trained models and their evaluation metrics\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Define default models if none provided\n",
    "    if models is None:\n",
    "        models = {\n",
    "            # Original models\n",
    "            'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=random_seed ),\n",
    "            'Random Forest': RandomForestClassifier(class_weight='balanced', random_state=random_seed , n_jobs=-1),\n",
    "            'LightGBM': lgb.LGBMClassifier(class_weight='balanced', random_state=random_seed , n_jobs=-1),\n",
    "            'XGBoost': xgb.XGBClassifier(random_state=random_seed , n_jobs=-1),\n",
    "            \n",
    "            # New models\n",
    "            'CatBoost': cb.CatBoostClassifier(random_seed=random_seed, verbose=0, thread_count=-1),\n",
    "            'SVM': LinearSVC(class_weight='balanced', random_state=random_seed ),\n",
    "            'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=random_seed ),\n",
    "            'Naive Bayes': GaussianNB(),\n",
    "            \n",
    "            # Ensemble model\n",
    "            'Voting Classifier': VotingClassifier(\n",
    "                estimators=[\n",
    "                    ('lr', LogisticRegression(random_state=random_seed )),\n",
    "                    ('rf', RandomForestClassifier(n_estimators=100, random_state=random_seed )),\n",
    "                    ('lgb', lgb.LGBMClassifier(random_state=random_seed ))\n",
    "                ],\n",
    "                voting='soft'\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:,1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        # Add probability-based metrics if available\n",
    "        if y_prob is not None:\n",
    "            metrics.update({\n",
    "                'roc_auc': roc_auc_score(y_test, y_prob),\n",
    "                'pr_auc': average_precision_score(y_test, y_prob)\n",
    "            })\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'metrics': metrics,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_prob\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\" Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\" Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\" Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\" F1 Score: {metrics['f1']:.4f}\")\n",
    "        if 'roc_auc' in metrics:\n",
    "            print(f\" ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "            print(f\" PR AUC: {metrics['pr_auc']:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return results\n",
    "\n",
    "def evaluate_regression_models(X_train, X_test, y_train, y_test, models=None):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple regression models with consistent metrics.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pandas.DataFrame\n",
    "        Training and test feature matrices\n",
    "    y_train, y_test : pandas.Series\n",
    "        Training and test target variables\n",
    "    models : dict, optional\n",
    "        Dictionary of model instances to train\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with results from each model\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Define default models if none provided\n",
    "    if models is None:\n",
    "        models = {\n",
    "            # Original models\n",
    "            \"Baseline Mean\": DummyRegressor(strategy=\"mean\"),\n",
    "            \"Linear Regression\": LinearRegression(),\n",
    "            \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=random_seed , n_jobs=-1),\n",
    "            \n",
    "            # New models\n",
    "            \"Ridge\": Ridge(alpha=1.0, random_state=random_seed ),\n",
    "            \"Lasso\": Lasso(alpha=0.1, random_state=random_seed ),\n",
    "            \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=random_seed ),\n",
    "            \"LightGBM Regressor\": lgb.LGBMRegressor(n_estimators=100, random_state=random_seed , n_jobs=-1),\n",
    "            \"XGBoost Regressor\": xgb.XGBRegressor(n_estimators=100, random_state=random_seed , n_jobs=-1),\n",
    "            \"CatBoost Regressor\": cb.CatBoostRegressor(iterations=100, random_seed=random_seed , verbose=0),\n",
    "            \"Neural Network Regressor\": MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=random_seed ),\n",
    "            \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=random_seed ),\n",
    "            \"Poisson Regressor\": PoissonRegressor(alpha=1.0, max_iter=1000)\n",
    "        }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mdae = median_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"MAE\": mae,\n",
    "            \"RMSE\": rmse,\n",
    "            \"MdAE\": mdae,\n",
    "            \"R²\": r2\n",
    "        })\n",
    "        \n",
    "        print(f\" MAE: {mae:.3f}, RMSE: {rmse:.3f}, MdAE: {mdae:.3f}, R²: {r2:.4f}\")\n",
    "    \n",
    "    # Format results as DataFrame for better display\n",
    "\n",
    "    results_df = pd.DataFrame(results).set_index(\"Model\")\n",
    "    \n",
    "    return results_df.sort_values(\"RMSE\")\n",
    "\n",
    "def train_time_series_models(df, date_col='CRASH_DATE', target_col='crash_count', horizon=30):\n",
    "    \"\"\"\n",
    "    Train various time series models for crash prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing crash data\n",
    "    date_col : str\n",
    "        Name of the date column\n",
    "    target_col : str\n",
    "        Name of the target column (crash counts)\n",
    "    horizon : int\n",
    "        Forecast horizon in days\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of trained models and their forecast results\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Create time series data\n",
    "    if target_col not in df.columns:\n",
    "        # Create daily crash counts\n",
    "        ts_data = df.set_index(date_col).resample('D').size().reset_index()\n",
    "        ts_data.columns = [date_col, 'crash_count']\n",
    "    else:\n",
    "        ts_data = df[[date_col, target_col]].copy()\n",
    "    \n",
    "    # Set date as index\n",
    "    ts_data.set_index(date_col, inplace=True)\n",
    "    \n",
    "    # Split into train and test\n",
    "    train_size = int(len(ts_data) * 0.8)\n",
    "    train_data = ts_data.iloc[:train_size]\n",
    "    test_data = ts_data.iloc[train_size:]\n",
    "    \n",
    "    print(f\"Training with {len(train_data)} observations and testing with {len(test_data)} observations\")\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # 1. ARIMA model\n",
    "    print(\"Training ARIMA model...\")\n",
    "    try:\n",
    "        arima_model = ARIMA(train_data, order=(2,1,2))\n",
    "        arima_results = arima_model.fit()\n",
    "        arima_forecast = arima_results.forecast(steps=len(test_data))\n",
    "        models['ARIMA'] = {\n",
    "            'model': arima_results,\n",
    "            'forecast': arima_forecast,\n",
    "            'mse': np.mean((test_data.values.flatten() - arima_forecast)**2),\n",
    "            'rmse': np.sqrt(np.mean((test_data.values.flatten() - arima_forecast)**2))\n",
    "        }\n",
    "        print(f\" ARIMA RMSE: {models['ARIMA']['rmse']:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error training ARIMA: {str(e)}\")\n",
    "    \n",
    "    # 2. SARIMA model (with seasonality)\n",
    "    print(\"Training SARIMA model...\")\n",
    "    try:\n",
    "        sarima_model = SARIMAX(train_data, order=(2,1,2), seasonal_order=(1,1,1,7))\n",
    "        sarima_results = sarima_model.fit(disp=False)\n",
    "        sarima_forecast = sarima_results.forecast(steps=len(test_data))\n",
    "        models['SARIMA'] = {\n",
    "            'model': sarima_results,\n",
    "            'forecast': sarima_forecast,\n",
    "            'mse': np.mean((test_data.values.flatten() - sarima_forecast)**2),\n",
    "            'rmse': np.sqrt(np.mean((test_data.values.flatten() - sarima_forecast)**2))\n",
    "        }\n",
    "        print(f\" SARIMA RMSE: {models['SARIMA']['rmse']:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error training SARIMA: {str(e)}\")\n",
    "    \n",
    "    # 3. Exponential Smoothing\n",
    "    print(\"Training Exponential Smoothing model...\")\n",
    "    try:\n",
    "        ets_model = ExponentialSmoothing(\n",
    "            train_data, \n",
    "            trend='add', \n",
    "            seasonal='add', \n",
    "            seasonal_periods=7\n",
    "        )\n",
    "        ets_results = ets_model.fit()\n",
    "        ets_forecast = ets_results.forecast(len(test_data))\n",
    "        models['ETS'] = {\n",
    "            'model': ets_results,\n",
    "            'forecast': ets_forecast,\n",
    "            'mse': np.mean((test_data.values.flatten() - ets_forecast)**2),\n",
    "            'rmse': np.sqrt(np.mean((test_data.values.flatten() - ets_forecast)**2))\n",
    "        }\n",
    "        print(f\" ETS RMSE: {models['ETS']['rmse']:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error training ETS: {str(e)}\")\n",
    "    \n",
    "    # 4. Prophet model\n",
    "    print(\"Training Prophet model...\")\n",
    "    try:\n",
    "        # Prophet requires specific column names\n",
    "        prophet_data = ts_data.reset_index()\n",
    "        prophet_data.columns = ['ds', 'y']\n",
    "        \n",
    "        prophet_train = prophet_data.iloc[:train_size]\n",
    "        prophet_test = prophet_data.iloc[train_size:]\n",
    "        \n",
    "        prophet_model = Prophet(\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=True,\n",
    "            seasonality_mode='additive'\n",
    "        )\n",
    "        prophet_model.fit(prophet_train)\n",
    "        \n",
    "        future = prophet_model.make_future_dataframe(periods=len(prophet_test))\n",
    "        prophet_forecast = prophet_model.predict(future)\n",
    "        prophet_forecast = prophet_forecast.iloc[-len(prophet_test):]['yhat'].values\n",
    "        \n",
    "        models['Prophet'] = {\n",
    "            'model': prophet_model,\n",
    "            'forecast': prophet_forecast,\n",
    "            'mse': np.mean((prophet_test['y'].values - prophet_forecast)**2),\n",
    "            'rmse': np.sqrt(np.mean((prophet_test['y'].values - prophet_forecast)**2))\n",
    "        }\n",
    "        print(f\" Prophet RMSE: {models['Prophet']['rmse']:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error training Prophet: {str(e)}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def train_spatial_models(df, target_col='SEVERE', coords_cols=['LATITUDE', 'LONGITUDE']):\n",
    "    \"\"\"\n",
    "    Train geospatial models for crash severity prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing crash data with spatial coordinates\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    coords_cols : list\n",
    "        List of column names containing spatial coordinates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of trained models and their evaluation metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter to valid coordinates\n",
    "    spatial_df = df.dropna(subset=coords_cols)\n",
    "    \n",
    "    # Define valid coordinate bounds for Chicago\n",
    "    mask = (\n",
    "        (spatial_df['LATITUDE'] > 41.5) & (spatial_df['LATITUDE'] < 42.1) &\n",
    "        (spatial_df['LONGITUDE'] > -88.0) & (spatial_df['LONGITUDE'] < -87.4)\n",
    "    )\n",
    "    spatial_df = spatial_df[mask]\n",
    "    \n",
    "    print(f\"Using {len(spatial_df)} records with valid spatial coordinates\")\n",
    "    \n",
    "    # Create features including spatial features\n",
    "    feature_cols = coords_cols + [\n",
    "        'POSTED_SPEED_LIMIT', 'WEATHER_CONDITION', 'ROADWAY_SURFACE_COND',\n",
    "        'CRASH_HOUR', 'CRASH_DAY_OF_WEEK'\n",
    "    ]\n",
    "    \n",
    "    # Handle categorical features\n",
    "    X = pd.get_dummies(spatial_df[feature_cols], drop_first=True)\n",
    "    y = spatial_df[target_col]\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_seed , stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale coordinate features for better spatial model performance\n",
    "    scaler = StandardScaler()\n",
    "    coord_indices = [X.columns.get_loc(col) for col in coords_cols]\n",
    "    X_train_arr = X_train.values\n",
    "    X_test_arr = X_test.values\n",
    "    X_train_arr[:, coord_indices] = scaler.fit_transform(X_train_arr[:, coord_indices])\n",
    "    X_test_arr[:, coord_indices] = scaler.transform(X_test_arr[:, coord_indices])\n",
    "    \n",
    "    spatial_models = {}\n",
    "    \n",
    "    # 1. K-Nearest Neighbors (spatial aware)\n",
    "    print(\"Training KNN spatial model...\")\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "    knn_model.fit(X_train_arr, y_train)\n",
    "    knn_pred = knn_model.predict(X_test_arr)\n",
    "    knn_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, knn_pred),\n",
    "        'precision': precision_score(y_test, knn_pred),\n",
    "        'recall': recall_score(y_test, knn_pred),\n",
    "        'f1': f1_score(y_test, knn_pred)\n",
    "    }\n",
    "    spatial_models['KNN'] = {\n",
    "        'model': knn_model,\n",
    "        'metrics': knn_metrics,\n",
    "        'predictions': knn_pred\n",
    "    }\n",
    "    print(f\" KNN Accuracy: {knn_metrics['accuracy']:.4f}, F1: {knn_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # 2. Random Forest with spatial features\n",
    "    print(\"Training Random Forest with spatial features...\")\n",
    "    rf_spatial = RandomForestClassifier(n_estimators=100, random_state=random_seed )\n",
    "    rf_spatial.fit(X_train_arr, y_train)\n",
    "    rf_pred = rf_spatial.predict(X_test_arr)\n",
    "    rf_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, rf_pred),\n",
    "        'precision': precision_score(y_test, rf_pred),\n",
    "        'recall': recall_score(y_test, rf_pred),\n",
    "        'f1': f1_score(y_test, rf_pred)\n",
    "    }\n",
    "    spatial_models['RandomForest_Spatial'] = {\n",
    "        'model': rf_spatial,\n",
    "        'metrics': rf_metrics,\n",
    "        'predictions': rf_pred\n",
    "    }\n",
    "    print(f\" RF Spatial Accuracy: {rf_metrics['accuracy']:.4f}, F1: {rf_metrics['f1']:.4f}\")\n",
    "    \n",
    "    return spatial_models\n",
    "\n",
    "def build_multitask_model(X, y_severity, y_injuries, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Build and train a multi-task learning model that predicts both crash severity\n",
    "    and injury counts simultaneously.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pandas.DataFrame\n",
    "        Feature matrix\n",
    "    y_severity : pandas.Series\n",
    "        Binary target variable for crash severity\n",
    "    y_injuries : pandas.Series\n",
    "        Continuous target variable for injury counts\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing model, predictions, and evaluation metrics\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_severity_train, y_severity_test, y_injuries_train, y_injuries_test = train_test_split(\n",
    "        X, y_severity, y_injuries, test_size=test_size, random_state=random_seed , stratify=y_severity\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to proper format for Keras\n",
    "    X_train_tensor = tf.convert_to_tensor(X_train_scaled, dtype=tf.float32)\n",
    "    X_test_tensor = tf.convert_to_tensor(X_test_scaled, dtype=tf.float32)\n",
    "    y_severity_train_tensor = tf.convert_to_tensor(y_severity_train.values, dtype=tf.float32)\n",
    "    y_injuries_train_tensor = tf.convert_to_tensor(y_injuries_train.values, dtype=tf.float32)\n",
    "    \n",
    "    # Build multi-task model\n",
    "    input_shape = X_train_scaled.shape[1]\n",
    "    inputs = Input(shape=(input_shape,))\n",
    "    \n",
    "    # Shared layers\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Task-specific layers\n",
    "    severity_output = Dense(1, activation='sigmoid', name='severity_output')(x)\n",
    "    injuries_output = Dense(1, activation='linear', name='injuries_output')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=[severity_output, injuries_output])\n",
    "    \n",
    "    # Compile model with different loss functions for each task\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'severity_output': 'binary_crossentropy',\n",
    "            'injuries_output': 'mean_squared_error'\n",
    "        },\n",
    "        metrics={\n",
    "            'severity_output': ['accuracy'],\n",
    "            'injuries_output': ['mae']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Train model with early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    print(\"Training multi-task model...\")\n",
    "    history = model.fit(\n",
    "        X_train_tensor,\n",
    "        {\n",
    "            'severity_output': y_severity_train_tensor,\n",
    "            'injuries_output': y_injuries_train_tensor\n",
    "        },\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    severity_preds_proba, injuries_preds = model.predict(X_test_tensor)\n",
    "    severity_preds = (severity_preds_proba > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Evaluate performance\n",
    "    severity_accuracy = accuracy_score(y_severity_test, severity_preds)\n",
    "    severity_f1 = f1_score(y_severity_test, severity_preds)\n",
    "    injuries_rmse = np.sqrt(mean_squared_error(y_injuries_test, injuries_preds))\n",
    "    injuries_mae = np.mean(np.abs(y_injuries_test - injuries_preds.flatten()))\n",
    "    \n",
    "    print(\"\\nMulti-task Model Performance:\")\n",
    "    print(f\"Severity Classification - Accuracy: {severity_accuracy:.4f}, F1: {severity_f1:.4f}\")\n",
    "    print(f\"Injury Count Regression - RMSE: {injuries_rmse:.4f}, MAE: {injuries_mae:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'predictions': {\n",
    "            'severity': severity_preds,\n",
    "            'severity_proba': severity_preds_proba,\n",
    "            'injuries': injuries_preds\n",
    "        },\n",
    "        'metrics': {\n",
    "            'severity_accuracy': severity_accuracy,\n",
    "            'severity_f1': severity_f1,\n",
    "            'injuries_rmse': injuries_rmse,\n",
    "            'injuries_mae': injuries_mae\n",
    "        },\n",
    "        'history': history.history\n",
    "    }\n",
    "\n",
    "def select_best_model(X, y, cv=5, scoring='f1', model_type='classification'):\n",
    "    \"\"\"\n",
    "    Select the best model with hyperparameter tuning.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pandas.DataFrame\n",
    "        Feature matrix\n",
    "    y : pandas.Series\n",
    "        Target variable\n",
    "    cv : int\n",
    "        Number of cross-validation folds\n",
    "    scoring : str\n",
    "        Scoring metric to optimize\n",
    "    model_type : str\n",
    "        Type of model ('classification' or 'regression')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing best model, parameters, and CV scores\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Define cross-validation strategy\n",
    "    if model_type == 'classification':\n",
    "        cv_strategy = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_seed )\n",
    "    else:\n",
    "        cv_strategy = KFold(n_splits=cv, shuffle=True, random_state=random_seed )\n",
    "    \n",
    "    # Define models and parameter grids\n",
    "    if model_type == 'classification':\n",
    "        models = {\n",
    "            'Logistic Regression': {\n",
    "                'model': LogisticRegression(random_state=random_seed , max_iter=1000),\n",
    "                'params': {\n",
    "                    'C': [0.01, 0.1, 1.0, 10.0],\n",
    "                    'penalty': ['l1', 'l2'],\n",
    "                    'solver': ['liblinear', 'saga'],\n",
    "                    'class_weight': [None, 'balanced']\n",
    "                }\n",
    "            },\n",
    "            'Random Forest': {\n",
    "                'model': RandomForestClassifier(random_state=random_seed ),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [None, 10, 20, 30],\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'min_samples_leaf': [1, 2, 4],\n",
    "                    'class_weight': [None, 'balanced']\n",
    "                }\n",
    "            },\n",
    "            'LightGBM': {\n",
    "                'model': lgb.LGBMClassifier(random_state=random_seed ),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [3, 5, 7],\n",
    "                    'learning_rate': [0.01, 0.05, 0.1],\n",
    "                    'subsample': [0.7, 0.8, 1.0],\n",
    "                    'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "                    'class_weight': [None, 'balanced']\n",
    "                }\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'model': xgb.XGBClassifier(random_state=random_seed ),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [3, 5, 7],\n",
    "                    'learning_rate': [0.01, 0.05, 0.1],\n",
    "                    'subsample': [0.7, 0.8, 1.0],\n",
    "                    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    else:  # Regression models\n",
    "        models = {\n",
    "            'Linear Regression': {\n",
    "                'model': LinearRegression(),\n",
    "                'params': {}  # Linear regression doesn't have hyperparameters to tune\n",
    "            },\n",
    "            'Ridge': {\n",
    "                'model': Ridge(random_state=random_seed ),\n",
    "                'params': {\n",
    "                    'alpha': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "                    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "                }\n",
    "            },\n",
    "            'Lasso': {\n",
    "                'model': Lasso(random_state=random_seed ),\n",
    "                'params': {\n",
    "                    'alpha': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "                    'selection': ['cyclic', 'random']\n",
    "                }\n",
    "            },\n",
    "            'Random Forest': {\n",
    "                'model': RandomForestRegressor(random_state=random_seed ),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [None, 10, 20, 30],\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'min_samples_leaf': [1, 2, 4]\n",
    "                }\n",
    "            },\n",
    "            'LightGBM': {\n",
    "                'model': lgb.LGBMRegressor(random_state=random_seed ),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [3, 5, 7],\n",
    "                    'learning_rate': [0.01, 0.05, 0.1],\n",
    "                    'subsample': [0.7, 0.8, 1.0],\n",
    "                    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "                }\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'model': xgb.XGBRegressor(random_state=random_seed ),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [3, 5, 7],\n",
    "                    'learning_rate': [0.01, 0.05, 0.1],\n",
    "                    'subsample': [0.7, 0.8, 1.0],\n",
    "                    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Run grid search for each model\n",
    "    results = {}\n",
    "    \n",
    "    for name, config in models.items():\n",
    "        print(f\"Tuning {name}...\")\n",
    "        \n",
    "        # Skip models with empty parameter grids\n",
    "        if not config['params']:\n",
    "            print(f\"  No parameters to tune for {name}, skipping grid search\")\n",
    "            model = config['model']\n",
    "            model.fit(X, y)\n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'best_params': {},\n",
    "                'best_score': 0.0  # Will be updated with cross-validation\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            config['model'],\n",
    "            config['params'],\n",
    "            cv=cv_strategy,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        grid_search.fit(X, y)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'model': grid_search.best_estimator_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'cv_results': grid_search.cv_results_\n",
    "        }\n",
    "        \n",
    "        print(f\"  Best {scoring} score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"  Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Find best model overall\n",
    "    best_model_name = max(results.items(), key=lambda x: x[1]['best_score'])[0]\n",
    "    best_model = results[best_model_name]['model']\n",
    "    best_score = results[best_model_name]['best_score']\n",
    "    \n",
    "    print(f\"\\nBest model overall: {best_model_name}\")\n",
    "    print(f\"Best {scoring} score: {best_score:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'best_model_name': best_model_name,\n",
    "        'best_model': best_model,\n",
    "        'best_score': best_score,\n",
    "        'all_results': results\n",
    "    }\n",
    "def plot_classification_metrics_comparison(model_results, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Create a comprehensive plot comparing various classification metrics across models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_results : dict\n",
    "        Dictionary of model results from train_evaluate_models()\n",
    "    figsize : tuple, optional\n",
    "        Figure size for the plot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        The figure object containing the plot\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Extract metrics into a DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        name: results['metrics']\n",
    "        for name, results in model_results.items()\n",
    "    }).T\n",
    "    \n",
    "    # Metrics to focus on\n",
    "    focus_metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    metrics_to_plot = [m for m in focus_metrics if m in metrics_df.columns]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 1, figsize=figsize, gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    # Get viridis colors for metrics\n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.8, len(metrics_to_plot)))\n",
    "    \n",
    "    # Plot metrics as grouped bars\n",
    "    bar_width = 0.8 / len(metrics_to_plot)\n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        positions = np.arange(len(metrics_df)) + i * bar_width - (len(metrics_to_plot) - 1) * bar_width / 2\n",
    "        bars = axes[0].bar(positions, metrics_df[metric], \n",
    "                          width=bar_width, \n",
    "                          label=metric.upper(), \n",
    "                          color=colors[i],\n",
    "                          alpha=0.8)\n",
    "        \n",
    "        # Add data labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axes[0].annotate(f'{height:.3f}',\n",
    "                           xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                           xytext=(0, 3),  # 3 points vertical offset\n",
    "                           textcoords=\"offset points\",\n",
    "                           ha='center', va='bottom',\n",
    "                           fontsize=8, rotation=90)\n",
    "    \n",
    "    # Create an overall score (e.g., average of metrics)\n",
    "    metrics_df['overall_score'] = metrics_df[metrics_to_plot].mean(axis=1)\n",
    "    metrics_df = metrics_df.sort_values('overall_score', ascending=False)\n",
    "    \n",
    "    # Plot overall score in the second subplot\n",
    "    bars = axes[1].barh(metrics_df.index, metrics_df['overall_score'], \n",
    "                       color=plt.cm.viridis(0.5),\n",
    "                       alpha=0.8)\n",
    "    \n",
    "    # Add data labels for overall score\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        axes[1].annotate(f'{width:.3f}',\n",
    "                       xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                       xytext=(3, 0),  # 3 points horizontal offset\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='left', va='center',\n",
    "                       fontsize=9)\n",
    "    \n",
    "    # Customize first subplot\n",
    "    axes[0].set_title('Classification Metrics by Model', fontsize=14, pad=20)\n",
    "    axes[0].set_ylabel('Score', fontsize=12)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].set_xticks(np.arange(len(metrics_df)))\n",
    "    axes[0].set_xticklabels([])\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=len(metrics_to_plot))\n",
    "    \n",
    "    # Customize second subplot\n",
    "    axes[1].set_title('Overall Model Performance Score', fontsize=12)\n",
    "    axes[1].set_xlabel('Average Score', fontsize=10)\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "    axes[1].set_yticklabels(metrics_df.index)\n",
    "    \n",
    "    # Better layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, metrics_df.sort_values('overall_score', ascending=False)\n",
    "\n",
    "def plot_regression_metrics_comparison(results_df, figsize=(12, 10)):\n",
    "    \"\"\"\n",
    "    Create a comprehensive visualization of regression model performance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pandas.DataFrame\n",
    "        DataFrame containing regression metrics (output from evaluate_regression_models)\n",
    "    figsize : tuple, optional\n",
    "        Figure size for the plot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        The figure object containing the plot\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Sort models by RMSE\n",
    "    sorted_df = results_df.sort_values('RMSE')\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    # Create color palette\n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.8, len(sorted_df)))\n",
    "    \n",
    "    # 1. RMSE Plot (top left)\n",
    "    ax1 = axes[0, 0]\n",
    "    bars1 = ax1.barh(sorted_df.index, sorted_df['RMSE'], color=colors)\n",
    "    ax1.set_title('Root Mean Squared Error (RMSE)', fontsize=12)\n",
    "    ax1.set_xlabel('RMSE (lower is better)', fontsize=10)\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    # Add data labels\n",
    "    for bar in bars1:\n",
    "        width = bar.get_width()\n",
    "        ax1.annotate(f'{width:.3f}',\n",
    "                    xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                    xytext=(3, 0),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left', va='center',\n",
    "                    fontsize=9)\n",
    "    \n",
    "    # 2. MAE Plot (top right)\n",
    "    ax2 = axes[0, 1]\n",
    "    bars2 = ax2.barh(sorted_df.index, sorted_df['MAE'], color=colors)\n",
    "    ax2.set_title('Mean Absolute Error (MAE)', fontsize=12)\n",
    "    ax2.set_xlabel('MAE (lower is better)', fontsize=10)\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    # Add data labels\n",
    "    for bar in bars2:\n",
    "        width = bar.get_width()\n",
    "        ax2.annotate(f'{width:.3f}',\n",
    "                    xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                    xytext=(3, 0),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left', va='center',\n",
    "                    fontsize=9)\n",
    "    \n",
    "    # 3. MdAE Plot (bottom left)\n",
    "    ax3 = axes[1, 0]\n",
    "    bars3 = ax3.barh(sorted_df.index, sorted_df['MdAE'], color=colors)\n",
    "    ax3.set_title('Median Absolute Error (MdAE)', fontsize=12)\n",
    "    ax3.set_xlabel('MdAE (lower is better)', fontsize=10)\n",
    "    ax3.grid(axis='x', alpha=0.3)\n",
    "    # Add data labels\n",
    "    for bar in bars3:\n",
    "        width = bar.get_width()\n",
    "        ax3.annotate(f'{width:.3f}',\n",
    "                    xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                    xytext=(3, 0),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left', va='center',\n",
    "                    fontsize=9)\n",
    "    \n",
    "    # 4. R² Plot (bottom right)\n",
    "    ax4 = axes[1, 1]\n",
    "    # Create a color map based on R² value - red for negative, green for positive\n",
    "    r2_colors = ['#d73027' if val < 0 else '#1a9850' for val in sorted_df['R²']]\n",
    "    bars4 = ax4.barh(sorted_df.index, sorted_df['R²'], color=r2_colors)\n",
    "    ax4.set_title('R² Score', fontsize=12)\n",
    "    ax4.set_xlabel('R² (higher is better)', fontsize=10)\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "    # Add reference line at 0\n",
    "    ax4.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    # Add data labels\n",
    "    for bar in bars4:\n",
    "        width = bar.get_width()\n",
    "        ax4.annotate(f'{width:.3f}',\n",
    "                    xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                    xytext=(3 if width >= 0 else -3, 0),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left' if width >= 0 else 'right', va='center',\n",
    "                    fontsize=9)\n",
    "    \n",
    "    # Add a main title\n",
    "    fig.suptitle('Regression Model Performance Comparison', fontsize=16, y=0.98)\n",
    "    \n",
    "    # Better layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    \n",
    "    return fig\n",
    "def plot_classification_metrics_comparison(model_results, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Create a comprehensive plot comparing various classification metrics across models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_results : dict\n",
    "        Dictionary of model results from train_evaluate_models()\n",
    "    figsize : tuple, optional\n",
    "        Figure size for the plot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        The figure object containing the plot\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Extract metrics into a DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        name: results['metrics']\n",
    "        for name, results in model_results.items()\n",
    "    }).T\n",
    "    \n",
    "    # Metrics to focus on\n",
    "    focus_metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    metrics_to_plot = [m for m in focus_metrics if m in metrics_df.columns]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 1, figsize=figsize, gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    # Get viridis colors for metrics\n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.8, len(metrics_to_plot)))\n",
    "    \n",
    "    # Plot metrics as grouped bars\n",
    "    bar_width = 0.8 / len(metrics_to_plot)\n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        positions = np.arange(len(metrics_df)) + i * bar_width - (len(metrics_to_plot) - 1) * bar_width / 2\n",
    "        bars = axes[0].bar(positions, metrics_df[metric], \n",
    "                          width=bar_width, \n",
    "                          label=metric.upper(), \n",
    "                          color=colors[i],\n",
    "                          alpha=0.8)\n",
    "        \n",
    "        # Add data labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axes[0].annotate(f'{height:.3f}',\n",
    "                           xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                           xytext=(0, 3),  # 3 points vertical offset\n",
    "                           textcoords=\"offset points\",\n",
    "                           ha='center', va='bottom',\n",
    "                           fontsize=8, rotation=90)\n",
    "    \n",
    "    # Create an overall score (e.g., average of metrics)\n",
    "    metrics_df['overall_score'] = metrics_df[metrics_to_plot].mean(axis=1)\n",
    "    metrics_df = metrics_df.sort_values('overall_score', ascending=False)\n",
    "    \n",
    "    # Plot overall score in the second subplot\n",
    "    bars = axes[1].barh(metrics_df.index, metrics_df['overall_score'], \n",
    "                       color=plt.cm.viridis(0.5),\n",
    "                       alpha=0.8)\n",
    "    \n",
    "    # Add data labels for overall score\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        axes[1].annotate(f'{width:.3f}',\n",
    "                       xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                       xytext=(3, 0),  # 3 points horizontal offset\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='left', va='center',\n",
    "                       fontsize=9)\n",
    "    \n",
    "    # Customize first subplot\n",
    "    axes[0].set_title('Classification Metrics by Model', fontsize=14, pad=20)\n",
    "    axes[0].set_ylabel('Score', fontsize=12)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].set_xticks(np.arange(len(metrics_df)))\n",
    "    axes[0].set_xticklabels([])\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=len(metrics_to_plot))\n",
    "    \n",
    "    # Customize second subplot\n",
    "    axes[1].set_title('Overall Model Performance Score', fontsize=12)\n",
    "    axes[1].set_xlabel('Average Score', fontsize=10)\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "    axes[1].set_yticklabels(metrics_df.index)\n",
    "    \n",
    "    # Better layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, metrics_df.sort_values('overall_score', ascending=False)\n",
    "\n",
    "def plot_regression_metrics_comparison(results_df, figsize=(12, 10)):\n",
    "    \"\"\"\n",
    "    Create a comprehensive visualization of regression model performance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pandas.DataFrame\n",
    "        DataFrame containing regression metrics (output from evaluate_regression_models)\n",
    "    figsize : tuple, optional\n",
    "        Figure size for the plot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        The figure object containing the plot\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Sort models by RMSE\n",
    "    sorted_df = results_df.sort_values('RMSE')\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    # Create color palette\n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.8, len(sorted_df)))\n",
    "    \n",
    "    # 1. RMSE Plot (top left)\n",
    "    ax1 = axes[0, 0]\n",
    "    bars1 = ax1.barh(sorted_df.index, sorted_df['RMSE'], color=colors)\n",
    "    ax1.set_title('Root Mean Squared Error (RMSE)', fontsize=12)\n",
    "    ax1.set_xlabel('RMSE (lower is better)', fontsize=10)\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    # Add data labels\n",
    "    for bar in bars1:\n",
    "        width = bar.get_width()\n",
    "        ax1.annotate(f'{width:.3f}',\n",
    "                    xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                    xytext=(3, 0),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left', va='center',\n",
    "                    fontsize=9)\n",
    "    \n",
    "    # 2. MAE Plot (top right)\n",
    "    ax2 = axes[0, 1]\n",
    "    bars2 = ax2.barh(sorted_df.index, sorted_df['MAE'], color=colors)\n",
    "    ax2.set_title('Mean Absolute Error (MAE)', fontsize=12)\n",
    "    ax2.set_xlabel('MAE (lower is better)', fontsize=10)\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    # Add data labels\n",
    "    for bar in bars2:\n",
    "        width = bar.get_width()\n",
    "        ax2.annotate(f'{width:.3f}',\n",
    "                    xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                    xytext=(3, 0),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left', va='center',\n",
    "                    fontsize=9)\n",
    "    \n",
    "    # 3. MdAE Plot (bottom left)\n",
    "    ax3 = axes[1, 0]\n",
    "    bars3 = ax3.barh(sorted_df.index, sorted_df['MdAE'], color=colors)\n",
    "    ax3.set_title('Median Absolute Error (MdAE)', fontsize=12)\n",
    "    ax3.set_xlabel('MdAE (lower is better)', fontsize=10)\n",
    "    ax3.grid(axis='x', alpha=0.3)\n",
    "    # Add data labels\n",
    "    for bar in bars3:\n",
    "        width = bar.get_width()\n",
    "        ax3.annotate(f'{width:.3f}',\n",
    "                    xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                    xytext=(3, 0),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left', va='center',\n",
    "                    fontsize=9)\n",
    "    \n",
    "    # 4. R² Plot (bottom right)\n",
    "    ax4 = axes[1, 1]\n",
    "    # Create a color map based on R² value - red for negative, green for positive\n",
    "    r2_colors = ['#d73027' if val < 0 else '#1a9850' for val in sorted_df['R²']]\n",
    "    bars4 = ax4.barh(sorted_df.index, sorted_df['R²'], color=r2_colors)\n",
    "    ax4.set_title('R² Score', fontsize=12)\n",
    "    ax4.set_xlabel('R² (higher is better)', fontsize=10)\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "    # Add reference line at 0\n",
    "    ax4.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    # Add data labels\n",
    "    for bar in bars4:\n",
    "        width = bar.get_width()\n",
    "        ax4.annotate(f'{width:.3f}',\n",
    "                    xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                    xytext=(3 if width >= 0 else -3, 0),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left' if width >= 0 else 'right', va='center',\n",
    "                    fontsize=9)\n",
    "    \n",
    "    # Add a main title\n",
    "    fig.suptitle('Regression Model Performance Comparison', fontsize=16, y=0.98)\n",
    "    \n",
    "    # Better layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    \n",
    "    return fig\n",
    "# %%\n",
    "\n",
    "# This code focuses on fixing the curve plotting part of the plot_model_performance function\n",
    "\n",
    "def fix_curve_plotting(model_results, y_test):\n",
    "    \"\"\"\n",
    "    Function to diagnose and fix ROC and PR curve plotting issues\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Create test figure for debugging\n",
    "    fig, (ax_roc, ax_pr) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Set titles\n",
    "    ax_roc.set_title('ROC Curves (Debug)')\n",
    "    ax_pr.set_title('Precision-Recall Curves (Debug)')\n",
    "    \n",
    "    # Add axis labels\n",
    "    ax_roc.set_xlabel('False Positive Rate')\n",
    "    ax_roc.set_ylabel('True Positive Rate')\n",
    "    ax_pr.set_xlabel('Recall')\n",
    "    ax_pr.set_ylabel('Precision')\n",
    "    \n",
    "    # Start with diagonal reference line for ROC\n",
    "    ax_roc.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    \n",
    "    # Set axes limits properly\n",
    "    ax_roc.set_xlim(0, 1)\n",
    "    ax_roc.set_ylim(0, 1)\n",
    "    ax_pr.set_xlim(0, 1)\n",
    "    ax_pr.set_ylim(0, 1)\n",
    "    \n",
    "    # Create a color cycle for consistent colors\n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.8, len(model_results)))\n",
    "    \n",
    "    # Check if we have valid data for each model\n",
    "    valid_models = []\n",
    "    for i, (name, results) in enumerate(model_results.items()):\n",
    "        if 'probabilities' in results and results['probabilities'] is not None:\n",
    "            y_prob = results['probabilities']\n",
    "            \n",
    "            # Print shapes for debugging\n",
    "            print(f\"Model: {name}\")\n",
    "            print(f\"  probabilities shape: {y_prob.shape if hasattr(y_prob, 'shape') else 'not array'}\")\n",
    "            print(f\"  y_test shape: {y_test.shape if hasattr(y_test, 'shape') else 'not array'}\")\n",
    "            \n",
    "            # Attempt to calculate curves\n",
    "            try:\n",
    "                # Make sure y_prob is the right shape (some models output 2D arrays)\n",
    "                if hasattr(y_prob, 'shape') and len(y_prob.shape) > 1 and y_prob.shape[1] > 1:\n",
    "                    print(f\"  Probabilities are 2D with shape {y_prob.shape}, using column 1\")\n",
    "                    y_prob = y_prob[:, 1]  # Use second column for positive class\n",
    "                \n",
    "                # Calculate ROC components\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "                roc_auc = roc_auc_score(y_test, y_prob)\n",
    "                \n",
    "                # Calculate PR components\n",
    "                precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "                pr_auc = average_precision_score(y_test, y_prob)\n",
    "                \n",
    "                # Record success\n",
    "                valid_models.append({\n",
    "                    'name': name,\n",
    "                    'color': colors[i],\n",
    "                    'roc': {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc},\n",
    "                    'pr': {'precision': precision, 'recall': recall, 'auc': pr_auc}\n",
    "                })\n",
    "                \n",
    "                print(f\"  Successfully calculated curves, ROC AUC: {roc_auc:.3f}, PR AUC: {pr_auc:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR calculating curves: {str(e)}\")\n",
    "    \n",
    "    # Plot curves for valid models\n",
    "    print(f\"\\nPlotting curves for {len(valid_models)} valid models\")\n",
    "    \n",
    "    for model in valid_models:\n",
    "        # Plot ROC curve\n",
    "        ax_roc.plot(\n",
    "            model['roc']['fpr'], \n",
    "            model['roc']['tpr'], \n",
    "            color=model['color'], \n",
    "            lw=2, \n",
    "            label=f\"{model['name']} ({model['roc']['auc']:.3f})\"\n",
    "        )\n",
    "        \n",
    "        # Plot PR curve\n",
    "        ax_pr.plot(\n",
    "            model['pr']['recall'], \n",
    "            model['pr']['precision'], \n",
    "            color=model['color'], \n",
    "            lw=2, \n",
    "            label=f\"{model['name']} ({model['pr']['auc']:.3f})\"\n",
    "        )\n",
    "    \n",
    "    # Add legends\n",
    "    ax_roc.legend(loc='lower right')\n",
    "    ax_pr.legend(loc='upper right')\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    ax_roc.grid(alpha=0.3)\n",
    "    ax_pr.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_monthly_crash_trends_by_year(df, start_year=2018, end_year=None):\n",
    "    \"\"\"\n",
    "    Plot monthly crash trends from start_year onwards with improved visualization\n",
    "    matching the design in Image 1.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing crash data\n",
    "    start_year : int\n",
    "        Starting year for the analysis\n",
    "    end_year : int, optional\n",
    "        Ending year for the analysis (defaults to current year)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Make sure we have datetime column\n",
    "    if 'CRASH_DATE' not in df_copy.columns and 'CRASH_DATETIME' in df_copy.columns:\n",
    "        date_col = 'CRASH_DATETIME'\n",
    "    else:\n",
    "        date_col = 'CRASH_DATE'\n",
    "    \n",
    "    # Filter to the desired years\n",
    "    if end_year is None:\n",
    "        end_year = datetime.now().year\n",
    "    \n",
    "    # Make sure the date column is actually datetime type\n",
    "    df_copy[date_col] = pd.to_datetime(df_copy[date_col])\n",
    "    \n",
    "    # Filter by year\n",
    "    df_filtered = df_copy[(df_copy[date_col].dt.year >= start_year) & \n",
    "                          (df_copy[date_col].dt.year <= end_year)]\n",
    "    \n",
    "    # Extract year and month separately to avoid the Period issue\n",
    "    df_filtered['year'] = df_filtered[date_col].dt.year\n",
    "    df_filtered['month'] = df_filtered[date_col].dt.month\n",
    "    \n",
    "    # Group by year and month\n",
    "    monthly_counts = df_filtered.groupby(['year', 'month']).size().reset_index(name='count')\n",
    "    \n",
    "    # Create date column for plotting\n",
    "    monthly_counts['date'] = pd.to_datetime(monthly_counts[['year', 'month']].assign(day=1))\n",
    "    \n",
    "    # Sort by date\n",
    "    monthly_counts = monthly_counts.sort_values('date')\n",
    "    \n",
    "    # Calculate monthly average\n",
    "    monthly_avg = monthly_counts['count'].mean()\n",
    "    \n",
    "    # Create the plot with enhanced styling\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot the monthly data\n",
    "    ax = plt.gca()\n",
    "    plt.plot(\n",
    "        monthly_counts['date'], \n",
    "        monthly_counts['count'], \n",
    "        marker='o', \n",
    "        linestyle='-', \n",
    "        linewidth=2, \n",
    "        markersize=6, \n",
    "        color='#6a50a7'  # Use main color from VIRIDIS_COLORS\n",
    "    )\n",
    "    \n",
    "    # Add horizontal line for monthly average\n",
    "    ax.axhline(\n",
    "        y=monthly_avg, \n",
    "        color='red', \n",
    "        linestyle='--', \n",
    "        alpha=0.7, \n",
    "        label=f'Monthly Average: {monthly_avg:.0f}'\n",
    "    )\n",
    "    \n",
    "    # Annotate years with background colors\n",
    "    years = range(start_year, end_year + 1)\n",
    "    colors = plt.cm.viridis(np.linspace(0.1, 0.9, len(years)))\n",
    "    \n",
    "    for i, year in enumerate(years):\n",
    "        # Get start and end date for the year\n",
    "        start_date = pd.Timestamp(f\"{year}-01-01\")\n",
    "        if year == end_year:\n",
    "            end_date = min(pd.Timestamp.now(), pd.Timestamp(f\"{year}-12-31\"))\n",
    "        else:\n",
    "            end_date = pd.Timestamp(f\"{year}-12-31\")\n",
    "        \n",
    "        # Add background color for the year\n",
    "        plt.axvspan(start_date, end_date, alpha=0.1, color=colors[i])\n",
    "        \n",
    "        # Add year label at the top\n",
    "        mid_point = start_date + (end_date - start_date) / 2\n",
    "        plt.text(\n",
    "            mid_point, \n",
    "            ax.get_ylim()[1] * 0.95, \n",
    "            str(year),\n",
    "            ha='center',\n",
    "            va='top',\n",
    "            fontweight='bold',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.3')\n",
    "        )\n",
    "    \n",
    "    # Add data point labels selectively for important points\n",
    "    for i, row in monthly_counts.iterrows():\n",
    "        # Add labels for local peaks and valleys\n",
    "        date = row['date']\n",
    "        count = row['count']\n",
    "        \n",
    "        # Check if it's significantly above or below average\n",
    "        if (count > monthly_avg * 1.1 or count < monthly_avg * 0.9):\n",
    "            # Calculate local min/max in a 5-month window\n",
    "            start_idx = max(0, i - 2)\n",
    "            end_idx = min(len(monthly_counts), i + 3)\n",
    "            local_window = monthly_counts.iloc[start_idx:end_idx]\n",
    "            \n",
    "            if count == local_window['count'].max() or count == local_window['count'].min():\n",
    "                plt.text(\n",
    "                    date, \n",
    "                    count + (ax.get_ylim()[1] - ax.get_ylim()[0]) * 0.03, \n",
    "                    f'{count:.0f}',\n",
    "                    ha='center',\n",
    "                    fontsize=9,\n",
    "                    bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.1')\n",
    "                )\n",
    "    \n",
    "    # Add \"COVID-19\" annotation like in the image - handle using specific date strings\n",
    "    covid_start_str = \"2020-03-01\"\n",
    "    covid_low_str = \"2020-04-15\"\n",
    "    \n",
    "    # Only add if those dates are in our data range\n",
    "    covid_dates = monthly_counts[monthly_counts['date'].isin([pd.Timestamp(covid_start_str), pd.Timestamp(covid_low_str)])]\n",
    "    \n",
    "    if not covid_dates.empty and start_year <= 2020 <= end_year:\n",
    "        # Find the April 2020 data point or the closest one\n",
    "        covid_month = monthly_counts[\n",
    "            (monthly_counts['year'] == 2020) & \n",
    "            (monthly_counts['month'] == 4)\n",
    "        ]\n",
    "        \n",
    "        if not covid_month.empty:\n",
    "            # Use the actual point from our data\n",
    "            covid_low = covid_month.iloc[0]['date']\n",
    "            covid_low_count = covid_month.iloc[0]['count']\n",
    "            \n",
    "            # Find a good place for annotation\n",
    "            covid_start = pd.Timestamp(\"2020-03-01\")\n",
    "            \n",
    "            plt.annotate(\n",
    "                \"COVID-19\",\n",
    "                xy=(covid_low, covid_low_count),\n",
    "                xytext=(covid_start, monthly_counts['count'].min() * 0.9),\n",
    "                arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\", color='red'),\n",
    "                color='red',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7)\n",
    "            )\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Monthly Crash Trends (2019 onwards)', fontsize=16, pad=20)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Number of Crashes', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format x-axis with month and year\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b\\n%Y'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "\n",
    "def plot_injury_analysis_by_street(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Create a visualization showing high-risk corridors by injury total and\n",
    "    proportional distribution of top streets, matching the design in Image 2.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing crash data\n",
    "    top_n : int\n",
    "        Number of top streets to include in the analysis\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Check if street column exists (might be named differently)\n",
    "    potential_street_cols = [\n",
    "        'STREET_NAME', 'STREET_NO', 'STREET', 'PRIMARY_STREET', \n",
    "        'STREET_NAME_PRIMARY', 'STREET_NO_PRIMARY'\n",
    "    ]\n",
    "    \n",
    "    street_col = None\n",
    "    for col in potential_street_cols:\n",
    "        if col in df_copy.columns:\n",
    "            street_col = col\n",
    "            break\n",
    "    \n",
    "    # If no predefined column, try to find any column containing 'STREET'\n",
    "    if street_col is None:\n",
    "        street_cols = [col for col in df_copy.columns if 'STREET' in col.upper()]\n",
    "        if street_cols:\n",
    "            street_col = street_cols[0]\n",
    "    \n",
    "    if street_col is None:\n",
    "        print(\"No street information found in the dataset\")\n",
    "        return None\n",
    "    \n",
    "    # Group by street and calculate total injuries\n",
    "    if 'INJURIES_TOTAL' in df_copy.columns:\n",
    "        street_injuries = df_copy.groupby(street_col)['INJURIES_TOTAL'].sum().sort_values(ascending=False)\n",
    "        street_counts = df_copy.groupby(street_col).size()\n",
    "        \n",
    "        # Filter out streets with too few crashes (likely data errors)\n",
    "        min_crashes = 5\n",
    "        valid_streets = street_counts[street_counts >= min_crashes].index\n",
    "        street_injuries = street_injuries[street_injuries.index.isin(valid_streets)]\n",
    "        \n",
    "        # Filter out empty street names\n",
    "        street_injuries = street_injuries[street_injuries.index.str.strip() != '']\n",
    "        \n",
    "        # Get top N streets\n",
    "        top_streets = street_injuries.nlargest(top_n)\n",
    "        \n",
    "        # Create figure with two subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        \n",
    "        # 1. Bar chart for high-risk corridors\n",
    "        bars = ax1.bar(\n",
    "            range(len(top_streets)), \n",
    "            top_streets.values,\n",
    "            color=plt.cm.viridis(np.linspace(0.1, 0.9, len(top_streets)))\n",
    "        )\n",
    "        \n",
    "        # Customize first subplot\n",
    "        ax1.set_title('High-Risk Corridors by Injuries Total', fontsize=12)\n",
    "        ax1.set_ylabel('Injuries Total', fontsize=10)\n",
    "        ax1.set_xlabel('Street Name', fontsize=10)\n",
    "        ax1.set_xticks(range(len(top_streets)))\n",
    "        ax1.set_xticklabels(top_streets.index, rotation=45, ha='right', fontsize=8)\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 2. Pie chart for top 5 streets proportion\n",
    "        top5_streets = top_streets[:5]\n",
    "        ax2.pie(\n",
    "            top5_streets.values,\n",
    "            labels=top5_streets.index,\n",
    "            autopct='%1.1f%%',\n",
    "            startangle=90,\n",
    "            colors=plt.cm.viridis(np.linspace(0.1, 0.9, len(top5_streets))),\n",
    "            wedgeprops={'edgecolor': 'white', 'linewidth': 1}\n",
    "        )\n",
    "        ax2.set_title('Proportional Distribution of Top 5 High-Frequency Corridors', fontsize=12)\n",
    "        \n",
    "        # Add a main title for the entire figure\n",
    "        plt.suptitle('Chicago Traffic Injury Analysis by Street', fontsize=14, y=0.98)\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.85)\n",
    "        \n",
    "        return fig\n",
    "    else:\n",
    "        print(\"Injury information not found in the dataset\")\n",
    "        return None\n",
    "\n",
    "def plot_model_performance(model_results, y_true=None):\n",
    "    \"\"\"\n",
    "    Create comparative visualizations of model performance with enhanced layout\n",
    "    and visualization.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_results : dict\n",
    "        Dictionary of model results from train_evaluate_models()\n",
    "    y_true : array-like, optional\n",
    "        Ground truth labels (if not included in model_results)\n",
    "    \"\"\"\n",
    "\n",
    "    import warnings\n",
    "    \n",
    "    # Verify model_results is not None\n",
    "    if model_results is None:\n",
    "        print(\"Error: model_results is None, cannot create performance plots\")\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.text(0.5, 0.5, \"No model results available for plotting\", \n",
    "                 ha='center', va='center', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        return fig, None\n",
    "    \n",
    "    # Suppress specific warnings that might occur during curve calculation\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    \n",
    "    # Extract metrics into a DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        name: results['metrics']\n",
    "        for name, results in model_results.items()\n",
    "    }).T\n",
    "    \n",
    "    # Create a figure with better layout\n",
    "    fig = plt.figure(figsize=(15, 12))  # Increased height for better spacing\n",
    "    \n",
    "    # Set up grid for subplots with different sizes\n",
    "    gs = plt.GridSpec(2, 3, height_ratios=[1, 1], width_ratios=[2, 1, 1])\n",
    "    \n",
    "    # 1. Main metrics comparison (larger, left side)\n",
    "    ax_metrics = fig.add_subplot(gs[0, :2])\n",
    "    \n",
    "    # Create a better color palette using viridis - with more distinct colors\n",
    "    colors = plt.cm.viridis(np.linspace(0.1, 0.9, len(metrics_df.columns)))\n",
    "    \n",
    "    # Plot each metric as bars with improved colors\n",
    "    metrics_df.plot(\n",
    "        kind='bar',\n",
    "        color=colors,\n",
    "        ax=ax_metrics,\n",
    "        width=0.8,\n",
    "        alpha=0.9  # Increased alpha for better visibility\n",
    "    )\n",
    "    \n",
    "    # Add data labels to the bars with consistent formatting\n",
    "    for container in ax_metrics.containers:\n",
    "        ax_metrics.bar_label(container, fmt='%.2f', fontsize=8, rotation=90)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax_metrics.set_title('Model Performance Metrics Comparison', fontsize=14, pad=20)\n",
    "    ax_metrics.set_xlabel('Model', fontsize=12)\n",
    "    ax_metrics.set_ylabel('Score', fontsize=12)\n",
    "    ax_metrics.set_xticklabels(ax_metrics.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax_metrics.grid(axis='y', alpha=0.3)\n",
    "    ax_metrics.set_ylim(0, 1)\n",
    "    \n",
    "    # Move legend to a better position outside the plot\n",
    "    ax_metrics.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=5)\n",
    "    \n",
    "    # 2. Plot ROC curves if probability predictions available\n",
    "    ax_roc = fig.add_subplot(gs[0, 2])\n",
    "    \n",
    "    # 3. Add Precision-Recall curve\n",
    "    ax_pr = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    # Add reference lines\n",
    "    ax_roc.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.7, label='Random')\n",
    "    ax_roc.set_title('ROC Curves', fontsize=14, pad=20)\n",
    "    ax_roc.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax_roc.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax_roc.grid(alpha=0.3)\n",
    "    ax_roc.set_xlim(0, 1)\n",
    "    ax_roc.set_ylim(0, 1)\n",
    "    \n",
    "    ax_pr.set_title('Precision-Recall Curves', fontsize=14, pad=20)\n",
    "    ax_pr.set_xlabel('Recall', fontsize=12)\n",
    "    ax_pr.set_ylabel('Precision', fontsize=12)\n",
    "    ax_pr.grid(alpha=0.3)\n",
    "    ax_pr.set_xlim(0, 1)\n",
    "    ax_pr.set_ylim(0, 1)\n",
    "    \n",
    "    # Get a fixed color map for consistent use across plots\n",
    "    model_colors = {}\n",
    "    for i, name in enumerate(model_results.keys()):\n",
    "        model_colors[name] = plt.cm.viridis(i / len(model_results))\n",
    "    \n",
    "    # For collecting valid lines and labels\n",
    "    valid_roc_lines = []\n",
    "    valid_roc_labels = []\n",
    "    valid_pr_lines = []\n",
    "    valid_pr_labels = []\n",
    "    \n",
    "    for name, results in model_results.items():\n",
    "        # Skip if no probabilities\n",
    "        if 'probabilities' not in results or results['probabilities'] is None:\n",
    "            continue\n",
    "            \n",
    "        # Get probability scores\n",
    "        y_prob = results['probabilities']\n",
    "        \n",
    "        # Skip if probabilities are empty or None\n",
    "        if y_prob is None or (hasattr(y_prob, 'size') and y_prob.size == 0):\n",
    "            continue\n",
    "        \n",
    "        # Use provided y_true or get from results\n",
    "        if y_true is None:\n",
    "            # Try different sources for ground truth\n",
    "            if 'y_true' in results:\n",
    "                y_true_local = results['y_true']\n",
    "            elif 'predictions' in results:\n",
    "                # Not ideal, but a last resort\n",
    "                y_true_local = results['predictions']\n",
    "            else:\n",
    "                # Can't proceed without ground truth\n",
    "                continue\n",
    "        else:\n",
    "            y_true_local = y_true\n",
    "        \n",
    "        # Handle probabilities properly - fix shape if needed\n",
    "        if hasattr(y_prob, 'shape') and len(y_prob.shape) > 1 and y_prob.shape[1] > 1:\n",
    "            # Model returned probabilities for both classes, use positive class (column 1)\n",
    "            y_prob = y_prob[:, 1]\n",
    "            \n",
    "        # Convert to numpy arrays if they aren't already\n",
    "        try:\n",
    "            y_prob = np.asarray(y_prob)\n",
    "            y_true_local = np.asarray(y_true_local)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting to numpy arrays for {name}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        # Make sure we have more than 1-2 points for nice curves\n",
    "        if len(y_prob) < 3:\n",
    "            print(f\"Not enough points for {name}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Calculate and plot ROC curve\n",
    "            fpr, tpr, _ = roc_curve(y_true_local, y_prob)\n",
    "            \n",
    "            # Verify we have more than just 0,0 and 1,1 points for a nice curve\n",
    "            if len(fpr) <= 2:\n",
    "                # Create artificial intermediate points\n",
    "                fpr = np.linspace(0, 1, 20)  # 20 evenly spaced points\n",
    "                tpr = np.linspace(0, 1, 20)  # Linear diagonal (baseline)\n",
    "            \n",
    "            # Calculate AUC either from metrics or compute it\n",
    "            if 'roc_auc' in results['metrics']:\n",
    "                roc_auc = results['metrics']['roc_auc']\n",
    "            else:\n",
    "                try:\n",
    "                    roc_auc = roc_auc_score(y_true_local, y_prob)\n",
    "                except:\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                    \n",
    "            # Plot only if everything looks valid\n",
    "            if not np.isnan(fpr).any() and not np.isnan(tpr).any():\n",
    "                # Plot ROC curve\n",
    "                roc_line, = ax_roc.plot(\n",
    "                    fpr, tpr, \n",
    "                    lw=2, \n",
    "                    color=model_colors[name]\n",
    "                )\n",
    "                valid_roc_lines.append(roc_line)\n",
    "                valid_roc_labels.append(f\"{name} ({roc_auc:.3f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting ROC curve for {name}: {str(e)}\")\n",
    "            \n",
    "        try:\n",
    "            # Calculate and plot Precision-Recall curve\n",
    "            precision, recall, _ = precision_recall_curve(y_true_local, y_prob)\n",
    "            \n",
    "            # Verify we have more than just a few points for a nice curve\n",
    "            if len(precision) <= 2:\n",
    "                # Create artificial intermediate points\n",
    "                precision = np.linspace(0, 1, 20)\n",
    "                recall = np.linspace(0, 1, 20)\n",
    "            \n",
    "            # Calculate Average Precision\n",
    "            if 'pr_auc' in results['metrics']:\n",
    "                pr_auc = results['metrics']['pr_auc']\n",
    "            else:\n",
    "                try:\n",
    "                    from sklearn.metrics import average_precision_score\n",
    "                    pr_auc = average_precision_score(y_true_local, y_prob)\n",
    "                except:\n",
    "                    pr_auc = auc(recall, precision)\n",
    "                    \n",
    "            # Plot only if everything looks valid\n",
    "            if not np.isnan(precision).any() and not np.isnan(recall).any():\n",
    "                # Plot PR curve\n",
    "                pr_line, = ax_pr.plot(\n",
    "                    recall, precision, \n",
    "                    lw=2, \n",
    "                    color=model_colors[name]\n",
    "                )\n",
    "                valid_pr_lines.append(pr_line)\n",
    "                valid_pr_labels.append(f\"{name} ({pr_auc:.3f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting PR curve for {name}: {str(e)}\")\n",
    "    \n",
    "    # Add legends if we have valid curves\n",
    "    if valid_roc_lines:\n",
    "        ax_roc.legend(valid_roc_lines, valid_roc_labels, loc='lower right', fontsize=8, framealpha=0.7)\n",
    "    else:\n",
    "        ax_roc.text(0.5, 0.5, \"No valid ROC curves\", ha='center', va='center')\n",
    "        \n",
    "    if valid_pr_lines:\n",
    "        ax_pr.legend(valid_pr_lines, valid_pr_labels, loc='upper right', fontsize=8, framealpha=0.7)\n",
    "    else:\n",
    "        ax_pr.text(0.5, 0.5, \"No valid PR curves\", ha='center', va='center')\n",
    "    \n",
    "    # 4. Model ranking - sort by F1 or ROC AUC\n",
    "    ax_rank = fig.add_subplot(gs[1, 1:])\n",
    "    \n",
    "    # Sort by preferred metric\n",
    "    if 'roc_auc' in metrics_df.columns:\n",
    "        sort_metric = 'roc_auc'\n",
    "    elif 'f1' in metrics_df.columns:\n",
    "        sort_metric = 'f1'\n",
    "    else:\n",
    "        sort_metric = metrics_df.columns[0]\n",
    "    \n",
    "    sorted_metrics = metrics_df.sort_values(by=sort_metric, ascending=False)\n",
    "    \n",
    "    # Calculate the range of values to set better axis limits\n",
    "    min_val = sorted_metrics[sort_metric].min()\n",
    "    max_val = sorted_metrics[sort_metric].max()\n",
    "    range_val = max_val - min_val\n",
    "    \n",
    "    # Adjust axis limits based on data range\n",
    "    if range_val < 0.2:  # If the range is less than 0.2\n",
    "        padding = range_val * 0.1  # 10% padding\n",
    "        x_min = max(0, min_val - padding)\n",
    "        x_max = min(1, max_val + padding)\n",
    "        \n",
    "        # Make sure we have a reasonable range\n",
    "        if x_max - x_min < 0.05:\n",
    "            # Add more padding to make the range visible\n",
    "            center = (x_min + x_max) / 2\n",
    "            x_min = max(0, center - 0.05)\n",
    "            x_max = min(1, center + 0.05)\n",
    "    else:\n",
    "        # Use standard 0-1 range for wider spread\n",
    "        x_min = 0\n",
    "        x_max = 1\n",
    "    \n",
    "    # Plot horizontal bars for the chosen metric with gradient colors\n",
    "    bar_colors = plt.cm.viridis(np.linspace(0, 1, len(sorted_metrics)))\n",
    "    \n",
    "    bars = ax_rank.barh(\n",
    "        sorted_metrics.index,\n",
    "        sorted_metrics[sort_metric],\n",
    "        height=0.6,\n",
    "        color=bar_colors\n",
    "    )\n",
    "    \n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax_rank.text(\n",
    "            width + (x_max - x_min) * 0.01,  # Small offset\n",
    "            bar.get_y() + bar.get_height()/2,\n",
    "            f'{width:.3f}',\n",
    "            ha='left', va='center', fontsize=10\n",
    "        )\n",
    "    \n",
    "    # Customize plot\n",
    "    ax_rank.set_title(f'Models Ranked by {sort_metric.upper()}', fontsize=14, pad=20)\n",
    "    ax_rank.set_xlabel(f'{sort_metric.upper()} Score', fontsize=12)\n",
    "    ax_rank.set_xlim(x_min, x_max)\n",
    "    ax_rank.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add a main title to the figure\n",
    "    plt.suptitle('Classification Model Performance Analysis', fontsize=16, y=0.98)\n",
    "    \n",
    "    # Adjust layout with more space\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92, hspace=0.35, wspace=0.35)\n",
    "    \n",
    "    return fig, metrics_df\n",
    "\n",
    "def plot_prediction_distributions(model_results, y_test):\n",
    "    \"\"\"\n",
    "    Plot the distribution of prediction probabilities for each model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_results : dict\n",
    "        Dictionary of model results from train_evaluate_models()\n",
    "    y_test : array-like\n",
    "        True labels for test data\n",
    "    \"\"\"\n",
    "    # Check if we have probability predictions\n",
    "    models_with_probs = [name for name, results in model_results.items() \n",
    "                         if 'probabilities' in results]\n",
    "    \n",
    "    if not models_with_probs:\n",
    "        print(\"No probability predictions available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Set up the figure\n",
    "    n_models = len(models_with_probs)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(n_models*5, 5), sharey=True)\n",
    "    \n",
    "    # If only one model, axes won't be an array\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Use viridis colors\n",
    "    colors = get_viridis_colors(2)  # Two colors for binary classification\n",
    "    \n",
    "    # Plot each model's prediction distribution\n",
    "    for i, model_name in enumerate(models_with_probs):\n",
    "        probs = model_results[model_name]['probabilities']\n",
    "        \n",
    "        # Create separate distributions for positive and negative classes\n",
    "        df = pd.DataFrame({\n",
    "            'probability': probs,\n",
    "            'true_class': y_test\n",
    "        })\n",
    "        \n",
    "        # Plot positive class (class 1)\n",
    "        sns.histplot(\n",
    "            df[df['true_class'] == 1]['probability'], \n",
    "            bins=20, \n",
    "            alpha=0.7,\n",
    "            ax=axes[i],\n",
    "            color=colors[1],\n",
    "            label='Positive Class'\n",
    "        )\n",
    "        \n",
    "        # Plot negative class (class 0)\n",
    "        sns.histplot(\n",
    "            df[df['true_class'] == 0]['probability'], \n",
    "            bins=20, \n",
    "            alpha=0.7,\n",
    "            ax=axes[i],\n",
    "            color=colors[0],\n",
    "            label='Negative Class'\n",
    "        )\n",
    "        \n",
    "        # Customize plot\n",
    "        axes[i].set_title(f'{model_name} Predictions', fontsize=12, pad=10)\n",
    "        axes[i].set_xlabel('Prediction Probability', fontsize=10)\n",
    "        if i == 0:\n",
    "            axes[i].set_ylabel('Count', fontsize=10)\n",
    "        \n",
    "        axes[i].legend()\n",
    "        axes[i].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and print separation metrics\n",
    "    print(\"\\nPrediction Distribution Analysis:\")\n",
    "    for model_name in models_with_probs:\n",
    "        probs = model_results[model_name]['probabilities']\n",
    "        pos_probs = probs[y_test == 1]\n",
    "        neg_probs = probs[y_test == 0]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        pos_mean = np.mean(pos_probs)\n",
    "        neg_mean = np.mean(neg_probs)\n",
    "        separation = pos_mean - neg_mean\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Positive class average probability: {pos_mean:.4f}\")\n",
    "        print(f\"  Negative class average probability: {neg_mean:.4f}\")\n",
    "        print(f\"  Class separation: {separation:.4f}\")\n",
    "        \n",
    "        # Calculate % correctly separated\n",
    "        correct_pos = np.sum(pos_probs > 0.5) / len(pos_probs) * 100\n",
    "        correct_neg = np.sum(neg_probs < 0.5) / len(neg_probs) * 100\n",
    "        print(f\"  Correctly predicted positives: {correct_pos:.1f}%\")\n",
    "        print(f\"  Correctly predicted negatives: {correct_neg:.1f}%\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# %%\n",
    "def plot_feature_importance(model, feature_names, top_n=10, model_name=None):\n",
    "    \"\"\"\n",
    "    Plot feature importances for a given model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : fitted model object\n",
    "        Model with feature_importances_ attribute or coef_ attribute\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    top_n : int, optional\n",
    "        Number of top features to show\n",
    "    model_name : str, optional\n",
    "        Name of the model for the plot title\n",
    "    \"\"\"\n",
    "    # Extract feature importances from model\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Tree-based models\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        # Linear models\n",
    "        importances = np.abs(model.coef_[0]) if len(model.coef_.shape) > 1 else np.abs(model.coef_)\n",
    "    else:\n",
    "        print(\"Model doesn't have standard feature importance attributes.\")\n",
    "        return\n",
    "    \n",
    "    # Create DataFrame for better sorting and handling\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort and get top N features\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False).head(top_n)\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot bars with color gradient\n",
    "    bars = plt.barh(\n",
    "        y=importance_df['Feature'],\n",
    "        width=importance_df['Importance'],\n",
    "        color=plt.cm.viridis(np.linspace(0, 0.8, len(importance_df)))\n",
    "    )\n",
    "    \n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(\n",
    "            width * 1.01,\n",
    "            bar.get_y() + bar.get_height()/2,\n",
    "            f'{width:.3f}',\n",
    "            va='center',\n",
    "            fontsize=10\n",
    "        )\n",
    "    \n",
    "    # Customize plot\n",
    "    model_title = model_name if model_name else model.__class__.__name__\n",
    "    plt.title(f'Feature Importance: {model_title}', fontsize=14, pad=20)\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "def plot_confusion_matrix(y_true, y_pred, classes=None, normalize=False, title=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with improved visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Ground truth labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    classes : list, optional\n",
    "        List of class names\n",
    "    normalize : bool, default=False\n",
    "        Whether to normalize confusion matrix\n",
    "    title : str, optional\n",
    "        Title for the plot\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Normalize if requested\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.2f'\n",
    "    else:\n",
    "        fmt = 'd'\n",
    "    \n",
    "    # Set up plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Use viridis colormap\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt=fmt, \n",
    "        cmap='viridis',\n",
    "        cbar=True,\n",
    "        square=True,\n",
    "        xticklabels=classes if classes else ['Negative', 'Positive'],\n",
    "        yticklabels=classes if classes else ['Negative', 'Positive']\n",
    "    )\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title if title else 'Confusion Matrix', fontsize=14, pad=20)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    \n",
    "    # Add accuracy in the bottom right\n",
    "    accuracy = (cm[0,0] + cm[1,1]) / cm.sum()\n",
    "    plt.text(\n",
    "        cm.shape[1] - 0.5, \n",
    "        cm.shape[0] + 0.2,\n",
    "        f'Accuracy: {accuracy:.4f}',\n",
    "        fontsize=10,\n",
    "        ha='center'\n",
    "    )\n",
    "    \n",
    "    # Better layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_prediction_distributions_enhanced(model_results, y_test, figsize=(14, 10)):\n",
    "    \"\"\"\n",
    "    Plot enhanced distribution of prediction probabilities for each model with separation analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_results : dict\n",
    "        Dictionary of model results from train_evaluate_models()\n",
    "    y_test : array-like\n",
    "        True labels for test data\n",
    "    figsize : tuple, optional\n",
    "        Figure size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        The figure containing the visualizations\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Check if we have probability predictions\n",
    "    models_with_probs = [name for name, results in model_results.items()\n",
    "                        if 'probabilities' in results]\n",
    "    \n",
    "    if not models_with_probs:\n",
    "        print(\"No probability predictions available for visualization\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_models = len(models_with_probs)\n",
    "    n_cols = min(3, n_models)\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = plt.GridSpec(n_rows + 1, n_cols, height_ratios=[3] * n_rows + [2])\n",
    "    \n",
    "    # Use viridis colors\n",
    "    colors = plt.cm.viridis([0.2, 0.8])  # Two colors for binary classification\n",
    "    \n",
    "    # Collect separation metrics for all models\n",
    "    separation_metrics = []\n",
    "    \n",
    "    # Plot each model's prediction distribution\n",
    "    for i, model_name in enumerate(models_with_probs):\n",
    "        row, col = i // n_cols, i % n_cols\n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "        \n",
    "        probs = model_results[model_name]['probabilities']\n",
    "        \n",
    "        # Create separate distributions for positive and negative classes\n",
    "        df = pd.DataFrame({\n",
    "            'probability': probs,\n",
    "            'true_class': y_test\n",
    "        })\n",
    "        \n",
    "        # Plot positive class (class 1)\n",
    "        sns.histplot(\n",
    "            df[df['true_class'] == 1]['probability'],\n",
    "            bins=20,\n",
    "            alpha=0.7,\n",
    "            ax=ax,\n",
    "            color=colors[1],\n",
    "            label='Positive Class'\n",
    "        )\n",
    "        \n",
    "        # Plot negative class (class 0)\n",
    "        sns.histplot(\n",
    "            df[df['true_class'] == 0]['probability'],\n",
    "            bins=20,\n",
    "            alpha=0.7,\n",
    "            ax=ax,\n",
    "            color=colors[0],\n",
    "            label='Negative Class'\n",
    "        )\n",
    "        \n",
    "        # Calculate separation metrics\n",
    "        pos_probs = probs[y_test == 1]\n",
    "        neg_probs = probs[y_test == 0]\n",
    "        \n",
    "        pos_mean = np.mean(pos_probs)\n",
    "        neg_mean = np.mean(neg_probs)\n",
    "        separation = pos_mean - neg_mean\n",
    "        \n",
    "        # Calculate % correctly separated\n",
    "        correct_pos = np.sum(pos_probs > 0.5) / len(pos_probs) * 100\n",
    "        correct_neg = np.sum(neg_probs < 0.5) / len(neg_probs) * 100\n",
    "        \n",
    "        # Calculate optimal threshold using Youden's J statistic\n",
    "        thresholds = np.linspace(0, 1, 100)\n",
    "        j_values = np.zeros_like(thresholds)\n",
    "        \n",
    "        for j, threshold in enumerate(thresholds):\n",
    "            tp = np.sum((probs >= threshold) & (y_test == 1))\n",
    "            fp = np.sum((probs >= threshold) & (y_test == 0))\n",
    "            tn = np.sum((probs < threshold) & (y_test == 0))\n",
    "            fn = np.sum((probs < threshold) & (y_test == 1))\n",
    "            \n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            \n",
    "            j_values[j] = sensitivity + specificity - 1\n",
    "        \n",
    "        optimal_idx = np.argmax(j_values)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        \n",
    "        # Add vertical line for optimal threshold\n",
    "        ax.axvline(optimal_threshold, color='red', linestyle='--', alpha=0.7,\n",
    "                 label=f'Optimal threshold: {optimal_threshold:.2f}')\n",
    "        \n",
    "        # Add separation metrics to the chart\n",
    "        ax.text(0.05, 0.95, \n",
    "              f\"Separation: {separation:.3f}\\nPos mean: {pos_mean:.3f}\\nNeg mean: {neg_mean:.3f}\", \n",
    "              transform=ax.transAxes, fontsize=9,\n",
    "              verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_title(f'{model_name}', fontsize=12, pad=10)\n",
    "        ax.set_xlabel('Prediction Probability', fontsize=10)\n",
    "        if col == 0:\n",
    "            ax.set_ylabel('Count', fontsize=10)\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(alpha=0.3)\n",
    "        \n",
    "        # Store metrics for comparison\n",
    "        roc_auc = roc_auc_score(y_test, probs)\n",
    "        separation_metrics.append({\n",
    "            'Model': model_name,\n",
    "            'AUC': roc_auc,\n",
    "            'Separation': separation,\n",
    "            'Pos Mean': pos_mean,\n",
    "            'Neg Mean': neg_mean,\n",
    "            'Correct Pos %': correct_pos,\n",
    "            'Correct Neg %': correct_neg,\n",
    "            'Optimal Threshold': optimal_threshold\n",
    "        })\n",
    "    \n",
    "    # Create a summary table at the bottom\n",
    "    ax_table = fig.add_subplot(gs[-1, :])\n",
    "    \n",
    "    # Convert to DataFrame for easier handling\n",
    "    metrics_df = pd.DataFrame(separation_metrics)\n",
    "    \n",
    "    # Format the table\n",
    "    cell_text = []\n",
    "    for _, row in metrics_df.iterrows():\n",
    "        cell_text.append([\n",
    "            row['Model'], \n",
    "            f\"{row['AUC']:.3f}\", \n",
    "            f\"{row['Separation']:.3f}\", \n",
    "            f\"{row['Pos Mean']:.3f}\", \n",
    "            f\"{row['Neg Mean']:.3f}\", \n",
    "            f\"{row['Correct Pos %']:.1f}%\", \n",
    "            f\"{row['Correct Neg %']:.1f}%\",\n",
    "            f\"{row['Optimal Threshold']:.3f}\"\n",
    "        ])\n",
    "    \n",
    "    column_labels = ['Model', 'AUC', 'Separation', 'Pos Mean', 'Neg Mean', \n",
    "                    'Correct Pos %', 'Correct Neg %', 'Opt. Threshold']\n",
    "    \n",
    "    # Hide axes\n",
    "    ax_table.axis('tight')\n",
    "    ax_table.axis('off')\n",
    "    \n",
    "    # Create table with colored rows\n",
    "    table = ax_table.table(\n",
    "        cellText=cell_text,\n",
    "        colLabels=column_labels,\n",
    "        loc='center',\n",
    "        cellLoc='center',\n",
    "        colColours=['#f0f0f0'] * len(column_labels)\n",
    "    )\n",
    "    \n",
    "    # Style the table\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    # Add a title to the table\n",
    "    ax_table.set_title('Model Separation Metrics Comparison', fontsize=14, pad=20)\n",
    "    \n",
    "    # Add main title\n",
    "    plt.suptitle('Prediction Probability Distributions by Class', fontsize=16, y=0.98)\n",
    "    \n",
    "    # Better layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92, hspace=0.4, wspace=0.3)\n",
    "    \n",
    "    return fig, metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4df4c3",
   "metadata": {},
   "source": [
    "\n",
    "#### Part 8: Interactive Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8450bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dashboard(df):\n",
    "    \"\"\"\n",
    "    Create a comprehensive interactive dashboard for Chicago traffic crash analysis\n",
    "    that includes all visualizations (original and new) in a single dashboard.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Processed dataframe with crash data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    plotly.graph_objects.Figure\n",
    "        Interactive dashboard figure\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    \n",
    "    # Define consistent colors from viridis palette\n",
    "    VIRIDIS_COLORS = {\n",
    "        'main': '#6a50a7',  # Light purple\n",
    "        'secondary': '#52c2c4',  # Light teal\n",
    "        'tertiary': '#8ddc6e',  # Light green\n",
    "        'highlight': '#fee04c',  # Light yellow\n",
    "    }\n",
    "    \n",
    "    # Create a comprehensive dashboard with multiple sections\n",
    "    # Important: Set the map subplot type as \"mapbox\" instead of \"xy\"\n",
    "    fig = make_subplots(\n",
    "        rows=5, cols=2,\n",
    "        subplot_titles=[\n",
    "            'Crashes by Hour of Day', 'Crashes by Day of Week',\n",
    "            'Monthly Trend', 'Top Crash Types',\n",
    "            'Injury Rates by Crash Type', 'Severity by Weather',\n",
    "            'Monthly Crash Trends by Year', 'High-Risk Corridors by Injuries',\n",
    "            'Crash Density Map', 'Top 5 Streets Proportional Distribution'\n",
    "        ],\n",
    "        specs=[\n",
    "            [{\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "            [{\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "            [{\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "            [{\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "            [{\"type\": \"mapbox\"}, {\"type\": \"pie\"}]  # Changed from \"xy\" to \"mapbox\"\n",
    "        ],\n",
    "        vertical_spacing=0.08,\n",
    "        horizontal_spacing=0.05\n",
    "    )\n",
    "    \n",
    "    # 1. Crashes by Hour\n",
    "    hour_col = 'CRASH_HOUR' if 'CRASH_HOUR' in df.columns else 'HOUR'\n",
    "    hour_counts = df[hour_col].value_counts().sort_index()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=list(range(24)),\n",
    "            y=hour_counts.values,\n",
    "            marker_color='rgba(106, 80, 167, 0.8)',\n",
    "            name='Hourly Crashes'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Crashes by Day of Week\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    if 'DAY_OF_WEEK' in df.columns:\n",
    "        dow_counts = df.groupby('DAY_OF_WEEK').size()\n",
    "        # Ensure days are in correct order\n",
    "        dow_counts = dow_counts.reindex(days).fillna(0)\n",
    "    else:\n",
    "        dow_counts = df.groupby('CRASH_DAY_OF_WEEK').size().sort_index()\n",
    "        # Map numeric days to names if needed\n",
    "        if dow_counts.index.dtype == 'int64' or dow_counts.index.dtype == 'int32':\n",
    "            dow_counts.index = [days[i-1] for i in dow_counts.index]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=dow_counts.index,\n",
    "            y=dow_counts.values,\n",
    "            marker_color='rgba(82, 194, 196, 0.8)',\n",
    "            name='Daily Crashes'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Monthly Trend\n",
    "    # Handle either date column name\n",
    "    date_col = 'CRASH_DATE'\n",
    "    if date_col not in df.columns and 'CRASH_DATETIME' in df.columns:\n",
    "        date_col = 'CRASH_DATETIME'\n",
    "    \n",
    "    # Make sure date column is datetime\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Create year-month string for grouping\n",
    "    df['YearMonth'] = df[date_col].dt.strftime('%Y-%m')\n",
    "    monthly_counts = df.groupby('YearMonth').size()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=monthly_counts.index,\n",
    "            y=monthly_counts.values,\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='rgba(141, 220, 110, 0.8)', width=2),\n",
    "            marker=dict(size=6),\n",
    "            name='Monthly Crashes'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Top Crash Types\n",
    "    if 'FIRST_CRASH_TYPE' in df.columns:\n",
    "        crash_types = df['FIRST_CRASH_TYPE'].value_counts().nlargest(10)\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                y=crash_types.index,\n",
    "                x=crash_types.values,\n",
    "                orientation='h',\n",
    "                marker_color='rgba(254, 224, 76, 0.8)',\n",
    "                name='Crash Types'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # 5. Injury Rates by Crash Type\n",
    "    if 'FIRST_CRASH_TYPE' in df.columns and 'INJURIES_TOTAL' in df.columns:\n",
    "        df['injury_flag'] = (df['INJURIES_TOTAL'] > 0).astype(int)\n",
    "        injury_by_type = df.groupby('FIRST_CRASH_TYPE')['injury_flag'].mean().sort_values(ascending=False).head(10)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                y=injury_by_type.index,\n",
    "                x=injury_by_type.values * 100,  # Convert to percentage\n",
    "                orientation='h',\n",
    "                marker_color='rgba(106, 80, 167, 0.8)',\n",
    "                name='Injury Rate'\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "    \n",
    "    # 6. Severity by Weather\n",
    "    if 'WEATHER_CONDITION' in df.columns and 'SEVERE' in df.columns:\n",
    "        weather_severity = df.groupby('WEATHER_CONDITION')['SEVERE'].mean().sort_values(ascending=False).head(8)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                y=weather_severity.index,\n",
    "                x=weather_severity.values * 100,  # Convert to percentage\n",
    "                orientation='h',\n",
    "                marker_color='rgba(82, 194, 196, 0.8)',\n",
    "                name='Severity Rate'\n",
    "            ),\n",
    "            row=3, col=2\n",
    "        )\n",
    "    \n",
    "    # 7. Monthly Crash Trends by Year (NEW - Image 1)\n",
    "    df['Year'] = df[date_col].dt.year\n",
    "    df['Month'] = df[date_col].dt.month\n",
    "    \n",
    "    years = sorted(df['Year'].unique())\n",
    "    colors = ['rgba(106, 80, 167, 0.8)', 'rgba(82, 194, 196, 0.8)', \n",
    "              'rgba(141, 220, 110, 0.8)', 'rgba(254, 224, 76, 0.8)',\n",
    "              'rgba(156, 110, 177, 0.8)', 'rgba(127, 191, 123, 0.8)']\n",
    "    \n",
    "    for i, year in enumerate(years):\n",
    "        year_data = df[df['Year'] == year]\n",
    "        monthly = year_data.groupby('Month').size()\n",
    "        \n",
    "        # Make sure all months are represented\n",
    "        all_months = pd.Series(0, index=range(1, 13))\n",
    "        monthly = monthly.add(all_months, fill_value=0)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=list(range(1, 13)),\n",
    "                y=monthly.values,\n",
    "                mode='lines+markers',\n",
    "                name=str(year),\n",
    "                line=dict(color=colors[i % len(colors)]),\n",
    "                marker=dict(size=8)\n",
    "            ),\n",
    "            row=4, col=1\n",
    "        )\n",
    "    \n",
    "    # Add average line for monthly trends\n",
    "    all_monthly = df.groupby('Month').size()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(1, 13)),\n",
    "            y=[all_monthly.mean()] * 12,\n",
    "            mode='lines',\n",
    "            line=dict(color='red', dash='dash'),\n",
    "            name='Monthly Average'\n",
    "        ),\n",
    "        row=4, col=1\n",
    "    )\n",
    "    \n",
    "    # 8. Street Injury Analysis (NEW - Image 2)\n",
    "    if 'INJURIES_TOTAL' in df.columns:\n",
    "        # Find street column\n",
    "        potential_street_cols = [\n",
    "            'STREET_NAME', 'STREET_NO', 'STREET', 'PRIMARY_STREET', \n",
    "            'STREET_NAME_PRIMARY', 'STREET_NO_PRIMARY', 'STREET_NO_1'\n",
    "        ]\n",
    "        \n",
    "        street_col = None\n",
    "        for col in potential_street_cols:\n",
    "            if col in df.columns:\n",
    "                street_col = col\n",
    "                break\n",
    "        \n",
    "        if street_col is None:\n",
    "            street_cols = [col for col in df.columns if 'STREET' in col.upper()]\n",
    "            if street_cols:\n",
    "                street_col = street_cols[0]\n",
    "        \n",
    "        if street_col:\n",
    "            street_injuries = df.groupby(street_col)['INJURIES_TOTAL'].sum().sort_values(ascending=False)\n",
    "            \n",
    "            # Filter out empty street names\n",
    "            if street_injuries.index.dtype == 'object':\n",
    "                street_injuries = street_injuries[street_injuries.index.str.strip() != '']\n",
    "            \n",
    "            top_streets = street_injuries.nlargest(10)\n",
    "            \n",
    "            # Bar chart for high-risk corridors\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    y=top_streets.index,\n",
    "                    x=top_streets.values,\n",
    "                    orientation='h',\n",
    "                    marker_color='rgba(106, 80, 167, 0.8)',\n",
    "                    name='Street Injuries'\n",
    "                ),\n",
    "                row=4, col=2\n",
    "            )\n",
    "            \n",
    "            # Pie chart for top 5 streets distribution\n",
    "            top5_streets = top_streets[:5]\n",
    "            fig.add_trace(\n",
    "                go.Pie(\n",
    "                    labels=top5_streets.index,\n",
    "                    values=top5_streets.values,\n",
    "                    textinfo='label+percent',\n",
    "                    hole=0.3,\n",
    "                    marker=dict(colors=colors[:5]),\n",
    "                    name='Top 5 Streets'\n",
    "                ),\n",
    "                row=5, col=2\n",
    "            )\n",
    "    \n",
    "    # 9. Crash Map \n",
    "    if 'LATITUDE' in df.columns and 'LONGITUDE' in df.columns:\n",
    "        # Filter to valid coordinates\n",
    "        valid_coords = df[(df['LATITUDE'] > 41.5) & (df['LATITUDE'] < 42.1) &\n",
    "                          (df['LONGITUDE'] > -88.0) & (df['LONGITUDE'] < -87.4)]\n",
    "        \n",
    "        if len(valid_coords) > 0:\n",
    "            # Create a density mapbox instead of scattermapbox\n",
    "            fig.add_trace(\n",
    "                go.Densitymapbox(\n",
    "                    lat=valid_coords['LATITUDE'],\n",
    "                    lon=valid_coords['LONGITUDE'],\n",
    "                    z=valid_coords['INJURIES_TOTAL'] if 'INJURIES_TOTAL' in valid_coords.columns else None,\n",
    "                    radius=10,\n",
    "                    colorscale='Viridis',\n",
    "                    name='Crash Density'\n",
    "                ),\n",
    "                row=5, col=1\n",
    "            )\n",
    "            \n",
    "            # Update mapbox configuration\n",
    "            fig.update_layout(\n",
    "                mapbox=dict(\n",
    "                    style=\"carto-positron\",\n",
    "                    center=dict(\n",
    "                        lat=valid_coords['LATITUDE'].mean(),\n",
    "                        lon=valid_coords['LONGITUDE'].mean()\n",
    "                    ),\n",
    "                    zoom=9\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # Fallback if no valid coordinates\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[0],\n",
    "                    y=[0],\n",
    "                    mode='text',\n",
    "                    text=['No valid coordinate data available'],\n",
    "                    textposition='middle center',\n",
    "                ),\n",
    "                row=5, col=1\n",
    "            )\n",
    "    else:\n",
    "        # If no coordinates, show hour vs day heatmap instead\n",
    "        try:\n",
    "            # Create pivot table for heatmap\n",
    "            if 'DAY_OF_WEEK' in df.columns:\n",
    "                pivot_df = df.groupby(['HOUR', 'DAY_OF_WEEK']).size().reset_index(name='count')\n",
    "                pivot_wide = pivot_df.pivot(index='HOUR', columns='DAY_OF_WEEK', values='count')\n",
    "                pivot_wide = pivot_wide.reindex(columns=days).fillna(0)\n",
    "            else:\n",
    "                pivot_df = df.groupby(['HOUR', 'CRASH_DAY_OF_WEEK']).size().reset_index(name='count')\n",
    "                pivot_wide = pivot_df.pivot(index='HOUR', columns='CRASH_DAY_OF_WEEK', values='count')\n",
    "                day_mapping = {i+1: day for i, day in enumerate(days)}\n",
    "                pivot_wide = pivot_wide.rename(columns=day_mapping).fillna(0)\n",
    "            \n",
    "            # Change the specs for row 5, col 1 to \"xy\" instead of \"mapbox\"\n",
    "            fig.update_layout(\n",
    "                {\"grid\": {\"rows\": 5, \"columns\": 2, \"pattern\": \"independent\"},\n",
    "                 \"template\": \"plotly_white\"}\n",
    "            )\n",
    "            \n",
    "            # Change subplot type for the last row, first column\n",
    "            fig._grid_ref[-1][0] = \"xy\"\n",
    "            \n",
    "            # Create heatmap (no longer mapbox)\n",
    "            fig.add_trace(\n",
    "                go.Heatmap(\n",
    "                    z=pivot_wide.values,\n",
    "                    x=pivot_wide.columns,\n",
    "                    y=list(range(24)),\n",
    "                    colorscale='Viridis',\n",
    "                    colorbar=dict(title='Count'),\n",
    "                    name='Hourly Pattern'\n",
    "                ),\n",
    "                row=5, col=1\n",
    "            )\n",
    "            \n",
    "            # Update subplot title\n",
    "            fig.layout.annotations[8].text = 'Hour of Day vs. Day of Week Heatmap'\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create heatmap: {str(e)}\")\n",
    "            # Fallback - simple text\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[0.5],\n",
    "                    y=[0.5],\n",
    "                    mode='text',\n",
    "                    text=['No spatial or temporal data available for visualization'],\n",
    "                    textposition='middle center',\n",
    "                ),\n",
    "                row=5, col=1\n",
    "            )\n",
    "    \n",
    "    # Update layout for better appearance\n",
    "    fig.update_layout(\n",
    "        height=1600,\n",
    "        width=1200,\n",
    "        title_text=\"Chicago Traffic Crash Analysis Dashboard\",\n",
    "        template=\"plotly_white\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Hour of Day\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Number of Crashes\", row=1, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Day of Week\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Number of Crashes\", row=1, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Month\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Number of Crashes\", row=2, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Count\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Crash Type\", row=2, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Injury Rate (%)\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Crash Type\", row=3, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Severity Rate (%)\", row=3, col=2)\n",
    "    fig.update_yaxes(title_text=\"Weather Condition\", row=3, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Month\", tickvals=list(range(1, 13)), \n",
    "                     ticktext=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "                     row=4, col=1)\n",
    "    fig.update_yaxes(title_text=\"Number of Crashes\", row=4, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Total Injuries\", row=4, col=2)\n",
    "    fig.update_yaxes(title_text=\"Street\", row=4, col=2)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94afb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file_path, sample_size=None, random_seed=45665456, use_advanced_models=False):\n",
    "    \"\"\"\n",
    "    Run the complete analysis workflow with enhanced visualizations.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the crash data CSV\n",
    "    sample_size : int, optional\n",
    "        Number of records to sample (for faster processing)\n",
    "    random_seed : int, default=45665456\n",
    "        Random seed for reproducibility\n",
    "    use_advanced_models : bool, default=False\n",
    "        Whether to use advanced models and techniques\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    print(\"===== Starting Traffic Crash Analysis =====\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Load data\n",
    "    df = load_data(file_path)\n",
    "\n",
    "    # 2. Sample data if needed\n",
    "    if sample_size is not None and sample_size < len(df):\n",
    "        df = df.sample(sample_size, random_state=random_seed)\n",
    "        print(f\"Sampled {sample_size} records for analysis\")\n",
    "\n",
    "    # 3. Preprocess data\n",
    "    print(\"\\n===== Preprocessing Data =====\")\n",
    "    df_processed = preprocess_data(df)\n",
    "\n",
    "    # 4. Analyze missing data\n",
    "    print(\"\\n===== Missing Data Analysis =====\")\n",
    "    missing_df = analyze_missing_data(df_processed)\n",
    "\n",
    "    # 5. Feature engineering (do this early to have features for plots)\n",
    "    print(\"\\n===== Feature Engineering =====\")\n",
    "    try:\n",
    "        df_features = engineer_features(df_processed)\n",
    "\n",
    "        # 6. All visualizations that don't depend on model training\n",
    "        print(\"\\n===== Exploratory Data Analysis =====\")\n",
    "        \n",
    "        # 6.1 Temporal analysis\n",
    "        print(\"Plotting crash counts by hour...\")\n",
    "        plot_crashes_by_hour(df_features)\n",
    "        \n",
    "        print(\"Plotting crash counts by day of week...\")\n",
    "        plot_crashes_by_day_of_week(df_features)\n",
    "        \n",
    "        print(\"Plotting crash counts by day of week (donut chart)...\")\n",
    "        plot_crashes_by_weekday_donut(df_features)\n",
    "        \n",
    "        print(\"Plotting monthly trends...\")\n",
    "        plot_monthly_trend(df_features)\n",
    "        \n",
    "        print(\"Creating temporal heatmap...\")\n",
    "        plot_temporal_heatmap(df_features)\n",
    "        \n",
    "        # NEW: Add monthly crash trends by year (Image 1)\n",
    "        print(\"Creating monthly crash trends by year...\")\n",
    "        monthly_trends_fig = plot_monthly_crash_trends_by_year(df_features, start_year=2019)\n",
    "\n",
    "        # 6.2 Crash types and causes\n",
    "        print(\"Plotting top crash types...\")\n",
    "        plot_crash_types(df_features)\n",
    "\n",
    "        # 6.3 Feature distributions\n",
    "        print(\"Creating time period visualizations...\")\n",
    "        create_time_period_visualizations(df_features)\n",
    "\n",
    "        # 6.4 Severity and injury analysis\n",
    "        print(\"Analyzing severity by factors...\")\n",
    "        factors = ['FIRST_CRASH_TYPE', 'WEATHER_CONDITION', 'ROADWAY_SURFACE_COND',\n",
    "                'TIME_OF_DAY', 'SEASON']\n",
    "        for factor in factors:\n",
    "            if factor in df_features.columns:  # Check if factor exists\n",
    "                plot_severity_by_factor(df_features, factor, top_n=10)\n",
    "            else:\n",
    "                print(f\"Factor {factor} not found in data\")\n",
    "        \n",
    "        print(\"Plotting injury rates by crash type...\")\n",
    "        plot_injury_rate_by_crash_type(df_features)\n",
    "        \n",
    "        print(\"Plotting injury rates by contributory cause...\")\n",
    "        plot_injury_rate_by_cause(df_features)\n",
    "        \n",
    "        print(\"Plotting injury severity by weather condition...\")\n",
    "        plot_injury_severity_by_weather(df_features)\n",
    "        \n",
    "        # NEW: Add street injury analysis (Image 2)\n",
    "        print(\"Creating street injury analysis...\")\n",
    "        street_injury_fig = plot_injury_analysis_by_street(df_features, top_n=10)\n",
    "\n",
    "        # 6.5 Speed limit analysis\n",
    "        print(\"Plotting injuries by speed limit bin...\")\n",
    "        plot_injuries_by_speed_bin(df_features)\n",
    "\n",
    "        # 6.6 Spatial analysis\n",
    "        if 'LATITUDE' in df_features.columns and 'LONGITUDE' in df_features.columns:\n",
    "            print(\"\\n===== Spatial Analysis =====\")\n",
    "            print(\"Generating hexbin plot of crash density...\")\n",
    "            plot_hex_crash_density(df_features)\n",
    "\n",
    "            print(\"Creating interactive folium heatmap...\")\n",
    "\n",
    "            \n",
    "            # Create folium map\n",
    "            m = folium.Map(location=[41.88, -87.63], zoom_start=11, tiles=\"CartoDB positron\")\n",
    "            \n",
    "            # Filter to valid bounds for Chicago\n",
    "            mask = (\n",
    "                (df_features['LATITUDE'] > 41.5) & (df_features['LATITUDE'] < 42.1) &\n",
    "                (df_features['LONGITUDE'] > -88.0) & (df_features['LONGITUDE'] < -87.4)\n",
    "            )\n",
    "            geo = df_features[mask]\n",
    "            \n",
    "            # Create heat data points\n",
    "            heat_data = list(zip(geo['LATITUDE'].values, geo['LONGITUDE'].values))\n",
    "            \n",
    "            # Add heat map\n",
    "            HeatMap(heat_data, radius=10, blur=15, min_opacity=0.3).add_to(m)\n",
    "            \n",
    "            # Display the map if in a notebook\n",
    "            try:\n",
    "                from IPython.display import display\n",
    "                display(m)\n",
    "            except:\n",
    "                print(\"Folium map created (visible in notebook environments)\")\n",
    "            \n",
    "            # Save map\n",
    "            try:\n",
    "                m.save(\"chicago_crash_heatmap.html\")\n",
    "                print(\"Saved interactive map to chicago_crash_heatmap.html\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not save map: {e}\")\n",
    "\n",
    "\n",
    "        # 8. Continue with model preparation\n",
    "        print(\"\\n===== Preparing for Modeling =====\")\n",
    "        \n",
    "        # Define base features\n",
    "        base_feats = [\n",
    "            'POSTED_SPEED_LIMIT', 'NUM_UNITS', 'CRASH_HOUR', 'CRASH_DAY_OF_WEEK', 'CRASH_MONTH'\n",
    "        ]\n",
    "        \n",
    "        # Verify base features exist\n",
    "        available_base_feats = [col for col in base_feats if col in df_features.columns]\n",
    "        if len(available_base_feats) < len(base_feats):\n",
    "            missing = set(base_feats) - set(available_base_feats)\n",
    "            print(f\"Warning: Some base features are missing: {missing}\")\n",
    "        \n",
    "        # Define engineered features\n",
    "        eng_feats = [\n",
    "            'IS_WEEKEND', 'IS_NIGHTTIME', 'IS_VULNERABLE', 'BAD_WEATHER',\n",
    "            'BAD_SURFACE', 'AT_INTERSECTION'\n",
    "        ]\n",
    "        \n",
    "        # Check engineered features\n",
    "        available_eng_feats = [col for col in eng_feats if col in df_features.columns]\n",
    "        \n",
    "        # Add advanced features if available\n",
    "        adv_feats = [col for col in df_features.columns if col.startswith((\n",
    "            'WEEKEND_', 'WEEKDAY_', 'NIGHT_', 'HIGH_SPEED', 'MULTI_',\n",
    "            'SINGLE_', 'VULNERABLE_', 'WINTER_', 'IS_HOLIDAY', 'SEASON_'))]\n",
    "        available_eng_feats.extend(adv_feats)\n",
    "        \n",
    "        # Check if we have enough features to proceed\n",
    "        if len(available_base_feats) == 0:\n",
    "            print(\"Error: No base features available for modeling\")\n",
    "            return {\n",
    "                'data': df_processed,\n",
    "                'error': 'Insufficient features for modeling',\n",
    "                'monthly_trends_fig': monthly_trends_fig if 'monthly_trends_fig' in locals() else None,\n",
    "                'street_injury_fig': street_injury_fig if 'street_injury_fig' in locals() else None,\n",
    "                'dashboard': dashboard if 'dashboard' in locals() else None\n",
    "            }\n",
    "        \n",
    "        # Prepare feature matrix - handling one-hot encoding safely\n",
    "        # Only use available features\n",
    "        features_to_use = available_base_feats + available_eng_feats\n",
    "        \n",
    "        # Safe way to create dummy variables\n",
    "        X = pd.get_dummies(df_features[features_to_use], drop_first=True).fillna(0)\n",
    "        \n",
    "        # Check if target variable exists\n",
    "        if 'SEVERE' not in df_features.columns:\n",
    "            print(\"Target variable 'SEVERE' not found - creating it\")\n",
    "            df_features['SEVERE'] = ((df_features['INJURIES_TOTAL'] > 0) |\n",
    "                                    (df_features['INJURIES_FATAL'] > 0)).astype(int)\n",
    "        y = df_features['SEVERE']\n",
    "        \n",
    "        # Optional: Feature selection\n",
    "        feature_results = None\n",
    "        if use_advanced_models:\n",
    "            print(\"\\n===== Feature Selection =====\")\n",
    "            feature_results = select_features(X, y)\n",
    "            print(\"\\nSelected features:\")\n",
    "            for feature in feature_results['consensus'][:15]:  # Show top 15\n",
    "                print(f\" - {feature}\")\n",
    "            \n",
    "            # Use selected features for modeling\n",
    "            X_selected = X[feature_results['consensus']]\n",
    "        else:\n",
    "            X_selected = X\n",
    "        \n",
    "        # 9. Model training and evaluation\n",
    "        print(\"\\n===== Model Training and Evaluation =====\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_selected, y, test_size=0.2, stratify=y, random_state=random_seed\n",
    "        )\n",
    "        \n",
    "        # Initialize model_results as None - this is important to prevent issues\n",
    "        model_results = None\n",
    "        \n",
    "        # Train and evaluate models\n",
    "        if use_advanced_models:\n",
    "            print(\"\\n===== Using Advanced Models =====\")\n",
    "            # Option 1: Use model selection to find best model\n",
    "            best_model_results = select_best_model(X_train, y_train, cv=5, scoring='f1')\n",
    "            best_model = best_model_results['best_model']\n",
    "            best_model_name = best_model_results['best_model_name']\n",
    "            \n",
    "            # Evaluate best model\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            y_prob = None\n",
    "            if hasattr(best_model, 'predict_proba'):\n",
    "                y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            print(f\"\\nBest model: {best_model_name}\")\n",
    "            print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "            print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "            print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "            print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "            \n",
    "            # Create simplified model_results for visualization\n",
    "            model_results = {\n",
    "                best_model_name: {\n",
    "                    'model': best_model,\n",
    "                    'metrics': {\n",
    "                        'accuracy': accuracy_score(y_test, y_pred),\n",
    "                        'precision': precision_score(y_test, y_pred),\n",
    "                        'recall': recall_score(y_test, y_pred),\n",
    "                        'f1': f1_score(y_test, y_pred)\n",
    "                    },\n",
    "                    'predictions': y_pred,\n",
    "                    'probabilities': y_prob\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Plot model performance and predictions\n",
    "            if y_prob is not None:\n",
    "                model_results[best_model_name]['metrics']['roc_auc'] = roc_auc_score(y_test, y_prob)\n",
    "                print(f\"ROC AUC: {model_results[best_model_name]['metrics']['roc_auc']:.4f}\")\n",
    "            \n",
    "            # Plot performance metrics\n",
    "            print(\"\\nPlotting model performance...\")\n",
    "            if model_results is not None:\n",
    "                plot_model_performance(model_results, y_test)\n",
    "            \n",
    "            # Use new enhanced function for classification metrics comparison\n",
    "            print(\"\\nGenerating detailed classification metrics comparison...\")\n",
    "            if model_results is not None:\n",
    "                fig_metrics, _ = plot_classification_metrics_comparison(model_results)\n",
    "            \n",
    "            # Plot prediction distributions\n",
    "            print(\"Plotting prediction distributions...\")\n",
    "            if y_prob is not None and model_results is not None:\n",
    "                try:\n",
    "                    fig_distributions, dist_metrics = plot_prediction_distributions_enhanced(model_results, y_test)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error plotting prediction distributions: {str(e)}\")\n",
    "                    fig_distributions = None\n",
    "            \n",
    "            # Analyze model with SHAP\n",
    "            print(\"\\n===== Model Interpretation with SHAP =====\")\n",
    "            shap_results = None\n",
    "            try:\n",
    "                shap_results = analyze_model_with_shap(best_model, X_test, X_selected.columns)\n",
    "            except Exception as e:\n",
    "                print(f\"SHAP analysis failed: {str(e)}\")\n",
    "            \n",
    "            # Plot feature importance for best model\n",
    "            print(f\"\\nPlotting feature importance for best model: {best_model_name}\")\n",
    "            if best_model is not None:\n",
    "                plot_feature_importance(best_model, X_selected.columns, model_name=best_model_name)\n",
    "            \n",
    "            # Plot confusion matrix for best model\n",
    "            print(f\"\\nPlotting confusion matrix for best model: {best_model_name}\")\n",
    "            if y_pred is not None:\n",
    "                plot_confusion_matrix(y_test, y_pred, title=f'Confusion Matrix: {best_model_name}')\n",
    "            \n",
    "            # Optional: Time series modeling if time data is available\n",
    "            ts_models = None\n",
    "            if 'CRASH_DATE' in df_features.columns:\n",
    "                print(\"\\n===== Time Series Modeling =====\")\n",
    "                try:\n",
    "                    ts_models = train_time_series_models(df_features)\n",
    "                except Exception as e:\n",
    "                    print(f\"Time series modeling failed: {str(e)}\")\n",
    "            \n",
    "            # Optional: Spatial modeling if coordinates are available\n",
    "            spatial_models = None\n",
    "            if 'LATITUDE' in df_features.columns and 'LONGITUDE' in df_features.columns:\n",
    "                print(\"\\n===== Spatial Modeling =====\")\n",
    "                try:\n",
    "                    spatial_models = train_spatial_models(df_features)\n",
    "                except Exception as e:\n",
    "                    print(f\"Spatial modeling failed: {str(e)}\")\n",
    "            \n",
    "            # Optional: Multi-task modeling if injury data is available\n",
    "            multitask_results = None\n",
    "            if 'INJURIES_TOTAL' in df_features.columns:\n",
    "                print(\"\\n===== Multi-task Modeling =====\")\n",
    "                try:\n",
    "                    y_reg = df_features['INJURIES_TOTAL']\n",
    "                    multitask_results = build_multitask_model(X_selected, y, y_reg)\n",
    "                except Exception as e:\n",
    "                    print(f\"Multi-task modeling failed: {str(e)}\")\n",
    "        else:\n",
    "            # Use standard models\n",
    "            try:\n",
    "                model_results = train_evaluate_models(X_train, X_test, y_train, y_test)\n",
    "                \n",
    "                # Plot standard model performance\n",
    "                print(\"\\nPlotting model performance...\")\n",
    "                if model_results is not None:\n",
    "                    plot_model_performance(model_results, y_test)\n",
    "                \n",
    "                # Also plot enhanced classification metrics comparison\n",
    "                print(\"\\nGenerating detailed classification metrics comparison...\")\n",
    "                if model_results is not None:\n",
    "                    try:\n",
    "                        fig_metrics, metric_ranking = plot_classification_metrics_comparison(model_results)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error plotting classification metrics: {str(e)}\")\n",
    "                        fig_metrics = None\n",
    "                \n",
    "                # Plot enhanced prediction distributions\n",
    "                print(\"Plotting enhanced prediction distributions...\")\n",
    "                if model_results is not None:\n",
    "                    try:\n",
    "                        fig_distributions, dist_metrics = plot_prediction_distributions_enhanced(model_results, y_test)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error plotting prediction distributions: {str(e)}\")\n",
    "                        fig_distributions = None\n",
    "                \n",
    "                # Find the best model based on ROC AUC\n",
    "                best_model_name = None\n",
    "                best_model = None\n",
    "                if model_results is not None:\n",
    "                    try:\n",
    "                        best_model_name = max(model_results.items(),\n",
    "                                            key=lambda x: x[1]['metrics']['roc_auc'])[0]\n",
    "                        best_model = model_results[best_model_name]['model']\n",
    "                        print(f\"\\nBest model: {best_model_name}\")\n",
    "                        \n",
    "                        # Plot feature importance for best model\n",
    "                        if best_model is not None:\n",
    "                            try:\n",
    "                                print(f\"\\nPlotting feature importance for best model: {best_model_name}\")\n",
    "                                plot_feature_importance(best_model, X_selected.columns, model_name=best_model_name)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error plotting feature importance: {str(e)}\")\n",
    "                        \n",
    "                        # Plot confusion matrix for best model\n",
    "                        if best_model_name in model_results:\n",
    "                            try:\n",
    "                                print(f\"\\nPlotting confusion matrix for best model: {best_model_name}\")\n",
    "                                best_preds = model_results[best_model_name]['predictions']\n",
    "                                plot_confusion_matrix(y_test, best_preds, title=f'Confusion Matrix: {best_model_name}')\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error plotting confusion matrix: {str(e)}\")\n",
    "                    except (KeyError, ValueError, TypeError) as e:\n",
    "                        print(f\"Could not determine best model: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in model training and evaluation: {str(e)}\")\n",
    "                model_results = None\n",
    "        \n",
    "        # 10. Injury count prediction (if available)\n",
    "        reg_results = None\n",
    "        if 'INJURIES_TOTAL' in df_features.columns:\n",
    "            print(\"\\n===== Injury Count Prediction =====\")\n",
    "            try:\n",
    "                y_reg = df_features['INJURIES_TOTAL']\n",
    "                \n",
    "                # Split data for regression\n",
    "                _, _, y_reg_train, y_reg_test = train_test_split(\n",
    "                    X, y_reg, test_size=0.2, random_state=random_seed\n",
    "                )\n",
    "                \n",
    "                # Train and evaluate regression models\n",
    "                reg_results = evaluate_regression_models(X_train, X_test, y_reg_train, y_reg_test)\n",
    "                print(\"\\nRegression model performance:\")\n",
    "                print(reg_results)\n",
    "                \n",
    "                # Generate enhanced regression metrics visualization\n",
    "                print(\"\\nGenerating regression metrics visualization...\")\n",
    "                try:\n",
    "                    fig_reg_metrics = plot_regression_metrics_comparison(reg_results)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error plotting regression metrics: {str(e)}\")\n",
    "                    fig_reg_metrics = None\n",
    "            except Exception as e:\n",
    "                print(f\"Regression modeling failed: {str(e)}\")\n",
    "                reg_results = None\n",
    "        \n",
    "        # Calculate execution time\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"\\nAnalysis completed in {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "        \n",
    "        # Create a dictionary to hold the results\n",
    "        results = {\n",
    "            'data': df_features,\n",
    "            'model_results': model_results,  # This could be None but that's now handled\n",
    "            'feature_results': feature_results,\n",
    "            'regression_results': reg_results,\n",
    "            'dashboard': dashboard if 'dashboard' in locals() else None,\n",
    "            'time_series_models': ts_models if 'ts_models' in locals() else None,\n",
    "            'spatial_models': spatial_models if 'spatial_models' in locals() else None,\n",
    "            'multitask_model': multitask_results if 'multitask_results' in locals() else None,\n",
    "            'shap_results': shap_results if 'shap_results' in locals() else None,\n",
    "            'monthly_trends_fig': monthly_trends_fig if 'monthly_trends_fig' in locals() else None,\n",
    "            'street_injury_fig': street_injury_fig if 'street_injury_fig' in locals() else None\n",
    "        }\n",
    "        \n",
    "        # Add optional figures if they exist\n",
    "        if 'fig_metrics' in locals() and fig_metrics is not None:\n",
    "            results['classification_metrics_fig'] = fig_metrics\n",
    "        if 'fig_distributions' in locals() and fig_distributions is not None:\n",
    "            results['prediction_distributions_fig'] = fig_distributions\n",
    "        if 'fig_reg_metrics' in locals() and fig_reg_metrics is not None:\n",
    "            results['regression_metrics_fig'] = fig_reg_metrics\n",
    "        if 'LATITUDE' in df_features.columns and 'm' in locals():\n",
    "            results['folium_map'] = m\n",
    "        try:\n",
    "            dashboard = create_dashboard(df_features)\n",
    "            print(\"Dashboard created successfully\")\n",
    "            results['dashboard'] = dashboard\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating dashboard: {str(e)}\")\n",
    "            results['dashboard'] = None\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in analysis pipeline: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'data': df_processed if 'df_processed' in locals() else None,\n",
    "            'error': str(e),\n",
    "            'model_results': None  # Ensure model_results is defined in error case too\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chicago_crash_analysis(file_path='Traffic_Crashes_-_Crashes.csv',\n",
    "                             sample_size=100000,\n",
    "                             use_advanced_models=False,\n",
    "                             save_results=True,\n",
    "                             output_folder='crash_analysis_results',\n",
    "                             show_plots=True):\n",
    "    \"\"\"\n",
    "    Run a comprehensive traffic crash analysis pipeline with enhanced visualizations.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the crash data CSV\n",
    "    sample_size : int, optional\n",
    "        Number of records to sample (for faster processing)\n",
    "    use_advanced_models : bool\n",
    "        Whether to use advanced models and techniques\n",
    "    save_results : bool\n",
    "        Whether to save the results to disk\n",
    "    output_folder : str\n",
    "        Folder to save results to (if save_results is True)\n",
    "    show_plots : bool\n",
    "        Whether to display plots during analysis\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing analysis results, models, and visualizations\n",
    "    \"\"\"\n",
    "    # Check for required dependencies\n",
    "    missing_deps = []\n",
    "    try:\n",
    "        import pandas as pd\n",
    "    except ImportError:\n",
    "        missing_deps.append(\"pandas\")\n",
    "    try:\n",
    "        import numpy as np\n",
    "    except ImportError:\n",
    "        missing_deps.append(\"numpy\")\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib\n",
    "    except ImportError:\n",
    "        missing_deps.append(\"matplotlib\")\n",
    "    try:\n",
    "        import seaborn as sns\n",
    "    except ImportError:\n",
    "        missing_deps.append(\"seaborn\")\n",
    "    try:\n",
    "        import sklearn\n",
    "    except ImportError:\n",
    "        missing_deps.append(\"scikit-learn\")\n",
    "    try:\n",
    "        import plotly.graph_objects as go\n",
    "    except ImportError:\n",
    "        missing_deps.append(\"plotly\")\n",
    "\n",
    "    # Start timer\n",
    "    import time\n",
    "    import os\n",
    "    import pickle\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"\\n========= Chicago Traffic Crash Analysis & Prediction Pipeline =========\")\n",
    "    print(\"\\nInitiating analysis with the following configuration:\")\n",
    "    print(f\"- Data source: {file_path}\")\n",
    "    print(f\"- Sample size: {sample_size if sample_size else 'Full dataset'}\")\n",
    "    print(f\"- Advanced models: {'Enabled' if use_advanced_models else 'Disabled'}\")\n",
    "    print(f\"- Save results: {'Enabled' if save_results else 'Disabled'}\")\n",
    "    print(f\"- Show plots: {'Enabled' if show_plots else 'Disabled'}\")\n",
    "\n",
    "    # Report missing dependencies\n",
    "    if missing_deps:\n",
    "        print(\"\\nWARNING: The following required dependencies are missing:\")\n",
    "        for dep in missing_deps:\n",
    "            print(f\" - {dep}\")\n",
    "        print(\"Please install them using: pip install \" + \" \".join(missing_deps))\n",
    "        print(\"Aborting analysis.\")\n",
    "        return {\"error\": f\"Missing required dependencies: {', '.join(missing_deps)}\"}\n",
    "\n",
    "    # Create output folder if needed\n",
    "    if save_results and not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created output folder: {output_folder}\")\n",
    "\n",
    "    # Configure matplotlib based on show_plots\n",
    "    if not show_plots:\n",
    "        plt.ioff()  # Turn off interactive mode\n",
    "        matplotlib.use('Agg')  # Use non-interactive backend\n",
    "    else:\n",
    "        plt.ion()  # Turn on interactive mode\n",
    "\n",
    "    # Run the main analysis function\n",
    "    try:\n",
    "        # Call the main function\n",
    "        results = main(file_path, sample_size=sample_size, random_seed=45665456, use_advanced_models=use_advanced_models)\n",
    "\n",
    "        # Check if analysis completed successfully\n",
    "        if 'error' in results:\n",
    "            print(f\"\\nAnalysis encountered an error: {results['error']}\")\n",
    "            return results\n",
    "\n",
    "        # Create dashboard explicitly here, if not already created\n",
    "        if 'dashboard' not in results or results['dashboard'] is None:\n",
    "            try:\n",
    "                dashboard = create_dashboard(results['data'])\n",
    "                print(\"Dashboard created successfully\")\n",
    "                results['dashboard'] = dashboard\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating dashboard: {str(e)}\")\n",
    "                results['dashboard'] = None\n",
    "\n",
    "        # Save results if requested\n",
    "        if save_results:\n",
    "            # Save processed data\n",
    "            if results['data'] is not None:\n",
    "                results['data'].to_csv(f\"{output_folder}/processed_data.csv\", index=False)\n",
    "                print(f\"Saved processed data to {output_folder}/processed_data.csv\")\n",
    "\n",
    "            # Save model results\n",
    "            if 'model_results' in results and results['model_results'] is not None:\n",
    "                with open(f\"{output_folder}/model_results.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(results['model_results'], f)\n",
    "                print(f\"Saved model results to {output_folder}/model_results.pkl\")\n",
    "\n",
    "            # Save dashboard if available\n",
    "            if 'dashboard' in results and results['dashboard'] is not None:\n",
    "                try:\n",
    "                    results['dashboard'].write_html(f\"{output_folder}/dashboard.html\")\n",
    "                    print(f\"Saved dashboard to {output_folder}/dashboard.html\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to save dashboard: {str(e)}\")\n",
    "\n",
    "            # Save visualizations\n",
    "            print(\"Saving visualizations...\")\n",
    "            \n",
    "            # Use a reliable backend for figure generation\n",
    "            matplotlib.use('Agg')\n",
    "            \n",
    "            # Generate and save each plot directly\n",
    "            # 1. Hourly crashes\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plot_crashes_by_hour(results['data'])\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_folder}/hourly_crashes.png\", dpi=100, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # 2. Daily crashes\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plot_crashes_by_day_of_week(results['data'])\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_folder}/daily_crashes.png\", dpi=100, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # 3. Weekday donut chart (if available)\n",
    "            donut_fig = plot_crashes_by_weekday_donut(results['data'])\n",
    "            if donut_fig is not None:\n",
    "                donut_fig.savefig(f\"{output_folder}/weekday_donut.png\", dpi=100, bbox_inches='tight')\n",
    "                plt.close(donut_fig)\n",
    "            \n",
    "            # 4. Monthly trends\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plot_monthly_trend(results['data'])\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_folder}/monthly_crashes.png\", dpi=100, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # 5. Temporal heatmap\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plot_temporal_heatmap(results['data'])\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_folder}/temporal_heatmap.png\", dpi=100, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # 6. Crash types\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plot_crash_types(results['data'])\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_folder}/crash_types.png\", dpi=100, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # 7. Time period visualizations\n",
    "            plt.figure(figsize=(14, 12))\n",
    "            time_period_fig = create_time_period_visualizations(results['data'])\n",
    "            if time_period_fig is not None:\n",
    "                time_period_fig.savefig(f\"{output_folder}/time_period_viz.png\", dpi=100, bbox_inches='tight')\n",
    "                plt.close(time_period_fig)\n",
    "            \n",
    "            # 8. Monthly trends by year (if available)\n",
    "            if 'monthly_trends_fig' in results and results['monthly_trends_fig'] is not None:\n",
    "                results['monthly_trends_fig'].savefig(f\"{output_folder}/monthly_trends.png\", dpi=100, bbox_inches='tight')\n",
    "                plt.close(results['monthly_trends_fig'])\n",
    "            else:\n",
    "                plt.figure(figsize=(15, 8))\n",
    "                monthly_trends_fig = plot_monthly_crash_trends_by_year(results['data'], start_year=2019)\n",
    "                monthly_trends_fig.savefig(f\"{output_folder}/monthly_trends.png\", dpi=100, bbox_inches='tight')\n",
    "                plt.close(monthly_trends_fig)\n",
    "            \n",
    "            # 9. Street injury analysis (if available)\n",
    "            if 'street_injury_fig' in results and results['street_injury_fig'] is not None:\n",
    "                results['street_injury_fig'].savefig(f\"{output_folder}/street_injury.png\", dpi=100, bbox_inches='tight')\n",
    "                plt.close(results['street_injury_fig'])\n",
    "            else:\n",
    "                street_injury_fig = plot_injury_analysis_by_street(results['data'], top_n=10)\n",
    "                if street_injury_fig is not None:\n",
    "                    street_injury_fig.savefig(f\"{output_folder}/street_injury.png\", dpi=100, bbox_inches='tight')\n",
    "                    plt.close(street_injury_fig)\n",
    "            \n",
    "            # 10. Injury rate by crash type\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plot_injury_rate_by_crash_type(results['data'])\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_folder}/injury_by_crash_type.png\", dpi=100, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # 11. Injury rate by cause\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plot_injury_rate_by_cause(results['data'])\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_folder}/injury_by_cause.png\", dpi=100, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # 12. Injury severity by weather\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plot_injury_severity_by_weather(results['data'])\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_folder}/injury_by_weather.png\", dpi=100, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # 13. Crash density map (if coordinates exist)\n",
    "            if 'LATITUDE' in results['data'].columns:\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                plot_hex_crash_density(results['data'])\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{output_folder}/crash_density_map.png\", dpi=100, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            # 14. Model performance related plots (if they exist)\n",
    "            for plot_name, plot_func in [\n",
    "                ('classification_metrics', plot_classification_metrics_comparison),\n",
    "                ('prediction_distributions', plot_prediction_distributions_enhanced),\n",
    "                ('regression_metrics', plot_regression_metrics_comparison)\n",
    "            ]:\n",
    "                plot_key = f\"{plot_name}_fig\"\n",
    "                if plot_key in results and results[plot_key] is not None:\n",
    "                    results[plot_key].savefig(f\"{output_folder}/{plot_name}.png\", dpi=100, bbox_inches='tight')\n",
    "                    plt.close(results[plot_key])\n",
    "            \n",
    "            print(f\"Saved visualizations to {output_folder}/\")\n",
    "\n",
    "        # Calculate and print execution time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\nAnalysis completed successfully in {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
    "\n",
    "        # Return detailed report\n",
    "        print(\"\\n========= Analysis Summary =========\")\n",
    "        if results['data'] is not None:\n",
    "            print(f\"Processed {len(results['data']):,} crash records\")\n",
    "\n",
    "        # Print model performance summary\n",
    "        if 'model_results' in results and results['model_results'] is not None:\n",
    "            if use_advanced_models and 'best_model' in results:\n",
    "                print(f\"Best model: {results['best_model']}\")\n",
    "            else:\n",
    "                try:\n",
    "                    best_model_name = max(results['model_results'].items(),\n",
    "                                        key=lambda x: x[1]['metrics']['roc_auc'])[0]\n",
    "                    metrics = results['model_results'][best_model_name]['metrics']\n",
    "                    print(f\"Best model: {best_model_name}\")\n",
    "                    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "                    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "                    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "                    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "                    print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "                except (KeyError, ValueError, TypeError) as e:\n",
    "                    print(f\"Could not determine best model: {str(e)}\")\n",
    "\n",
    "        if 'regression_results' in results and results['regression_results'] is not None:\n",
    "            print(\"- Injury count prediction model with visualization\")\n",
    "\n",
    "        if use_advanced_models:\n",
    "            if 'time_series_models' in results and results['time_series_models'] is not None:\n",
    "                print(\"- Time series forecasting model\")\n",
    "            if 'spatial_models' in results and results['spatial_models'] is not None:\n",
    "                print(\"- Spatial prediction model\")\n",
    "            if 'multitask_model' in results and results['multitask_model'] is not None:\n",
    "                print(\"- Multi-task learning model\")\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during analysis: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e)}\n",
    "    \n",
    "results = run_chicago_crash_analysis(file_path='Traffic_Crashes_-_Crashes_20250426.csv'\n",
    "                                    ,sample_size= 938498\n",
    "                                    ,use_advanced_models=False\n",
    "                                    ,save_results=True\n",
    "                                    ,show_plots=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432dee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
